# write.csv(results_1890_1899, "chronicling_america_1890_1899_ida_b_wells.csv", row.names = FALSE)
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
library(httr)
scrape_chronicling_america_search <- function(year) {
base_url <- "https://chroniclingamerica.loc.gov/search/pages/results/"
query <- paste0(
"?date1=", year,
"&date2=", year,
"&searchType=advanced",
"&language=&sequence=0",
"&phrasetext=Ida+B+Wells",
"&ortext=&andtext=",
"&proxtext=&proxdistance=5",
"&state=&rows=20",
"&dateFilterType=yearRange"
)
url <- paste0(base_url, query)
print(paste("Fetching:", url))
all_results <- data.frame()
page_num <- 1
repeat {
current_url <- paste0(url, "&page=", page_num)
print(paste("Fetching page", page_num, ":", current_url))
response <- tryCatch(GET(current_url), error = function(e) {
warning(paste("Error fetching page", page_num, "for year:", year, ":", e$message))
print(e)
return(NULL)
})
if (is.null(response) || response$status_code != 200) {
warning(paste("HTTP Error:", response$status_code, "for URL:", current_url))
break
}
page <- tryCatch(content(response, "parsed"), error = function(e) {
warning(paste("Error parsing page", page_num, "for year:", year, ":", e$message))
print(e)
return(NULL)
})
if (is.null(page)) {
break
}
#Check for results on the page.  If none, break the loop.
results <- page %>% html_nodes(".highlite")
if(length(results) == 0) {
print("No results found on this page.  Ending search for this year.")
break
}
results_text <- results %>% html_text(trim = TRUE)
parsed_results <- map_dfr(results_text, function(text) {
tryCatch({
newspaper <- str_extract(text, "^[^\\[\\(]+") %>% trimws()
location <- str_extract(text, "\\([^\\)]+\\)") %>%
gsub("[\\(\\)]", "", .) %>%
trimws()
date <- str_extract(text, "\\w+ \\d{1,2}, \\d{4}")
data.frame(
newspaper = newspaper,
location = location,
date = date,
full_text = text,
stringsAsFactors = FALSE
)
}, error = function(e) {
warning("Error parsing result: ", text)
print(e)
data.frame(
newspaper = NA,
location = NA,
date = NA,
full_text = text,
stringsAsFactors = FALSE
)
})
all_results <- rbind(all_results, parsed_results)
page_num <- page_num + 1
Sys.sleep(2)
}
return(all_results)
}
scrape_year_range <- function(start_year, end_year, delay = 2) {
results <- map_dfr(start_year:end_year, function(year) {
Sys.sleep(delay)
scrape_chronicling_america_search(year)
})
return(results)
}
results_1890_1899 <- scrape_year_range(1890, 1892)
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
# Function to scrape results for a single year
scrape_chronicling_america_search <- function(year) {
base_url <- "https://chroniclingamerica.loc.gov/search/pages/results/"
# Construct the search URL
query <- paste0(
"?date1=", year,
"&date2=", year,
"&searchType=advanced",
"&language=&sequence=0",
"&proxtext=Ida+B+Wells",
"&ortext=lynch",
"&proxdistance=5",
"&state=&rows=20",
"&dateFilterType=yearRange"
)
url <- paste0(base_url, query)
print(paste("Fetching:", url))
# Read and parse the search results page
page <- tryCatch(read_html(url), error = function(e) {
warning(paste("Error fetching year:", year))
return(NULL)
})
if (is.null(page)) return(data.frame())
# Get all search result blocks
results <- page %>%
html_nodes(".highlite") %>%  # Targets search result items
html_text(trim = TRUE)
# Print raw results for debugging
print(paste("Results found for year", year, ":", length(results)))
# Process each result
parsed_results <- map_dfr(results, function(text) {
tryCatch({
# Extract newspaper name, location, and date
newspaper <- str_extract(text, "^[^\\[\\(]+") %>% trimws()
location <- str_extract(text, "\\([^\\)]+\\)") %>%
gsub("[\\(\\)]", "", .) %>%
trimws()
date <- str_extract(text, "\\w+ \\d{1,2}, \\d{4}")
data.frame(
newspaper = newspaper,
location = location,
date = date,
full_text = text,
stringsAsFactors = FALSE
)
}, error = function(e) {
warning("Error parsing result: ", text)
data.frame(
newspaper = NA,
location = NA,
date = NA,
full_text = text,
stringsAsFactors = FALSE
)
})
return(parsed_results)
}
# Function to scrape for a range of years
scrape_year_range <- function(start_year, end_year, delay = 2) {
results <- map_dfr(start_year:end_year, function(year) {
Sys.sleep(delay)  # Add a delay between requests to avoid overloading the server
scrape_chronicling_america_search(year)
})
return(results)
}
# Scrape results for 1890-1892
results_1880_1889 <- scrape_year_range(1880, 1889)
# Display summary
print(paste("Total results found:", nrow(results_1880_1889)))
print(head(results_1880_1889))
# Save results to CSV
write.csv(results_1880_1889, "chronicling_america_1880_1889.csv", row.names = FALSE)
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
# Function to scrape results for a single year
scrape_chronicling_america_search <- function(year) {
base_url <- "https://chroniclingamerica.loc.gov/search/pages/results/"
# Construct the search URL
query <- paste0(
"?date1=", year,
"&date2=", year,
"&searchType=advanced",
"&language=&sequence=0",
"&proxtext=Ida+B+Wells",
"&ortext=lynch",
"&proxdistance=5",
"&state=&rows=20",
"&dateFilterType=yearRange"
)
url <- paste0(base_url, query)
print(paste("Fetching:", url))
# Read and parse the search results page
page <- tryCatch(read_html(url), error = function(e) {
warning(paste("Error fetching year:", year))
return(NULL)
})
if (is.null(page)) return(data.frame())
# Get all search result blocks
results <- page %>%
html_nodes(".highlite") %>%  # Targets search result items
html_text(trim = TRUE)
# Print raw results for debugging
print(paste("Results found for year", year, ":", length(results)))
# Process each result
parsed_results <- map_dfr(results, function(text) {
tryCatch({
# Extract newspaper name, location, and date
newspaper <- str_extract(text, "^[^\\[\\(]+") %>% trimws()
location <- str_extract(text, "\\([^\\)]+\\)") %>%
gsub("[\\(\\)]", "", .) %>%
trimws()
date <- str_extract(text, "\\w+ \\d{1,2}, \\d{4}")
data.frame(
newspaper = newspaper,
location = location,
date = date,
full_text = text,
stringsAsFactors = FALSE
)
}, error = function(e) {
warning("Error parsing result: ", text)
data.frame(
newspaper = NA,
location = NA,
date = NA,
full_text = text,
stringsAsFactors = FALSE
)
})
return(parsed_results)
}
# Function to scrape for a range of years
scrape_year_range <- function(start_year, end_year, delay = 2) {
results <- map_dfr(start_year:end_year, function(year) {
Sys.sleep(delay)  # Add a delay between requests to avoid overloading the server
scrape_chronicling_america_search(year)
})
return(results)
}
# Scrape results for 1890-1892
results_1900_1963 <- scrape_year_range(1900, 1963)
# Display summary
print(paste("Total results found:", nrow(results_1900_1963)))
print(head(results_1900_1963))
# Save results to CSV
write.csv(results_1900_1963, "chronicling_america_1900_1963.csv", row.names = FALSE)
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
# Function to scrape results for a single year
scrape_chronicling_america_search <- function(year) {
base_url <- "https://chroniclingamerica.loc.gov/search/pages/results/"
# Construct the search URL
query <- paste0(
"?date1=", year,
"&date2=", year,
"&searchType=advanced",
"&language=&sequence=0",
"&proxtext=Ida+B+Wells",
"&ortext=lynch",
"&proxdistance=5",
"&state=&rows=100",
"&dateFilterType=yearRange"
)
url <- paste0(base_url, query)
print(paste("Fetching:", url))
# Read and parse the search results page
page <- tryCatch(read_html(url), error = function(e) {
warning(paste("Error fetching year:", year))
return(NULL)
})
if (is.null(page)) return(data.frame())
# Get all search result blocks
results <- page %>%
html_nodes(".highlite") %>%  # Targets search result items
html_text(trim = TRUE)
# Print raw results for debugging
print(paste("Results found for year", year, ":", length(results)))
# Process each result
parsed_results <- map_dfr(results, function(text) {
tryCatch({
# Extract newspaper name, location, and date
newspaper <- str_extract(text, "^[^\\[\\(]+") %>% trimws()
location <- str_extract(text, "\\([^\\)]+\\)") %>%
gsub("[\\(\\)]", "", .) %>%
trimws()
date <- str_extract(text, "\\w+ \\d{1,2}, \\d{4}")
data.frame(
newspaper = newspaper,
location = location,
date = date,
full_text = text,
stringsAsFactors = FALSE
)
}, error = function(e) {
warning("Error parsing result: ", text)
data.frame(
newspaper = NA,
location = NA,
date = NA,
full_text = text,
stringsAsFactors = FALSE
)
})
return(parsed_results)
}
# Function to scrape for a range of years
scrape_year_range <- function(start_year, end_year, delay = 2) {
results <- map_dfr(start_year:end_year, function(year) {
Sys.sleep(delay)  # Add a delay between requests to avoid overloading the server
scrape_chronicling_america_search(year)
})
return(results)
}
# Scrape results for 1890-1892
results_1890_1899 <- scrape_year_range(1890, 1899)
# Display summary
print(paste("Total results found:", nrow(results_1890_1899)))
print(head(results_1890_1899))
# Save results to CSV
write.csv(results_1890_1899, "chronicling_america_1890_1899_pt2.csv", row.names = FALSE)
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
# Function to scrape results for a single year
scrape_chronicling_america_search <- function(year) {
base_url <- "https://chroniclingamerica.loc.gov/search/pages/results/"
# Construct the search URL
query <- paste0(
"?date1=", year,
"&date2=", year,
"&searchType=advanced",
"&language=&sequence=0",
"&proxtext=Ida+B+Wells",
"&ortext=lynch",
"&proxdistance=5",
"&state=&rows=1000",
"&dateFilterType=yearRange"
)
url <- paste0(base_url, query)
print(paste("Fetching:", url))
# Read and parse the search results page
page <- tryCatch(read_html(url), error = function(e) {
warning(paste("Error fetching year:", year))
return(NULL)
})
if (is.null(page)) return(data.frame())
# Get all search result blocks
results <- page %>%
html_nodes(".highlite") %>%  # Targets search result items
html_text(trim = TRUE)
# Print raw results for debugging
print(paste("Results found for year", year, ":", length(results)))
# Process each result
parsed_results <- map_dfr(results, function(text) {
tryCatch({
# Extract newspaper name, location, and date
newspaper <- str_extract(text, "^[^\\[\\(]+") %>% trimws()
location <- str_extract(text, "\\([^\\)]+\\)") %>%
gsub("[\\(\\)]", "", .) %>%
trimws()
date <- str_extract(text, "\\w+ \\d{1,2}, \\d{4}")
data.frame(
newspaper = newspaper,
location = location,
date = date,
full_text = text,
stringsAsFactors = FALSE
)
}, error = function(e) {
warning("Error parsing result: ", text)
data.frame(
newspaper = NA,
location = NA,
date = NA,
full_text = text,
stringsAsFactors = FALSE
)
})
return(parsed_results)
}
# Function to scrape for a range of years
scrape_year_range <- function(start_year, end_year, delay = 2) {
results <- map_dfr(start_year:end_year, function(year) {
Sys.sleep(delay)  # Add a delay between requests to avoid overloading the server
scrape_chronicling_america_search(year)
})
return(results)
}
# Scrape results for 1890-1892
results_1890_1899 <- scrape_year_range(1890, 1899)
# Display summary
print(paste("Total results found:", nrow(results_1890_1899)))
print(head(results_1890_1899))
# Save results to CSV
write.csv(results_1890_1899, "chronicling_america_1890_1899_pt2.csv", row.names = FALSE)
View(results_1880_1882)
combo <- rbind(results_1880_1889, results_1890_1899, results_1900_1963)
872-859
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
# Function to scrape results for a single year
scrape_chronicling_america_search <- function(year) {
base_url <- "https://chroniclingamerica.loc.gov/search/pages/results/"
# Construct the search URL
query <- paste0(
"?date1=", year,
"&date2=", year,
"&searchType=advanced",
"&language=&sequence=0",
"&proxtext=Ida+B+Wells",
"&ortext=lynch",
"&proxdistance=5",
"&state=&rows=1000",
"&dateFilterType=yearRange"
)
url <- paste0(base_url, query)
print(paste("Fetching:", url))
# Read and parse the search results page
page <- tryCatch(read_html(url), error = function(e) {
warning(paste("Error fetching year:", year))
return(NULL)
})
if (is.null(page)) return(data.frame())
# Get all search result blocks
results <- page %>%
html_nodes(".highlite") %>%  # Targets search result items
html_text(trim = TRUE)
# Print raw results for debugging
print(paste("Results found for year", year, ":", length(results)))
# Process each result
parsed_results <- map_dfr(results, function(text) {
tryCatch({
# Extract newspaper name, location, and date
newspaper <- str_extract(text, "^[^\\[\\(]+") %>% trimws()
location <- str_extract(text, "\\([^\\)]+\\)") %>%
gsub("[\\(\\)]", "", .) %>%
trimws()
date <- str_extract(text, "\\w+ \\d{1,2}, \\d{4}")
data.frame(
newspaper = newspaper,
location = location,
date = date,
full_text = text,
stringsAsFactors = FALSE
)
}, error = function(e) {
warning("Error parsing result: ", text)
data.frame(
newspaper = NA,
location = NA,
date = NA,
full_text = text,
stringsAsFactors = FALSE
)
})
return(parsed_results)
}
# Function to scrape for a range of years
scrape_year_range <- function(start_year, end_year, delay = 2) {
results <- map_dfr(start_year:end_year, function(year) {
Sys.sleep(delay)  # Add a delay between requests to avoid overloading the server
scrape_chronicling_america_search(year)
})
return(results)
}
# Scrape results for 1890-1892
results_1900_1963 <- scrape_year_range(1900, 1963)
# Display summary
print(paste("Total results found:", nrow(results_1900_1963)))
print(head(results_1900_1963))
# Save results to CSV
write.csv(results_1900_1963, "chronicling_america_1900_1963_pt2.csv", row.names = FALSE)
combo <- rbind(results_1880_1889, results_1890_1899, results_1900_1963)
combo <- combo |>
mutate(date1 = as.Date(date))
View(combo)
combo1 <- combo |>
separate(date, into = c("date1", "year"), sep = ",")
View(combo1)
combo <- combo |>
mutate(date_original = date) |>
separate(date, into = c("date1", "year"), sep = ",")
View(combo)
ida_years <- combo |>
count(year)
View(ida_years)
combo |>
count(year) |>
ggplot(aes(x = year, y = n, fill = n)) +
geom_col(position = "dodge") +
labs(title = "Articles Mentioning Ida B Wells By Year",
subtitle = "Search of Chronicling America",
caption = "Graphic by Rob Wells, 11/15/2024",
y="Count of Articles",
x="Year")
combo |>
count(year) |>
ggplot(aes(x = year, y = n, fill = n)) +
geom_col(position = "dodge") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(title = "Articles Mentioning Ida B Wells By Year",
subtitle = "Search of Chronicling America",
caption = "Graphic by Rob Wells, 11/15/2024",
y="Count of Articles",
x="Year")
write.csv(combo, "ida_b_wells_mentions_1880-1963.csv")
list <- combo |>
count(newspaper)
View(list)
list <- combo |>
count(newspaper, location)
write.csv(list, "list_of_ida_newspapers.csv")
