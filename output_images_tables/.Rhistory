inner_join(jackindex, by=c("doc_id"="filename"))
raw_docs <- lynch1
key_corpus <- corpus(raw_docs, text_field = "text")
View(raw_docs)
# Preprocessing with quanteda and create a dfm object
#key_corpus <- corpus(raw_docs, text_field = "text")
key_corpus <- corpus(raw_docs, text = "text")
names(raw_docs)
raw_docs  <- lynch1 %>%
as.data.frame()
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(raw_docs))
# create corpus object
key_corpus <- Corpus(DataframeSource(raw_docs))
key_token <- tokens(key_corpus)
key_corpus <- Corpus(DataframeSource(raw_docs))
key_corpus <- corpus(key_corpus, text = "text")
View(lynch1)
raw_docs  <- lynch1 %>%
select(doc_id, text, newspaper_state, newspaper_name, date, year) %>%
as.data.frame()
key_corpus <- corpus(raw_docs, text = "text")
View(raw_docs)
raw_docs %>%
count(doc_id) %>%
arrange(desc(n))
View(lynch1)
raw_docs  <- lynch1 %>%
select(doc_id, text, newspaper_state, newspaper_name, date, year) %>%
distinct(doc_id, .keep_all = TRUE) %>%
as.data.frame()
key_corpus <- corpus(raw_docs, text = "text")
key_corpus <- corpus(raw_docs, text_field = "text", docvars = COVARIATES)
# You can conduct a variety of preprocessing in this step as shown in the next section
key_token <- tokens(key_corpus)
# Create a document-feature matrix (a dfm object) from a token object
key_dfm <- dfm(key_token)
data_tokens <- tokens(
key_corpus,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) %>%
tokens_tolower() %>%
tokens_remove(
c(stopwords("english"),
"may", "shall", "can",
"must", "upon", "with", "without"
)
) %>%
tokens_select(min_nchar = 3)
data_dfm <- dfm(data_tokens) %>%
dfm_trim(min_termfreq = 5, min_docfreq = 2)
ncol(data_dfm)  # the number of unique words
keyATM_docs <- keyATM_read(texts = data_dfm)
summary(keyATM_docs)
keyATM_docs0 <- keyATM_read(texts = data_dfm_len0)
library(magrittr)
keyATM_docs0 <- keyATM_read(texts = data_dfm_len0)
data_dfm_rm0 <- dfm_subset(data_dfm, ntoken(data_dfm) > 0)
keywords <- list(
victim = c("wife", "girl"),
lynch_mob = c("mob", "rope", "crowd"),
legal_proceedings = c("punish", "trial", "grand"),
graphic_violence = c("dragged", "riddled"),
politics = c("governor", "prison")
)
#
# Keywords should appear reasonable times (typically more than 0.1% of the corpus) in the documents. The visualize_keywords() function plots the frequency of keywords by topic.
key_viz <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
key_viz
save_fig(key_viz, "../output/keyword.pdf", width = 6.5, height = 4)
#Get actual values
values_fig(key_viz)
#Get actual values
values <- values_fig(key_viz)
values <- values_fig(key_viz) %>%
as.data.frame()
View(values)
write.csv(values, "../output/keyATM_values.csv")
set.seed(225)  # set the seed before split the dfm
docs_withSplit <- keyATM_read(texts = data_dfm,
split = 0.3)  # split each document
out <- weightedLDA(
docs              = docs_withSplit$W_split,  # 30% of the corpus
number_of_topics  = 10,  # the number of potential themes in the corpus
model             = "base",
options           = list(seed = 250)
)
top_words(out)  # top words can aid selecting keywords
out <- keyATM(
docs              = docs_withSplit,  # 70% of the corpus
no_keyword_topics = 5,               # number of topics without keywords
keywords          = keywords,        # selected keywords
model             = "base",          # select the model
options           = list(seed = 250)
)
save(out, file = "../output/keyATM_topic_model.rds")
top_words(out)
plot_topicprop(out, show_topic = 1:5)
plot<- plot_topicprop(out, show_topic = 1:5)
save_fig(plot, "../output/plot_keyword.pdf", width = 6.5, height = 4)
top_docs(out)
plot_alpha(out)
plot_pi(out)
#install.packages("here")
here::here()
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
library(rio)
#install.packages("quanteda")
library(quanteda)
black_papers <- read_csv("../data/black_papers.csv") %>%
rename(newspaper_name = Title) %>%
as.data.frame()
library(tidyverse)
#install.packages('textdata')
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
library(googlesheets4)
#loads text of 3500 articles
#lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 3500 articles
lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 3500 articles
#lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 7162 articles
lynch <- read_csv("../data/articles_oct_19.csv")
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, URL)
View(lynch)
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, url)
names(lynch)
lynch  <- lynch [ -c(22) ]
write.csv(lynch, ("../data/articles_oct_19.csv"))
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, url)
View(x)
#loads text of 3500 articles
#lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 7162 articles
lynch <- read_csv("../data/articles_oct_19.csv")
#loads text of 3500 articles
#lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 7162 articles
lynch <- read.csv("../data/articles_oct_19.csv")
View(lynch)
View(x)
z <- lynch %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words)) %>%
ungroup()
View(lynch)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, url)
#
# # append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")
z <- y %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words)) %>%
ungroup()
z$file_id <- gsub('.txt',"", z$filename)
library(tidyr)
z <- separate(data = z, col = filename, into = c('fn1', 'crap'), sep = '_', extra ='merge', fill = 'right')
z$fn1 <- as.numeric(z$fn1)
z <- subset(z, select =-crap)
z <- y %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
#Count by decade, filter off 1960 as noise
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade==1960)
View(lynch_word_decade)
View(z)
6448+714
#76020 rows of BP text data for tokenization
bp_text <- read.csv("../data/black_press_article_text_oct_19.csv")
#loads text of 3500 articles
#lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 6448 articles predominantly white articles
white_lynch <- read.csv("../data/articles_oct_19.csv")
#loads xxx join 714 articles + 358 LOC articles xxx black press articles
#76020 rows of BP text data for tokenization
bp_text <- read.csv("../data/black_press_article_text_oct_19.csv")
View(bp_text)
black <- bp_text
xx <- stringi::stri_count_words(black$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
black <- cbind(black, xx)
yy <- black %>%
select(filename, sentence, words, year)
# append decade information for aggregation
yy$decade <- paste0(substr(yy$year, 0, 3), "0")
zz <- yy %>%
select(filename, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
#Count words by decade
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
#Count articles by year
black_articles <- blackindex %>%
select(filename, year) %>%
group_by(year) %>%
count()
#1045 black press articles. join 714 articles + 358 LOC articles
blackindex_master <- read.csv("../data/blackindex_master.csv")
black_articles <- blackindex_master %>%
select(filename, year) %>%
group_by(year) %>%
count()
View(black_articles)
black_articles_decade <- zz %>%
select(decade, filename) %>%
group_by(decade) %>%
count() %>%
mutate(Pct_Total =formattable::percent(round(n/714,2)))
View(black_articles_decade)
sum(black_articles_decade$n)
165/1045
black_articles_decade <- zz %>%
select(decade, filename) %>%
group_by(decade) %>%
count() %>%
mutate(Pct_Total =formattable::percent(round(n/1045,1)))
black_articles_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
library(kableExtra)
black_articles_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
black_articles_decade <- zz %>%
select(decade, filename) %>%
group_by(decade) %>%
count() %>%
mutate(Pct_Total =formattable::percent(round(n/1045,2)))
library(kableExtra)
black_articles_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
black_word_decade %>%
ggplot(aes(x = decade, y = avg_words,fill = avg_words)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Black Press Avg Word Count in Lynching News Coverage",
subtitle = "Based in 1,045 extracted articles, 1892-2002",
caption = "Graphic by Rob Wells, 12-26-2023",
y="Average Article Word Count",
x="Decade")
#ggsave("../output_images_tables/FigureX_black_press_avg_word_count_ap_19.png",device = "png",width=9,height=6, dpi=800)
View(zz)
View(black)
onlybptext <- filter(yy, grepl("bp", filename))
View(onlybptext)
write.csv(onlybptext("../data/only_bp_text.csv"))
write_csv(onlybptext("../data/only_bp_text.csv"))
write.csv(onlybptext,("../data/only_bp_text.csv"))
b <- stringi::stri_count_words(onlybptext$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
View(b)
onlybytext <- cbind(onlybptext, b)
bb <- onlybytext %>%
select(filename, sentence, words, year, newspaper_name, url)
names(onlybptext)
#65257 rows only the bp files.
onlybptext <- filter(black, grepl("bp", filename))
write.csv(onlybptext,("../data/only_bp_text.csv"))
b <- stringi::stri_count_words(onlybptext$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
onlybytext <- cbind(onlybptext, b)
bb <- onlybytext %>%
select(filename, sentence, words, year, newspaper_name, url)
names(onlybptext)
bb <- onlybptext %>%
select(filename, sentence, words, year, newspaper_name, url)
#
# # append decade information for aggregation
bb$decade <- paste0(substr(bb$year, 0, 3), "0")
yyy <- cbind(bb, y)
yyy <- rbind(bb, y)
zzz <- yyy %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words)) %>%
ungroup()
zzz$file_id <- gsub('.txt',"", zzz$filename)
library(tidyr)
zzz <- separate(data = zzz, col = filename, into = c('fn1', 'crap'), sep = '_', extra ='merge', fill = 'right')
zzz$fn1 <- as.numeric(zzz$fn1)
zzz <- subset(zzz, select =-crap)
View(zzz)
yyy <- rbind(bb, y)
#MAKE A NEW DF THAT GROUPS BY FILE NAME, SUMMARIZES WORD COUNT SO EACH ARTICLE HAS A WORD COUNT
zzz <- yyy %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words)) %>%
ungroup()
View(zzz)
z <- yyy %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
View(z)
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade==1960)
View(lynch_word_decade)
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade>1960)
View(zzz)
View(yyy)
lynch_word_decade %>%
ggplot(aes(x = decade, y = avg_words,fill = avg_words)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Average Word Count in Lynching News Coverage",
subtitle = "Based in 7,162 extracted articles, 1805-1969",
caption = "Graphic by Rob Wells, 12-26-2023",
y="Average Article Word Count",
x="Decade")
# ggsave("../output_images_tables/FigureX_avg_word_count_ap_16.png",device = "png",width=9,height=6, dpi=800)
ggsave("../output_images_tables/FigureX_avg_word_count_dec_26.png",device = "png",width=9,height=6, dpi=800)
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = count(decade))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = summarise(count(decade)))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = summarise(count=decade()))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = count=decade())
glimpse(z)
z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
z %>%
select(decade, total) %>%
group_by(decade) %>%
mutate(stories_decade = count(decade)) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
z %>%
select(decade, total) %>%
# group_by(decade) %>%
mutate(stories_decade = count(decade)) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
z %>%
select(decade, total) %>%
# group_by(decade) %>%
mutate(stories_decade = count(decade))
z %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n()
)
z %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n()
) %>%
mutate(avg_words = round(avg_words, 0))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n()
) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade>1960)
View(lynch_word_decade)
mean(lynch_word_decade$avg_words)
median(lynch_word_decade$avg_words)
black <- bp_text
xx <- stringi::stri_count_words(black$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
black <- cbind(black, xx)
View(black)
yy <- black %>%
select(filename, sentence, words, year)
# append decade information for aggregation
yy$decade <- paste0(substr(yy$year, 0, 3), "0")
zz <- yy %>%
select(filename, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n() )%>%
mutate(avg_words = round(avg_words, 0))
View(black_word_decade)
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n() )%>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade>1960)
mean(black_word_decade$avg_words)
sum(black_word_decade$num_articles)
black_word_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n() )%>%
mutate(avg_words = round(avg_words, 0))
library(kableExtra)
# black_articles_decade %>%
black_word_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
library(reticulate)
reticulate::repl_python()
py_install("boto3")
reticulate::repl_python()
py_install("pillow")
reticulate::repl_python()
install.packages("retriculate")
library(reticulate)
install.packages("reticulate")
library(reticulate)
py_install("pillow")
# use_python("/path/to/your/python")
reticulate::repl_python()
library(reticulate)
py_install("pillow")
# use_python("/path/to/your/python")
reticulate::repl_python()
