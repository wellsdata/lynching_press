---
title: "NRC Sentiment_Analysis"
author: "Rob Wells"
date: "2022-12-28"
output: html_document
---

```{r}
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)


```

# Tokenizing data

```{r}
#import df created from sequence below
#lynch <- read_csv("articles_1pct_dec26.csv")
lynch <- read.csv("../data/articles_oct_19.csv")


#index of 1 pct sample which has been checked by a coder and represents all valid entries
#jackindex <- read_csv("jackindex_dec26.csv")
jackindex <- read.csv("../data/master_article_index_10.19.csv")

all_text <- str_replace_all(lynch$sentence, "- ", "")
text_df <- tibble(all_text,)

# unnest includes lower, punct removal

text_tokenized <- text_df %>%
  unnest_tokens(word,all_text)

text_tokenized

#Remove stopwords

data(stop_words)

text_tokenized<- text_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  #NOT SURE IF THIS LINE SHOULD REMAIN
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

# fix the script so it doesn't pick up these file names, numbers  
# forcibly removing for now


# Word Count

text_word_ct <- text_tokenized %>%
  count(word, sort=TRUE)
```

# NRC Sentiment

NRC Lexicon on Whole Corpus
"The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust."
```{r}
# cite this lexicon
#install.packages("textdata")
nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")

nrc_sentiments %>% count(sentiment)

#sentiment & count
# anger	1246			
# anticipation	837			
# disgust	1056			
# fear	1474			
# joy	687			
# negative	3318			
# positive	2308			
# sadness	1187			
# surprise	532			
# trust	1230	

nrc_sentiments %>% 
  group_by(word) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  distinct()
```

### Review NRC Overall Sentiment


```{r}

sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments) 

#this dictionary assigns different values to the same word. negro is negative, sadness whereas lynch is anger, disgust, fear, negative and sadness.

# word sentiment
# 1 negro
# negative
# 2
# negro
# sadness
# 3
# assault
# anger
# 4
# assault
# fear
# 5
# assault
# negative
# 6
# republic
# negative
# 7
# special
# joy
# 8
# special
# positive
# 9
# negro
# negative
# 10
# negro
# sadness
# 11
# john
# disgust
# 12
# john
# negative
# 13

x <- sentiments_all %>% 
  group_by(word) %>% 
    count(sentiment)

```

# Sentiment by decade
```{r}
#before 1870
pre1870 <- lynch %>% 
  filter(year < 1870)

#1870-1879
the1870s <-  lynch %>% 
  filter(year >= 1870 & year <=1879)

#1880-1889
the1880s <-  lynch %>% 
  filter(year >= 1880 & year <=1889)

#1890-1899
the1890s <-  lynch %>% 
  filter(year >= 1890 & year <=1899)

#1900-1909
the1900s <-  lynch %>% 
  filter(year >= 1900 & year <=1909)

#1910-1919
the1910s <-  lynch %>% 
  filter(year >= 1910 & year <=1919)

#1920-1929
the1920s <-  lynch %>% 
  filter(year >= 1920 & year <=1929)

#1930-1960
post1930s <-  lynch %>% 
  filter(year >= 1930)

```

#### Compile by decade
```{r}
lynch_decade <- lynch %>% 
  mutate(decade = case_when(
         year < 1870 ~ "pre1870",
        year >= 1870 & year <=1879 ~ "1870s",
         year >= 1880 & year <=1889 ~ "1880s",
         year >= 1890 & year <=1899 ~ "1890s",
        year >= 1900 & year <=1909 ~ "1900s",
        year >= 1910 & year <=1919 ~ "1910s",
        year >= 1920 & year <=1929 ~ "1920s",
        year >= 1930 ~ "post1930s"
         ))

```

### Regional classification
```{r}
# lynch_decade %>% 
#   count(newspaper_state)
lynch_decade <- lynch_decade %>% 
  mutate(region=newspaper_state) %>% 
  mutate(region = case_when(region=="South Carolina" ~ "South",
                            region=="Texas" ~ "South",
                            region=="Louisiana" ~ "South",
                            region=="Tennessee" ~ "South",
                            region=="Mississippi" ~ "South",
                            region=="Arkansas" ~ "South",
                            region=="Alabama" ~ "South",
                            region=="Georgia" ~ "South",
                            region=="Virginia" ~ "South",
                            region=="Florida" ~ "South",
                            region=="North Carolina" ~ "South",
                            region=="Maryland" ~ "Border",
                            region=="Delaware" ~ "Border",
                            region=="West Virginia" ~ "Border",
                            region=="Kentucky" ~ "Border",
                            region=="Missouri" ~ "Border",
                            region=="Maine" ~ "North",
                            region=="New York" ~ "North",
                            region=="New Hampshire" ~ "North",
                            region=="Vermont" ~ "North",
                            region=="Massassachusetts" ~ "North",
                            region=="Connecticut" ~ "North",
                            region=="Rhode Island" ~ "North",
                            region=="Pennsylvania" ~ "North",
                            region=="New Jersey" ~ "North",
                            region=="Ohio" ~ "North",
                            region=="Indiana" ~ "North",
                            region=="Kansas" ~ "North",
                            region=="Michigan" ~ "North",
                             region=="Wisconsin" ~ "North",
                             region=="Minnesota" ~ "North",
                             region=="Iowa" ~ "North",
                             region=="California" ~ "North",
                             region=="Nevada" ~ "North",
                             region=="Oregon" ~ "North",
                            region=="Illinois" ~ "North",
                            region=="Nebraska" ~ "Misc",
                            region=="Colorado" ~ "Misc",
                            region=="North Dakota" ~ "Misc",
                            region=="South Dakota" ~ "Misc",
                            region=="Montana" ~ "Misc",
                            region=="Washington" ~ "Misc",
                            region=="Idaho" ~ "Misc",
                            region=="Wyoming" ~ "Misc",
                            region=="Utah" ~ "Misc",
                            region=="Oklahoma" ~ "Misc",
                            region=="New Mexico" ~ "Misc",
                            region=="Arizona" ~ "Misc",
                            region=="Alaska" ~ "Misc",
                            region=="Hawaii" ~ "Misc",
                            region=="District of Columbia" ~ "Misc",
                            region=="Virgin Islands" ~ "Misc",
                                                     TRUE~region)) 


```

### Tokenize all lynching
```{r}
#replace variable

all_text2 <- lynch_decade %>% 
  filter(region=="Misc")

all_text2 <- str_replace_all(all_text2$sentence, "- ", "")
text_df <- tibble(all_text2,)

text_tokenized <- text_df %>%
  unnest_tokens(word,all_text2)

data(stop_words)

text_tokenized <- text_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

NRC : replace variable
sentiments_post1930s <- text_tokenized %>%
  inner_join(nrc_sentiments) %>%
  count(sentiment, sort = TRUE) %>%
  mutate(pct_total =round(n/sum(n), digits=2)) %>%
  #replace variable
  mutate(decade = "post1930s")



```

### Count Overall Sentiment with NRC

```{r}
sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments) %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2))

sentiments_all


```


### Figure 4: NRC Sentiment by decade
https://afit-r.github.io/sentiment_analysis

```{r}
ggplot(sentiment_decade2,aes(x = decade)) +
  geom_point(aes(y=negative), size=4, color="red") +
  geom_point(aes(y=positive), size=4, color="green") +
  scale_x_discrete(limits = c('pre1870s', '1870s', '1880s', '1890s', '1900s', '1910s', '1920s', 'post1930s')) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "News Sentiment in Lynching Coverage by Decade",
       subtitle = "Based in 846 extracted articles, 1837-1960",
       caption = "Red = negative sentiment; Green = positive.\n      NRC sentiment Lexicon. Graphic by Rob Wells, 12-28-2022",
       y="Sentiment, percentage",
       x="Decade")

# ggsave("Figure4_sentiment_decade_NRC_dec28.png",device = "png",width=9,height=6, dpi=1000)
```


### Create custom dictionary

```{r}
#We should do our own custom sentiment dictionary based on the top 500 words. 

text_500 <- text_word_ct %>% 
  filter(n >= 29)

custom_dictionary <- text_500 %>%
  inner_join(nrc_sentiments)

#write.csv(custom_dictionary, "custom_dictionary.csv")

```


# CUT

### Data Table = Count Overall Sentiment with Afinn
#### Table 1 afinn sentiment jan_3_2023 
```{r}
sentiments_afinn <- text_tokenized %>%
  inner_join(afinn_sentiments) %>%
  count(value, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2)*100)

#install.packages("DT")
library(DT)
#table_1_afinn_sentiment_jan_3_2023.png
sentiments_afinn %>% 
  rename(Sentiment = value, Words_Total= n, Pct_total = pct_total) %>% 
  arrange(Sentiment) %>% 
  datatable(options = list(
  autoWidth = TRUE,
  columnDefs = list(
    list(width = '40px', targets = c("Sentiment", "Words_Total", "Pct_total")))
    )
)


```

## Figure 5: Afinn sentiment chart
```{r}
library(ggplot2)

afinn_plot <- ggplot(sentiments_afinn,aes(x = value, y = n,fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "Negative Sentiment in Lynching News Coverage",
       subtitle = "Based in 846 extracted articles, 1837-1960",
       caption = "Afinn Sentiment analysis. Graphic by Rob Wells, 1-03-2023",
       y="Score",
       x="Sentiment -5 = Negative and 5 = Positive")

afinn_plot + scico::scale_fill_scico(palette = "vik")

# ggsave("Figure5_afinn_sentiment_jan2.png",device = "png",width=9,height=6, dpi=800)


```





Overall, negative sentiment was 70% of the Southern newspaper coverage but just 69% of the Northern newspaper coverage.

```{r}
sentiments_south <- sentiments_south %>% 
  rename(south_n = n, south_pct = pct_total)

sentiments_north <- sentiments_north %>% 
  rename(north_n = n, north_pct = pct_total)

sentiments_border <- sentiments_border %>% 
  rename(border_n = n, border_pct = pct_total)

sentiments_misc <- sentiments_misc %>% 
  rename(misc_n = n, misc_pct = pct_total)



sent_regions <- sentiments_south %>% 
  inner_join(sentiments_north) %>% 
  inner_join(sentiments_border) %>% 
  inner_join(sentiments_misc)

#write.csv(sent_regions, "sent_regions_jan6.csv")
```

#### Table of Regional Sentiment with Afinn
```{r}
sent_regions %>% 
  select(value, south_pct, north_pct, border_pct, misc_pct) %>% 
  arrange(value) %>% 
  datatable(options = list(
  autoWidth = TRUE,
  columnDefs = list(
    list(width = '10px', targets = c("value", "south_pct", "north_pct", "border_pct", "misc_pct")))
    )
)

```

### Figure 8: Regional Sentiment with Afinn
```{r}

ggplot(sent_regions, aes(x=value))+
  geom_point(aes(y=south_pct), position=position_jitter(h=0.01, w=.09), size=4, color="orange") +
    # geom_point(aes(y=south_pct),  size=4, color="orange") +
  geom_point(aes(y=north_pct), position=position_jitter(h=0.01, w=.09), size=4, color="blue") +
    geom_point(aes(y=border_pct), position=position_jitter(h=0.01, w=.09), size=4, color="red") +
    geom_point(aes(y=misc_pct), position=position_jitter(h=0.01, w=.09), size=4, color="green") +
   scale_y_continuous(labels = scales::percent) +
  labs(title = "Similar Overall Sentiment by Region in Lynching News Coverage",
       subtitle = "Based in 846 extracted articles, 1837-1960",
       caption = "Blue = North. Orange = South. Red= Border. Green = Misc.  Graphic by Rob Wells, 1-05-2023",
       y="Pct of Words",
       x="Afinn Sentiment analysis. Sentiment -5 = Negative and 5 = Positive")
ggsave("Figure8_regional_sentiment_afinn_jan5.png",device = "png",width=9,height=6, dpi=1000)
```


### Tokenize by decade
Each decade was manually looped through this script
```{r}
#replace variable
all_text <- str_replace_all(post1930s$sentence, "- ", "")
text_df <- tibble(all_text,)

text_tokenized <- text_df %>%
  unnest_tokens(word,all_text)

data(stop_words)

text_tokenized <- text_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

#NRC : replace variable
# sentiments_post1930s <- text_tokenized %>%
#   inner_join(nrc_sentiments) %>% 
#   count(sentiment, sort = TRUE) %>% 
#   mutate(pct_total =round(n/sum(n), digits=2)) %>%
#   #replace variable
#   mutate(decade = "post1930s")

#Afinn: replace variable
sentiments_afinn_post1930s <- text_tokenized %>%
  inner_join(afinn_sentiments) %>% 
  count(value, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2)) %>%
  #replace variable
  mutate(decade = "post1930s")

```

NRC: Compile decades into single df
```{r}
sentiment_decade <- rbind(sentiments_pre1870s, sentiments_the1870s, sentiments_the1880s, sentiments_the1890s, sentiments_the1900s, sentiments_the1910s, sentiments_the1920s, sentiments_post1930s)

sentiment_decade2 <- sentiment_decade %>% 
  filter(sentiment=="positive" | sentiment =="negative") %>% 
  select(!(n)) %>% 
  pivot_wider(names_from = sentiment, values_from = pct_total)

```

Afinn: Compile decades into single df
```{r}
sentiments_afinn_decade <- rbind(sentiments_afinn_pre1870, sentiments_afinn_the1870s, sentiments_afinn_the1880s, sentiments_afinn_the1890s, sentiments_afinn_the1900s, sentiments_afinn_the1910s, sentiments_afinn_the1920s, sentiments_afinn_post1930s)

sentiments_afinn_decade2 <- sentiments_afinn_decade %>% 
  select(!(n)) %>% 
  pivot_wider(names_from = value, values_from = pct_total)

x <- t(sentiments_afinn_decade2) %>% 
  as.data.frame()

library(tibble)
x <- tibble::rownames_to_column(x, "Sentiment")

names(x) <- x[1,]
x <- x[-1,]

sentiments_afinn_decade2 <- x %>% 
  rename(sentiment = decade) %>% 
  mutate(sentiment = as.numeric(sentiment)) %>% 
  arrange(sentiment)


```




### Figure 6: Afinn Sentiment by decade
```{r}
afinn_sentiment_plot <- ggplot(sentiments_afinn_decade, aes(x = decade, y=value, size=pct_total)) +
  geom_point(aes(color = factor(pct_total))) +
  theme(legend.position = "none") +
  scale_x_discrete(limits = c('pre1870s', '1870s', '1880s', '1890s', '1900s', '1910s', '1920s', 'post1930s')) +
  labs(title = "Negative News Sentiment Dominates in Lynching Coverage",
       subtitle = "Based in 846 extracted articles, 1837-1960",
       caption = "Afinn sentiment Lexicon. Graphic by Rob Wells, 1-3-2022",
       y="Sentiment score. -5 is Negative",
       x="Decade")

afinn_sentiment_plot + scale_fill_manual(values = c("red", "yellow","green"))

# ggsave("Figure6_sentiment_decade_afinn_jan3.png",device = "png",width=9,height=6, dpi=1000)

```

#### Total Afinn sentiment by decade
```{r}
sentiments_afinn_decade3 <- sentiments_afinn_decade %>% 
  mutate(neg_pos = case_when(
    value < 0 ~"negative",
    value > 0 ~"positive"
  )) %>% 
  group_by(decade, neg_pos) %>% 
  summarize(sum(pct_total)) %>% 
  rename(percent = "sum(pct_total)") 

# %>% 
#   pivot_wider(names_from = neg_pos, values_from = percent)
```
  
### Figure 7: Negative News Sentiment Dominates in Lynching Coverage
```{r}
 ggplot(sentiments_afinn_decade3, aes(x = decade, y=percent, fill=neg_pos)) +
  geom_bar(stat="identity", position = "dodge") +
    scale_x_discrete(limits = c('pre1870', '1870s', '1880s', '1890s', '1900s', '1910s', '1920s', 'post1930s')) +
   scale_y_continuous(labels = scales::percent) +
  labs(title = "Negative News Sentiment Dominates in Lynching Coverage",
       subtitle = "Based in 846 extracted articles, 1837-1960",
       caption = "Afinn sentiment Lexicon. Graphic by Rob Wells, 1-4-2022",
       y="Sentiment score",
       x="Decade")

# ggsave("Figure7_sentiment_decade_afinn_jan4.png",device = "png",width=9,height=6, dpi=1000)
```

# Regional Sentiment Analysis
```{r}
sentiments_afinn_decade


```





### Notes Below

```{r}
# Anger

nrc_anger <- nrc_sentiments %>%
  filter(sentiment == "anger")

lynching_anger <- text_tokenized %>%
  inner_join(nrc_anger) %>%
  count(word, sort = TRUE)

lynching_anger

```

```{r}
# Anticipation
# results / themes not as clear as anger

nrc_anticipation <- nrc_sentiments %>%
  filter(sentiment == "anticipation")

lynching_anticipation <- lynching_tokenized %>%
  inner_join(nrc_anticipation) %>%
  count(word, sort = TRUE)

lynching_anticipation

```



```{r}
# Fear
# see a reflection of the basic word count in these results

nrc_fear <- nrc_sentiments %>%
  filter(sentiment == "fear")

lynching_fear <- lynching_tokenized %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)

lynching_fear

```


```{r}
# Disgust
# see a reflection of the basic word count in these results

nrc_disgust <- nrc_sentiments %>%
  filter(sentiment == "disgust")

lynching_disgust <- lynching_tokenized %>%
  inner_join(nrc_disgust) %>%
  count(word, sort = TRUE)

lynching_disgust

```

NRC Sentiment Dictionary Analysis

Need to recognize these are modern dictionaries, so looking here at words in corpus that are excluded from analysis.

Interesting findings - white, lynching, brown, colored. We'd likely have to modify a dictionary if sentiment analysis is a desired approach.

(Also available AFINN and bing, both measures of positive and negative. Interesting approach in "Text Mining with R" where sentiment measured by chapters of books; possible application to time segments? )

```{r}
excluded <- lynching_tokenized %>%
  anti_join(nrc_sentiments) %>%
  count(word, sort = TRUE)

excluded

```

# TF-IDF

```{r}

lynching_words <- lynching_tokenized %>%
  count(word, sort = TRUE)

lynching_words$document <- c("lynching")

lynching_words
  
```

Load in not-lynching articles.

Load and clean text. 

```{r}
# Uses "notlynch_merge" script to build lynching_corpus.txt.

not_lynching <- read_file("not_lynching/not_lynching.txt")

# close hyphenated line breaks (handling -^ but not ^-^ per review of hyphenation patterns)
not_lynching <- str_replace_all(not_lynching, "- ", "")

# not_lynching

```

Convert to df for tokenization

```{r}
not_lynching_df <- tibble(not_lynching,)
not_lynching_df
```

```{r}
# unnest includes lower, punct removal

not_lynching_tokenized <- not_lynching_df %>%
  unnest_tokens(word,not_lynching)

not_lynching_tokenized

```

Remove stopwords

```{r}

not_lynching_tokenized <- not_lynching_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "not_lynching") %>%
  filter(!grepl('[0-9]', word))

# fix the script so it doesn't pick up these file names, numbers  
# forcibly removing for now

not_lynching_tokenized

```

```{r}

not_lynching_words <- not_lynching_tokenized %>%
  count(word, sort=TRUE)

not_lynching_words$document <- c("not_lynching")

not_lynching_words

```

```{r}
coded_for_tfidf <- rbind(lynching_words, not_lynching_words)
coded_for_tfidf
```


```{r}

lynching_tf_idf <- coded_for_tfidf %>%
  bind_tf_idf(word, document, n) %>%
  arrange(desc(tf_idf))

lynching_tf_idf

```

```{r}
library(forcats)

lynching_tf_idf %>%
  group_by(document) %>%
  slice_max(tf_idf, n = 20) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = document)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~document, ncol = 2, scales = "free") + 
  labs (x = "tf-idf", y = NULL)

```

Ideally, this would have given us a set of words that could distinguish lynching from not-lynching articles after the first cut by keyword. (Values so small, not sure this is useful, but worth diffing the output perhaps?)



