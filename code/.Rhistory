newspaper2 <- newspaper2 %>%
count(region)
newspaper2 <- newspaper2 %>%
mutate(pct_total_pages =(n/sum(n))) %>%
arrange(desc(pct_total_pages))
newspaper2$pct_total_pages <-formattable::percent(newspaper2$pct_total_pages, 1)
#fact check
#sum(newspaper2$pct)
#sum(newspaper2$n)
View(newspaper2)
tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
# This contains
# tolnay_beck	5871 confirmed and probable
tolnay_beck <- read_csv("https://osf.io/download/vb8wa/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
# This contains
# tolnay_beck	5871 confirmed and probable
tolnay_beck %>%
count(status)
#Cleaning the types of lynchings
tolnay_beck <- tolnay_beck %>%
mutate(
status_clean = str_to_lower(status))
tolnay_beck$status_clean <- stringr::str_trim(tolnay_beck$status_clean)
tolnay_beck %>%
count(status_clean)
library(htmlTable)
#install.packages("rempsyc")
library(rempsyc)
tolnay_beck <- tolnay_beck %>%
mutate(
status_clean = case_when(
status_clean == 'coincident death' ~ 'coincidental death',
status_clean == 'possiible lynching' ~ 'possible lynching',
TRUE ~ status_clean
))
tolnay_counts <- tolnay_beck %>%
count(status_clean) %>%
mutate(pct_total = round(n/5871, 3)) %>%
mutate(pct_total = formattable::percent(pct_total, 1))
nice_table(tolnay_counts, short = TRUE)
library(kableExtra)
kbl(tolnay_counts) %>%
kable_paper(full_width = F) %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
tolnay_counts %>%
arrange(desc(n)) %>%
kbl(caption = "Lynching Totals, Tolnay & Beck, 2022", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
#Tolnay Beck Lynch Totals 4_14_2023.png
tolnay_graphic <- tolnay_counts %>%
arrange(desc(n)) %>%
kbl(caption = "Lynching Totals, Tolnay & Beck, 2022", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
save_kable(tolnay_graphic, file = "../lynching_press/output_images_tables/Article_Images/Figure_3_Lynching_by_Type_Tolnay_Beck_ Bailey_6_21_2024.png")
save_kable(tolnay_graphic, file ="Figure_3_Lynching_by_Type_Tolnay_Beck_ Bailey_6_21_2024.png")
save_kable(tolnay_graphic, file ="../lynching_press/output_images_tables/Article_Images/Figure_3_Lynching_by_Type_Tolnay_Beck_ Bailey_6_21_2024.png")
here()
save_kable(tolnay_graphic, file ="Figure_3_Lynching_by_Type_Tolnay_Beck_ Bailey_6_21_2024.png", width=9,height=6, dpi=800)
save_kable(tolnay_graphic, file ="Figure_3_Lynching_by_Type_Tolnay_Beck_Bailey_6_21_2024.png", width=9,height=6, dpi=800)
# Save as PNG file with specific dimensions and DPI
webshot2::webshot(kable_styling(tolnay_graphic, bootstrap_options = "striped", full_width = FALSE),
file = "tolnay_table.png",
zoom = 4,  # Adjust the zoom factor to achieve desired dimensions
delay = 2) # Optional delay to ensure proper rendering
tolnay_counts %>%
arrange(desc(n)) %>%
kbl(caption = "Lynching Totals, Tolnay & Beck, 2022", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
# Save as PNG file with specific dimensions and DPI
webshot2::webshot(kable_styling(tolnay_graphic, bootstrap_options = "striped", full_width = FALSE),
file = "tolnay_table.png",
cliprect = "viewport",
zoom = 4,
delay = 2)
kable_ggplot <- as_ggplot(tolnay_graphic)
library(knitr)
kable_ggplot <- as_ggplot(tolnay_graphic)
kable_ggplot <- kableExtra::as_ggplot(tolnay_graphic)
library(kableExtra)
kable_ggplot <- kableExtra::as_ggplot(tolnay_graphic)
ggsave("kable_table.png", tolnay_graphic, dpi = 800)
temp_html <- tempfile(fileext = ".html")
save_kable(tolnay_graphic, file = temp_html)
img_file <- tempfile(fileext = ".png")
webshot(temp_html, file = img_file, zoom = 2)
library(webshot)
temp_html <- tempfile(fileext = ".html")
save_kable(tolnay_graphic, file = temp_html)
img_file <- tempfile(fileext = ".png")
webshot(temp_html, file = img_file, zoom = 2) # Adjust zoom to control resolution
img <- grid::rasterGrob(png::readPNG(img_file), interpolate = TRUE)
ggsave("tolnay_graphic.png", plot = ggplot() + annotation_custom(img, xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf), dpi = 800, width = 10, height = 10)
# This contains tolnay_beck	5871 confirmed and probable	lynchings
tolnay_beck <- read_csv("https://osf.io/download/vb8wa/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
as.data.frame()
library(tidyverse)
#install.packages("sampler")
library(sampler)
#install.packages("rio")
library(rio)
#install.packages("kableExtra")
#install.packages("formattable")
library(formattable)
library(kableExtra)
library(knitr)
library(here)
#install.packages('htmlTable')
library(htmlTable)
#install.packages("rempsyc")
library(rempsyc)
here::here('/Users/robwells/Code/lynching_press')
# This contains tolnay_beck	5871 confirmed and probable	lynchings
tolnay_beck <- read_csv("https://osf.io/download/vb8wa/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
#Cleaning the types of lynchings
tolnay_beck <- tolnay_beck %>%
mutate(
status_clean = str_to_lower(status))
tolnay_beck$status_clean <- stringr::str_trim(tolnay_beck$status_clean)
tolnay_beck %>%
count(status_clean)
tolnay_beck %>%
count(year) %>%
group_by(year) %>%
#Sandwich it onto a simple ggplot
ggplot(aes(x = year, y = n, fill = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Actual, Threatened Lynchings, 1865-2020",
subtitle = "Count of Actual, Probable Lynchings. Tolnay-Beck Data",
caption = "n=5,871 incidents (lynchings = 5,039. Graphic by (redacted - peer review), 6/21/2024",
y="Count",
x="Year")
ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_4_Actual_Threatened_Lynchings_Tolnay_6_21_2024.png"),device = "png",width=9,height=6, dpi=800)
tolnay_beck %>%
select(year, status_clean) %>%
group_by(year) %>%
count(status_clean) %>%
#Sandwich it onto a simple ggplot
ggplot(aes(x = year, y = n, fill = n)) +
geom_col(position = "dodge") +
labs(title = "Actual, Threatened Lynchings, 1865-2020",
subtitle = "Count of Actual, Probable Lynchings. Tolnay-Beck Data",
caption = "n=5,871 incidents (lynchings = 5,039. Graphic by (redacted - peer review), 4/14/2023",
y="Count",
x="Year")
tolnay_beck_year <- tolnay_beck %>%
group_by(year) %>%
count(year) %>%
rename(total = n) %>%
mutate(pct_total = formattable::percent(total/5871, 1))
tolnay_beck_year
View(tolnay_beck)
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
here::here()
library(tidyverse)
library(tidytext)
library(rio)
library(readtext)
#topic modeling
library(quanteda)
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr)
library(kableExtra)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)
# activate klippy for copy-to-clipboard button
klippy::klippy()
lynch <- read_csv("https://osf.io/download/w3pmh/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
master_article_index_june_18_2024 <- read_csv("https://osf.io/download/fwzny/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
as.data.frame()
11396/60000
main_index <- read_csv("https://osf.io/download/hda4v/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
main_index <- janitor::clean_names(main_index)
lynch1 <- readtext("~/Code/hcij_lynching_phase_two/articles_10_19")
View(lynch1)
View(lynch)
View(lynch1)
View(lynch)
lynch1 <- lynch %>%
select(filename, sentence)
textdata <- lynch %>%
select(filename, sentence) %>%
as.data.frame()
# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
View(textdata)
View(lynch1)
textdata <- lynch %>%
select(filename, sentence) %>%
as.data.frame() %>%
rename(doc_id = filename, text= sentence)
# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
#5 term minimum[1] 1387 3019
View(lynch)
# append decade information for aggregation
textdata$decade <- paste0(substr(textdata$year, 0, 3), "0")
View(textdata)
textdata <- lynch %>%
select(filename, sentence, year) %>%
as.data.frame() %>%
rename(doc_id = filename, text= sentence)
# append decade information for aggregation
textdata$decade <- paste0(substr(textdata$year, 0, 3), "0")
#install.packages("formattable")
articles_decades <- textdata %>%
count(decade) %>%
mutate(pct_total= (n/sum(n))) %>%
mutate(pct_total= formattable::percent(pct_total)) %>%
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(decade))
library(kableExtra)
articles_decades %>%
kbl(caption = "LOC Lynching Articles by Decade (n=7,162, 11/11/2023)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em") %>%
column_spec(3, width = "5em", background = "yellow")
#Fact check 1387 articles tabulated
# textdata %>%
#   count(decade) %>%
#   summarize(sum(n))
textdata %>%
count(decade) %>%
summarize(sum(n))
articles_decades %>%
kbl(caption = "LOC Lynching Articles by Decade (n=7,162, 11/11/2023)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em") %>%
column_spec(3, width = "5em", background = "yellow")
articles_decades %>%
kbl(caption = "LOC Lynching Articles by Decade (n=11,392, 6/21/2024)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em") %>%
column_spec(3, width = "5em", background = "yellow")
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
topicNames
# get mean topic proportions per decade
topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
if (nrow(theta) != length(textdata$decade)) {
stop("The number of rows in 'theta' does not match the length of 'textdata$decade'.")
}
n_theta <- nrow(theta)
n_textdata <- length(textdata$decade)
head(theta)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
if (n_theta != n_textdata) {
stop("The number of rows in 'theta' does not match the length of 'textdata$decade'.")
}
# Optional: Check if they are already aligned
if (!all(rownames(theta) == textdata$doc_id)) {
stop("Document IDs in 'theta' do not match with 'textdata'. Please align them.")
}
# Step 3: Combine data
topic_data <- data.frame(theta, decade = textdata$decade)
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
common_ids <- intersect(rownames(theta), textdata$doc_id) # Adjust `doc_id` to your actual identifier column
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
n_theta <- nrow(theta)
n_textdata <- length(textdata$decade)
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
n_theta <- nrow(theta)
n_textdata <- length(textdata$decade)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a
# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
# If the order doesn't match, reorder one to match the other
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
stop("The document IDs still do not match. Please check the data alignment.")
}
# Step 2: Combine data
topic_data <- data.frame(theta_aligned, decade = textdata_filtered$decade)
# Step 3: Aggregate data
topic_proportion_per_decade <- aggregate(. ~ decade, data = topic_data, FUN = mean)
View(topic_proportion_per_decade)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "decade")
vizDataFrame <- vizDataFrame %>%
filter(!decade==1960)
View(vizDataFrame)
vizDataFrame3 <- vizDataFrame %>%
filter(grepl ("sheriff", variable))
# vizDataFrame1 <- vizDataFrame %>%
#   filter(grepl ("sheriff", variable))
View(vizDataFrame3)
View(textdata)
View(theta)
View(vizDataFrame)
View(theta_aligned)
# What are the most probable topics in the entire collection?
topicProportions <- colSums(theta) / nDocs(DTM)  # mean probabilities over all paragraphs
#longer topic names for this dataframe
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
names(topicProportions) <- topicNames     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order
x <- topicProportions %>%
as.data.frame()
x <- tibble::rownames_to_column(x, "row_names")
x <- x %>%
rename(proportion = '.')
x <- x %>%
arrange(desc(proportion))
x
View(x)
xx <- vizDataFrame %>%
select(variable, value) %>%
group_by(variable) %>%
summarise(sum = sum(value)) %>%
arrange(desc(sum))
View(xx)
# re-rank top topic terms for topic names
topicNames <- apply(lda::top.topic.words(beta, 10, by.score = T), 2, paste, collapse = " ")
countsOfPrimaryTopics <- rep(0, K)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nDocs(DTM)) {
topicsPerDoc <- theta[i, ] # select topic distribution for document i
# get first element position from ordered list
primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1]
countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
sort(countsOfPrimaryTopics, decreasing = TRUE)
# get topic counts per decade
#topic_count_per_decade <- aggregate(theta, by = list(decade = textdata$decade), sum)
topic_count_per_decade <- aggregate(theta, by = list(decade = textdata$decade), sum)
View(vizDataFrame3)
theta2 <- as.data.frame(theta)
View(theta2)
View(theta)
topicNames
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
topicNames
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
lynch_law_topic <- theta2 %>%
#renaming for a general topic
rename(lynch_law = '1') %>%
top_n(20, lynch_law) %>%
arrange(desc(lynch_law)) %>%
select(lynch_law)
# Apply rownames_to_column
lynch_law_topic <- tibble::rownames_to_column(lynch_law_topic, "story_id")
#list of lynch story ids, but not needed actually.
#lynch_stories <- print(lynch_mob_topic$story_id)
View(lynch_law_topic)
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
calltoaction_topic <- theta2 %>%
#renaming for a general topic
rename(calltoaction = '2') %>%
top_n(20, calltoaction) %>%
arrange(desc(calltoaction)) %>%
select(calltoaction)
# Apply rownames_to_column
calltoaction_topic <- tibble::rownames_to_column(calltoaction_topic, "story_id")
#Checks out June 21
View(calltoaction_topic)
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
unclear_topic <- theta2 %>%
#renaming for a general topic
rename(unclear = '2') %>%
top_n(20, unclear) %>%
arrange(desc(unclear)) %>%
select(unclear)
# Apply rownames_to_column
unclear_topic <- tibble::rownames_to_column(unclear_topic, "story_id")
#Checks out June 21
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
violent_crime_topic <- theta2 %>%
#renaming for a general topic
rename(violent_crime = '4') %>%
top_n(20, violent_crimer) %>%
arrange(desc(violent_crime)) %>%
select(violent_crime)
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
violent_crime_topic <- theta2 %>%
#renaming for a general topic
rename(violent_crime = '4') %>%
top_n(20, violent_crime) %>%
arrange(desc(violent_crime)) %>%
select(violent_crime)
# Apply rownames_to_column
violent_crime_topic <- tibble::rownames_to_column(violent_crime_topic, "story_id")
#Checks out June 21
#no particular pattern seen in the top articles.
View(violent_crime_topic)
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
lynching_violence_topic <- theta2 %>%
#renaming for a general topic
rename(lynching_violence = '4') %>%
top_n(20, lynching_violence) %>%
arrange(desc(lynching_violence)) %>%
select(lynching_violence)
# Apply rownames_to_column
lynching_violence_topic <- tibble::rownames_to_column(lynching_violence_topic, "story_id")
#Checks out June 21
#no particular pattern seen in the top articles.
unkn_mob_gathering_topic <- theta2 %>%
#renaming for a general topic
rename(unkn_mob_gathering = '6') %>%
top_n(20, unkn_mob_gathering) %>%
arrange(desc(unkn_mob_gathering)) %>%
select(unkn_mob_gathering)
# Apply rownames_to_column
unkn_mob_gathering_topic <- tibble::rownames_to_column(unkn_mob_gathering_topic, "story_id")
#Checks out June 21
#no particular pattern seen in the top articles.
View(unkn_mob_gathering_topic)
lynchings_topic <- theta2 %>%
#renaming for a general topic
rename(lynchings = '6') %>%
top_n(20, lynchings) %>%
arrange(desc(lynchings)) %>%
select(lynchings)
# Apply rownames_to_column
lynchings_topic <- tibble::rownames_to_column(lynchings_topic, "story_id")
#Checks out June 21
#no particular pattern seen in the top articles.
