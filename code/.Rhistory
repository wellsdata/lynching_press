# print(head(joined_df))
library(dplyr)
# Function to process each row
create_article_text <- function(row_value) {
temp <- index %>%
slice(row_value)
temp_filename <- temp$filename
# Read lines from file
tryCatch({
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
articles_df <<- bind_rows(articles_df, articles_df_temp)
}, error = function(e) {
message(paste("Error processing row", row_value, ":", conditionMessage(e)))
})
}
# Create elements needed to run function
articles_df <- tibble()
# # Running the function using lapply with smaller chunks of rows
# chunk_size <- 100  # Adjust chunk size as needed
# row_values <- split(1:nrow(index), ceiling(seq_along(index)/chunk_size))
row_values <- 1:nrow(index)
# Execute function using lapply in chunks
lapply(row_values, create_article_text)
articles_df %>%
summarize(n_unique_files = n_distinct(filename))
library(dplyr)
library(readr)
library(purrr)
# Function to process a chunk of rows
process_chunk <- function(chunk_rows, chunk_index) {
articles_chunk <- tibble()
for (row_value in chunk_rows) {
temp <- index %>%
slice(row_value)
temp_filename <- temp$filename
# Read lines from file
tryCatch({
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Append to chunk results
articles_chunk <- bind_rows(articles_chunk, articles_df_temp)
}, error = function(e) {
message(paste("Error processing row", row_value, ":", conditionMessage(e)))
})
}
# Write chunk to disk
write_csv(articles_chunk, paste0("articles_chunk_", chunk_index, ".csv"))
}
# Determine chunk size and create chunks
chunk_size <- 500  # Adjust chunk size as needed
row_values <- split(1:nrow(index), ceiling(seq_along(1:nrow(index))/chunk_size))
# Process each chunk
walk2(row_values, seq_along(row_values), process_chunk)
# Combine all chunks into one data frame
chunk_files <- list.files(pattern = "articles_chunk_.*\\.csv")
articles_df <- map_dfr(chunk_files, read_csv)
# Perform the inner join after all files are processed
joined_df <- articles_df %>%
select(filename, sentence = value) %>%
inner_join(index, by = "filename")
# View the resulting data frame
print(head(joined_df))
articles_df %>%
summarize(n_unique_files = n_distinct(filename))
articles_df %>%
summarize(n_unique_files = n_distinct(filename))
View(articles_df)
View(joined_df)
write.csv(joined_df, ("../data/extracted_text_june_18_2024.csv"))
#install.packages("here")
here::here()
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
library(rio)
#install.packages("quanteda")
library(quanteda)
blackindex_master <- read.csv("../data/master_article_index_june_18_2024.csv") %>%
filter(black_papers =="Y")
View(joined_df)
blackindex_master <- read.csv("../data/master_article_index_june_18_2024.csv") %>%
filter(black_press =="Y")
write.csv(blackindex_master, "../datablackindex_master_june_18_2024.csv")
write.csv(blackindex_master, "../data/blackindex_master_june_18_2024.csv")
black_articles_text_june_18_2024 < read.csv("../data/extracted_text_june_18_2024.csv")%>%
filter(black_press =="Y")
black_articles_text_june_18_2024 <- read.csv("../data/extracted_text_june_18_2024.csv")%>%
filter(black_press =="Y")
write.csv(black_articles_text_june_18_2024, "../data/black_articles_text_june_18_2024.csv")
library(tidyverse)
#install.packages("sampler")
library(sampler)
#install.packages("rio")
library(rio)
#install.packages("kableExtra")
#install.packages("formattable")
library(formattable)
library(kableExtra)
library(knitr)
library(here)
here::here('/Users/robwells/Code/hcij_lynching_phase_two')
library(tidyverse)
#install.packages("sampler")
library(sampler)
#install.packages("rio")
library(rio)
#install.packages("kableExtra")
#install.packages("formattable")
library(formattable)
library(kableExtra)
library(knitr)
library(here)
here::here('/Users/robwells/Code/lynching_press')
index <- read_csv("../data/main_index_dec_28_2023.csv")
#DEC 27 VER
index <- read_csv("../storage_old_article_indexes_lists/main_index_dec_28_2023.csv")
View(index)
#DEC 27 VER
index <- read_csv("../data/main_index_dec_28_2023.csv")
index2 <- read_csv("../data/master_article_index_june_18_2024.csv") %>%
as.data.frame()
#sample code: ssampcalc(df, n, strata, over=0)
x <- ssampcalc(index, n=60042, strata=year, over=0.5)
x <- janitor::clean_names(x)
x
#Fact check
#sum(x$nh)
#nh is the total pages per year
#wt[,1] is the percentage of the total corpus (60042 pages)
#Shows peak media coverage of lynching activity between 1893-1910
#Peak year for coverage was 1903, with 2971 pages and 4.94% of all pages
#10% sample = 6,004
# Draw stratified sample (proportional allocation)
# ssamp(df, n, strata, over=1)
y <- ssamp(index, n=6004, strata=year, over=1)
y
#fact check
#this shows a very close parallel to the stratified sample (x) done above of the 60,042 pages
y_pagecount <- y %>%
count(year) %>%
group_by(year) %>%
mutate(pct = (n/6004)*100)
#Write the sample corpus to a spreadsheet
write.csv(y, "sample_corpus_10pct_dec_27_2023.csv")
#1789-1963, 18.1 million rows
index_chron <- rio::import("/Users/robwells/Library/CloudStorage/GoogleDrive-robwells@umd.edu/My Drive/Lynching UMD/Data/Sampling and Full Chron America/chron_am_manifest.csv")
index_chron <- janitor::clean_names(index_chron)
#sample code: ssampcalc(df, n, strata, over=0)
xz <- ssampcalc(index_chron, n=18091200, strata=year, over=0.5)
xz <- janitor::clean_names(x)
xz
#Fact check
#sum(x$nh)
#nh is the total pages per year
#wt[,1] is the percentage of the total corpus (108,575 pages)
#Shows peak media coverage of lynching activity between 1893-1910
pageplacement <- index2 %>%
count(page) %>%
group_by(page, decade) %>%
ungroup()
index2$decade <- paste0(substr(index2$year, 0, 3), "0")
View(index2)
index2$decade <- paste0(substr(index2$year, 0, 3), "0")
pageplacement <- index2 %>%
count(page) %>%
group_by(page, decade) %>%
ungroup()
pageplacement <- index2 %>%
group_by(page, decade) %>%
count(page) %>%
ungroup()
View(pageplacement)
pageplacement <- index2 %>%
mutate(page = str_replace(page, "seq-", "")) %>%
group_by(page, decade) %>%
count(page) %>%
ungroup()
pageplacement <- pageplacement %>%
mutate(pct =(n/sum(n)))
pageplacement$pct <-formattable::percent(pageplacement$pct, 1)
pageplacement %>%
top_n(10,pct) %>%
ggplot(aes(x = page, y = pct, fill = pct)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(breaks=c(1:15)) +
scale_y_continuous(labels = scales::percent) +
geom_text(aes(label= pct, x= page, y= pct), hjust=.5, vjust=0) +
labs(title = "Page Placement, Lynching Coverage, 1789-1963",
subtitle = "Page Number Placement of Lynching Stories",
caption = "Page 1 stories were 29% of 59,967 pages. Graphic by Rob Wells, 12/27/2023",
y="Pct of Pages",
x="Page Number")
pageplacement %>%
top_n(10,pct) %>%
ggplot(aes(x = page, y = pct, fill = pct))
pageplacement %>%
top_n(10,pct) %>%
ggplot(aes(x = page, y = pct, fill = pct)) +
geom_col(position = "dodge") +
theme(legend.position = "none")
pageplacement %>%
top_n(10,pct) %>%
ggplot(aes(x = page, y = pct, fill = pct)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(breaks=c(1:15)) +
scale_y_continuous(labels = scales::percent)
pageplacement %>%
top_n(10,pct) %>%
ggplot(aes(x = page, y = pct, fill = pct)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(breaks=c(1:15))
pageplacement %>%
top_n(10,pct) %>%
ggplot(aes(x = page, y = pct, fill = pct)) +
geom_col(position = "dodge") +
theme(legend.position = "none")
pageplacement %>%
top_n(10,pct)
index2$decade <- paste0(substr(index2$year, 0, 3), "0")
index2<- index2 %>%
select(newspaper_name, newspaper_state, sn, year, month, day, decade, edition,page, filepath)
index_pages <- index2 %>%
mutate(page_one = ifelse(page > 1, FALSE, TRUE))
pages_decade <- index_pages %>%
group_by(page_one, decade) %>%
count(page_one) %>%
ungroup()
pages_decade <- pages_decade %>%
group_by(decade, page_one) %>%
summarise(n = sum(n)) %>%
mutate(percentage = round(n / sum(n) * 100, 1))
write.csv(pages_decade, "../output/pages_decade_12_27_.csv")
index2$decade <- paste0(substr(index2$year, 0, 3), "0")
index2<- index2 %>%
select(newspaper_name, newspaper_state, sn, year, month, day, decade, edition,page, filepath)
index_pages <- index2 %>%
mutate(page_one = ifelse(page > 1, FALSE, TRUE))
pages_decade <- index_pages %>%
group_by(page_one, decade) %>%
count(page_one) %>%
ungroup()
pages_decade <- pages_decade %>%
group_by(decade, page_one) %>%
summarise(n = sum(n)) %>%
mutate(percentage = round(n / sum(n) * 100, 1))
# write.csv(pages_decade, "../output/pages_decade_12_27_.csv")
pages_decade %>%
filter(decade > "1820") %>%
mutate(page = case_when(
str_detect(page_one, "TRUE") ~ "PageOne",
str_detect(page_one, "FALSE") ~ "Inside")) %>%
ggplot(aes(x = decade, y = percentage, fill = page)) +
geom_col(position = "dodge") +
labs(title = "Percentage Page One Lynching Stories, 1830-1960",
subtitle = "Page One Stories By Decade",
caption = "Page 1 lynching stories peaked at 35% in the 1880s and 1920s. Graphic by Rob Wells, 12/27/2023",
y="Pct of Pages",
x="Decade")
# ggsave("../output_images_tables/pages_decades_6_18_2023.png",device = "png",width=9,height=6, dpi=800)
View(index_pages)
index2 <- read_csv("../data/master_article_index_june_18_2024.csv") %>%
mutate(page = str_replace(page, "seq-", "")) %>%
as.data.frame()
index2$decade <- paste0(substr(index2$year, 0, 3), "0")
index2<- index2 %>%
select(newspaper_name, newspaper_state, sn, year, month, day, decade, edition,page, filepath)
index_pages <- index2 %>%
mutate(page_one = ifelse(page > 1, FALSE, TRUE))
pages_decade <- index_pages %>%
group_by(page_one, decade) %>%
count(page_one) %>%
ungroup()
View(pages_decade)
pages_decade <- pages_decade %>%
group_by(decade, page_one) %>%
summarise(n = sum(n)) %>%
mutate(percentage = round(n / sum(n) * 100, 1))
pages_decade %>%
filter(decade > "1820") %>%
mutate(page = case_when(
str_detect(page_one, "TRUE") ~ "PageOne",
str_detect(page_one, "FALSE") ~ "Inside")) %>%
ggplot(aes(x = decade, y = percentage, fill = page)) +
geom_col(position = "dodge") +
labs(title = "Percentage Page One Lynching Stories, 1830-1960",
subtitle = "Page One Stories By Decade",
caption = "Page 1 lynching stories peaked at 35% in the 1880s and 1920s. Graphic by Rob Wells, 12/27/2023",
y="Pct of Pages",
x="Decade")
View(joined_df)
#11,396 articles, includes all bp articles
master_article_index_june_18_2024 <- read.csv("../data/master_article_index_june_18_2024.csv")
View(master_article_index_june_18_2024)
master_article_index_6_17 <- master_article_index_6_17 %>%
mutate(page = str_replace(page, "seq-", "")) %>%
mutate(page = coalesce(page, pages))
master_article_index_6_17 <- master_article_index_june_18_2024
master_article_index_6_17 <- master_article_index_6_17 %>%
mutate(page = str_replace(page, "seq-", "")) %>%
mutate(page = coalesce(page, pages))
View(master_article_index_6_17)
master_article_index_6_17 %>%
count(page) %>%
arrange(desc(n))
master_article_index_6_17 <- master_article_index_6_17 %>%
mutate(page = str_replace(page, "seq-", "")) %>%
mutate(page = coalesce(page, pages)) %>%
mutate(page = str_replace(page, "A", ""))
master_article_index_6_17 %>%
count(page) %>%
arrange(desc(n))
master_article_index_6_17 <- master_article_index_6_17 %>%
mutate(page = str_replace(page, "seq-", "")) %>%
mutate(page = coalesce(page, pages)) %>%
mutate(page = str_replace(page, "A", "")) %>%
mutate(start_page = str_replace(start_page, "A", "")) %>%
mutate(page2 = coalesce(page, start_page)) %>%
subset(select = -c(pages, X, seq))
master_article_index_6_17 %>%
count(page2) %>%
arrange(desc(n))
master_article_index_6_17 <- master_article_index_june_18_2024
master_article_index_6_17 <- master_article_index_6_17 %>%
mutate(page = str_replace(page, "seq-", "")) %>%
mutate(page = coalesce(page, pages)) %>%
mutate(page = str_replace(page, "A", "")) %>%
mutate(start_page = str_replace(start_page, "A", "")) %>%
mutate(page2 = coalesce(page, start_page)) %>%
rename(page_old = page, page = page2) %>%
subset(select = -c(pages, X, seq, start_page, page_old))
View(master_article_index_6_17)
master_article_index_6_17 %>%
count(page2) %>%
arrange(desc(n))
master_article_index_6_17 %>%
count(page) %>%
arrange(desc(n))
write.csv(master_article_index_6_17, "../data/master_article_index_june_18_2024.csv")
index2 <- read_csv("../data/master_article_index_june_18_2024.csv") %>%
as.data.frame()
index2$decade <- paste0(substr(index2$year, 0, 3), "0")
index2<- index2 %>%
select(newspaper_name, newspaper_state, sn, year, month, day, decade, edition,page, filepath)
index_pages <- index2 %>%
mutate(page_one = ifelse(page > 1, FALSE, TRUE))
pages_decade <- index_pages %>%
group_by(page_one, decade) %>%
count(page_one) %>%
ungroup()
View(pages_decade)
pages_decade <- pages_decade %>%
group_by(decade, page_one) %>%
summarise(n = sum(n)) %>%
mutate(percentage = round(n / sum(n) * 100, 1))
pages_decade %>%
filter(decade > "1820") %>%
mutate(page = case_when(
str_detect(page_one, "TRUE") ~ "PageOne",
str_detect(page_one, "FALSE") ~ "Inside")) %>%
ggplot(aes(x = decade, y = percentage, fill = page)) +
geom_col(position = "dodge") +
labs(title = "Percentage Page One Lynching Stories, 1830-1960",
subtitle = "Page One Stories By Decade",
caption = "Page 1 lynching stories peaked at 35% in the 1880s and 1920s. Graphic by Rob Wells, 6/18/2024",
y="Pct of Pages",
x="Decade")
pages_decade %>%
filter(decade > "1820" | decade < "1970") %>%
mutate(page = case_when(
str_detect(page_one, "TRUE") ~ "PageOne",
str_detect(page_one, "FALSE") ~ "Inside")) %>%
ggplot(aes(x = decade, y = percentage, fill = page)) +
geom_col(position = "dodge") +
labs(title = "Percentage Page One Lynching Stories, 1830-1960",
subtitle = "Page One Stories By Decade",
caption = "Page 1 lynching stories peaked at 35% in the 1880s and 1920s. Graphic by Rob Wells, 6/18/2024",
y="Pct of Pages",
x="Decade")
pages_decade %>%
filter(decade > "1820") %>%
filter(decade < "1970") %>%
mutate(page = case_when(
str_detect(page_one, "TRUE") ~ "PageOne",
str_detect(page_one, "FALSE") ~ "Inside")) %>%
ggplot(aes(x = decade, y = percentage, fill = page)) +
geom_col(position = "dodge") +
labs(title = "Percentage Page One Lynching Stories, 1830-1960",
subtitle = "Page One Stories By Decade",
caption = "Page 1 lynching stories peaked at 35% in the 1880s and 1920s. Graphic by Rob Wells, 6/18/2024",
y="Pct of Pages",
x="Decade")
pages_decade %>%
filter(decade > "1820") %>%
filter(decade < "1970") %>%
mutate(page = case_when(
str_detect(page_one, "TRUE") ~ "PageOne",
str_detect(page_one, "FALSE") ~ "Inside")) %>%
ggplot(aes(x = decade, y = percentage, fill = page)) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("PageOne" = "red", "Inside" = "blue")) +
labs(title = "Percentage Page One Lynching Stories, 1830-1960",
subtitle = "Page One Stories By Decade",
caption = "Page 1 lynching stories peaked at 46% in the 1920s. Graphic by Rob Wells, 6/18/2024",
y="Pct of Pages",
x="Decade")
View(pages_decade)
pages_decade %>%
filter(!is.na(page_one)) %>%
filter(decade > "1820") %>%
filter(decade < "1970") %>%
mutate(page = case_when(
str_detect(page_one, "TRUE") ~ "PageOne",
str_detect(page_one, "FALSE") ~ "Inside")) %>%
ggplot(aes(x = decade, y = percentage, fill = page)) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("PageOne" = "red", "Inside" = "blue")) +
labs(title = "Percentage Page One Lynching Stories, 1830-1960",
subtitle = "Page One Stories By Decade",
caption = "Page 1 lynching stories peaked at 46% in the 1920s. Graphic by Rob Wells, 6/18/2024",
y="Pct of Pages",
x="Decade")
pages_decade %>%
filter(!is.na(page_one)) %>%
filter(decade > "1820") %>%
filter(decade < "1970") %>%
mutate(page = case_when(
str_detect(page_one, "TRUE") ~ "PageOne",
str_detect(page_one, "FALSE") ~ "Inside")) %>%
ggplot(aes(x = decade, y = percentage, fill = page)) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("PageOne" = "red", "Inside" = "lightblue")) +
labs(title = "Percentage Page One Lynching Stories, 1830-1960",
subtitle = "Page One Stories By Decade",
caption = "Page 1 lynching stories peaked at 46% in the 1920s. Graphic by Rob Wells, 6/18/2024",
y="Pct of Pages",
x="Decade")
ggsave("../output_images_tables/pages_decades_6_18_2023.png",device = "png",width=9,height=6, dpi=800)
#Outdated data using the seguin - tolnay merged. See Notes for why this is problematic. April 11, 2023
# seguin_tolnay <- read_csv(here::here("../victim_name_search/victim_name_regex_data/seguin_and_tolnay_beck_merged.csv")) %>%
#   as.data.frame()
tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
# This contains
# tolnay_beck	5871 confirmed and probable
tolnay_beck <- tolnay_beck %>%
mutate(
status_clean = str_to_lower(status))
tolnay_beck$status_clean <- stringr::str_trim(tolnay_beck$status_clean)
tolnay_beck %>%
count(status_clean)
d <- tolnay_beck %>%
select(lynch_state, year) %>%
group_by(lynch_state) %>%
count(lynch_state)
y <- sum(d$n)
d <- d %>%
mutate(pct_total = formattable::percent(n/y, 1))
#write.csv(d, "seguin_tolnay_state_totals.csv")
#Determine sample size by strata using proportional allocation
#ssampcalc(df, n, strata, over=0)
xx <- ssampcalc(tolnay_beck, n=5871, strata=year, over=0.5)
xx
#fact check article totals
sum(d$n)
sum(xx$Nh)
#fact check weighting
d <- janitor::clean_names(d)
xx <- janitor::clean_names(xx)
sum(xx$wt)
#adds up to 1
umdsample <- x %>%
rename(umdnh = nh, umdwt = wt)
tolnaysample <- xx %>%
rename(tolnaynh = nh, tolnaywt = wt)
combo <- umdsample %>%
right_join(tolnaysample, by="year") %>%
filter(year <="1963")
#write.csv(combo,("../output/combo_feb10_2024.csv"))
View(combo)
#plot it
pl <- ggplot(data = combo, aes(x = year))
pl <- pl + geom_line(aes(y=tolnaywt), colour = "blue")
pl <- pl + geom_line(aes(y=umdwt), colour = "red")
pl <- pl + theme_classic()
pl <- pl + labs(x = "Year", y = "Weighted Score")
pl <- pl + labs(title = "Media Coverage vs Lynchings, 1865-1963",
subtitle = "Weighted Samples: Media Coverage (Red) vs. Actual Victims (Blue)",
caption = "Tolnay_Beck Victims n = 5871. Media n = 59,597 pages. Graphic by Rob Wells, 4/11/2023")
# ggsave("../output_images_tables/media_lynchings_4_11_2023.png", device = "png",width=9,height=6, dpi=800)
pl
#focus on the change in 1890 - 1930
combo1 <- combo %>%
filter(year >= "1890" & year <= "1930") %>%
select(year, tolnaywt, umdwt) %>%
rename(lynchings = tolnaywt, news_reports = umdwt)
df <- combo1 %>%
pivot_longer(!year, names_to = "type", values_to = "weight")
ggplot(df, aes(x=year, y=weight, fill=type)) +
geom_bar(stat='identity', position='dodge') +
scale_x_continuous(labels = c(seq(1890, 1930, 5)), breaks = seq(1890, 1930, 5)) +
labs(title = "Media Coverage Exceeds Actual Lynchings After 1893",
subtitle = "Weighted Samples: Media Coverage (Red) vs. Actual Victims (Blue)",
caption = "tolnay_beck Victims n = 5871. Media n = 59,597 pages. Graphic by Rob Wells, 4/11/2023")
#ggsave(here::here("../hcij_lynching_phase_two/narratives/output_images_tables/media_lynching_2.png"),device = "png",width=9,height=6, dpi=800)
