region %>%
ggplot(aes(x = region, y = pct, fill = pct)) +
geom_col(position = "dodge") +
theme(legend.position = "none", plot.subtitle = element_text(color = "blue", size = 8, face = "italic")) +
scale_y_continuous(labels = scales::percent) +
geom_text(aes(label = scales::percent(pct_total_pages)), size = 4, hjust=.5, vjust=0) +
labs(title = "Regional Distribution of Lynching Coverage, 1805-1963",
subtitle = "Newspapers by Census Region",
caption = "Newspapers by region with lynching coverage. n=11,223 articles. Graphic by (redacted - peer review), 6/23/2024",
y="Pct of Pages",
x="Region")
ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_2_regional_coverage_6_23_2024.png"),device = "png",width=9,height=6, dpi=800)
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
here::here()
library(tidyverse)
library(tidytext)
library(rio)
library(readtext)
#topic modeling
library(quanteda)
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr)
library(kableExtra)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)
# activate klippy for copy-to-clipboard button
klippy::klippy()
#import 11,396 text files that were compiled into a df
lynch <- read_csv("https://osf.io/download/p32he/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
#60,042 Library of Congress articles on lynching captured.
main_index <- read_csv("https://osf.io/download/hda4v/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
main_index <- janitor::clean_names(main_index)
textdata <- lynch %>%
select(filename, sentence, year) %>%
as.data.frame() %>%
rename(doc_id = filename, text= sentence)
# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339
# append decade information for aggregation
textdata$decade <- paste0(substr(textdata$year, 0, 3), "0")
#install.packages("formattable")
articles_decades <- textdata %>%
count(decade) %>%
mutate(pct_total= (n/sum(n))) %>%
mutate(pct_total= formattable::percent(pct_total)) %>%
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(decade))
library(kableExtra)
articles_decades %>%
kbl(caption = "LOC Lynching Articles by Decade (n=11,223, 6/23/2024)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em") %>%
column_spec(3, width = "5em", background = "yellow")
#Fact check 308597 rows tabulated
# textdata %>%
#   count(decade) %>%
#   summarize(sum(n))
View(textdata)
articles_decades <- textdata %>%
distinct(doc_id, .keep_all=TRUE) %>%
count(decade) %>%
mutate(pct_total= (n/sum(n))) %>%
mutate(pct_total= formattable::percent(pct_total)) %>%
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(decade))
View(articles_decades)
sum(articles_decades$n)
library(kableExtra)
articles_decades %>%
kbl(caption = "LOC Lynching Articles by Decade (n=11,223, 6/23/2024)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em") %>%
column_spec(3, width = "5em", background = "yellow")
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$decade)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column
# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")
# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
# If the order doesn't match, reorder one to match the other
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}
# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
stop("The document IDs still do not match. Please check the data alignment.")
}
# Step 2: Combine data
topic_data <- data.frame(theta_aligned, decade = textdata_filtered$decade)
# Step 3: Aggregate data
topic_proportion_per_decade <- aggregate(. ~ decade, data = topic_data, FUN = mean)
# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "decade")
# #filter out 1960 - one article
vizDataFrame <- vizDataFrame %>%
filter(!decade==1960)
topicNames
#add categories
#Updated June 27. See notes: https://docs.google.com/document/d/1BvYAye_8biVUaJchm1PB1Z0hrycBvdX0jBwlB38xKsc/edit
vizDataFrame <- vizDataFrame %>%
mutate(category = case_when(
str_detect(variable, "lynch mob negro men jail hang murder law prison made") ~ "lynch_act",
#check this
str_detect(variable, "law peopl crime race great south lynch countri good public") ~ "civil_society?",
str_detect(variable, "bodi shot fire hang hous jail tree found door head") ~ "lynch_mob",
str_detect(variable, "negro white murder man year kill lynch charg assault mrs") ~ "lynching_violence",
#check this
str_detect(variable, "state court juri governor case unit judg call trial investig") ~ "legal_proceedings",
#check this
str_detect(variable,  "counti sheriff night jail citi morn day deputi mile town") ~ "law_enforcement?",
))
View(vizDataFrame)
lynchings_topic <- theta2 %>%
#renaming for a general topic
rename(law_enforcement = '3') %>%
top_n(20, law_enforcement) %>%
arrange(desc(law_enforcement)) %>%
select(law_enforcement)
lynchings_topic <- theta %>%
#renaming for a general topic
rename(law_enforcement = '3') %>%
top_n(20, law_enforcement) %>%
arrange(desc(law_enforcement)) %>%
select(law_enforcement)
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
lynchings_topic <- theta2 %>%
#renaming for a general topic
rename(law_enforcement = '3') %>%
top_n(20, law_enforcement) %>%
arrange(desc(law_enforcement)) %>%
select(law_enforcement)
# Apply rownames_to_column
law_enforcement_topic <- tibble::rownames_to_column(law_enforcement_topic, "story_id")
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
law_enforcement_topic <- theta2 %>%
#renaming for a general topic
rename(law_enforcement = '3') %>%
top_n(20, law_enforcement) %>%
arrange(desc(law_enforcement)) %>%
select(law_enforcement)
# Apply rownames_to_column
law_enforcement_topic <- tibble::rownames_to_column(law_enforcement_topic, "story_id")
#Checks out June 21
#no particular pattern seen in the top articles.
View(law_enforcement_topic)
law_enforcement_topic <- gsub("X", "", law_enforcement_topic$story_id)
theta2 <- as.data.frame(theta)
law_enforcement_topic <- theta2 %>%
#renaming for a general topic
rename(law_enforcement = '3') %>%
top_n(20, law_enforcement) %>%
arrange(desc(law_enforcement)) %>%
select(law_enforcement)
# Apply rownames_to_column
law_enforcement_topic <- tibble::rownames_to_column(law_enforcement_topic, "story_id")
View(law_enforcement_topic)
law_enforcement_topic$story_id <- gsub("X", "", law_enforcement_topic$story_id)
print(topicNames)
vizDataFrame <- vizDataFrame %>%
mutate(category = case_when(
str_detect(variable, "lynch mob negro men jail hang murder law prison made") ~ "lynch_act",
#check this
str_detect(variable, "law peopl crime race great south lynch countri good public") ~ "civil_society?",
str_detect(variable, "bodi shot fire hang hous jail tree found door head") ~ "lynch_mob",
str_detect(variable, "negro white murder man year kill lynch charg assault mrs") ~ "lynching_violence",
#check this
str_detect(variable, "state court juri governor case unit judg call trial investig") ~ "legal_proceedings",
#check this
str_detect(variable,  "counti sheriff night jail citi morn day deputi mile town") ~ "lynchings",
))
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
civil<- theta2 %>%
#renaming for a general topic
rename(law_enforcement = '5') %>%
top_n(20, civil) %>%
arrange(desc(civil)) %>%
select(civil)
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
civil<- theta2 %>%
#renaming for a general topic
rename(civil = '5') %>%
top_n(20, civil) %>%
arrange(desc(civil)) %>%
select(civil)
# Apply rownames_to_column
civil <- tibble::rownames_to_column(civil, "story_id")
civil$story_id <- gsub("X", "", civil$story_id)
#Checks out June 23
View(civil)
#for topic 1, lynch mob
theta2 <- as.data.frame(theta)
legal<- theta2 %>%
#renaming for a general topic
rename(legal = '6') %>%
top_n(20, legal) %>%
arrange(desc(legal)) %>%
select(legal)
# Apply rownames_to_column
legal <- tibble::rownames_to_column(legal, "story_id")
legal$story_id <- gsub("X", "", legal$story_id)
#Checks out June 23
View(legal)
lynch1 <- lynch %>%
filter(!black_press=="Y")
View(lynch)
lynch1 <- lynch %>%
filter(black_press=="Y")
lynch1 <- lynch %>%
filter(!black_press=="Y")
lynch1 <- lynch %>%
filter(black_press!="Y")
wspapers
lynch1 <- lynch %>%
filter(black_press==is.na)
lynch1 <- lynch %>%
filter(black_press != "Y" | is.na(black_press))
textdata <- lynch1 %>%
select(filename, sentence, year) %>%
as.data.frame() %>%
rename(doc_id = filename, text= sentence)
# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
vizDataFrame_white_black <- vizDataFrame
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339
# append decade information for aggregation
textdata$decade <- paste0(substr(textdata$year, 0, 3), "0")
#install.packages("formattable")
articles_decades <- textdata %>%
distinct(doc_id, .keep_all=TRUE) %>%
count(decade) %>%
mutate(pct_total= (n/sum(n))) %>%
mutate(pct_total= formattable::percent(pct_total)) %>%
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(decade))
library(kableExtra)
articles_decades %>%
kbl(caption = "LOC Lynching Articles by Decade (n=11,223, 6/23/2024)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em") %>%
column_spec(3, width = "5em", background = "yellow")
#Fact check 308597 rows tabulated
#sum(articles_decades$n)
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$decade)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column
# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")
# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
# If the order doesn't match, reorder one to match the other
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}
# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
stop("The document IDs still do not match. Please check the data alignment.")
}
# Step 2: Combine data
topic_data <- data.frame(theta_aligned, decade = textdata_filtered$decade)
# Step 3: Aggregate data
topic_proportion_per_decade <- aggregate(. ~ decade, data = topic_data, FUN = mean)
# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "decade")
# #filter out 1960 - one article
vizDataFrame <- vizDataFrame %>%
filter(!decade==1960)
topicNames
theta2 <- as.data.frame(theta)
critic<- theta2 %>%
#renaming for a general topic
rename(critic = '2') %>%
top_n(20, critic) %>%
arrange(desc(critic)) %>%
select(critic)
# Apply rownames_to_column
critic <- tibble::rownames_to_column(critic, "story_id")
critic$story_id <- gsub("X", "", critic$story_id)
#Checks out June 23
View(critic)
head(critic$story_id)
head(critic$story_id, 20)
theta2 <- as.data.frame(theta)
unknown <- theta2 %>%
#renaming for a general topic
rename(unknown = '3') %>%
top_n(20, unknown ) %>%
arrange(desc(unknown )) %>%
select(unknown )
# Apply rownames_to_column
unknown  <- tibble::rownames_to_column(unknown , "story_id")
unknown $story_id <- gsub("X", "", unknown $story_id)
#Checks out June 23
head(unknown$story_id, 20)
theta2 <- as.data.frame(theta)
legal <- theta2 %>%
#renaming for a general topic
rename(legal = '3') %>%
top_n(20, legal ) %>%
arrange(desc(legal )) %>%
select(legal )
# Apply rownames_to_column
legal  <- tibble::rownames_to_column(legal , "story_id")
legal $story_id <- gsub("X", "", legal $story_id)
head(legal$story_id, 20)
#Checks out June 23
theta2 <- as.data.frame(theta)
legal <- theta2 %>%
#renaming for a general topic
rename(legal = '5') %>%
top_n(20, legal ) %>%
arrange(desc(legal )) %>%
select(legal )
# Apply rownames_to_column
legal  <- tibble::rownames_to_column(legal , "story_id")
legal $story_id <- gsub("X", "", legal $story_id)
head(legal$story_id, 20)
#Checks out June 23
#add categories
#Updated June 27. See notes: https://docs.google.com/document/d/1BvYAye_8biVUaJchm1PB1Z0hrycBvdX0jBwlB38xKsc/edit
vizDataFrame <- vizDataFrame %>%
mutate(category = case_when(
str_detect(variable,  "counti citi night mile jail day town morn march juli") ~ "lynchings",
str_detect(variable, "law crime peopl lynch great excit state good citizen countri") ~ "critizing_lynching",
str_detect(variable, "lynch mob negro jail men hang night crowd prison attempt") ~ "negro_lynching",
str_detect(variable, "negro murder white lynch man kill year assault charg mrs") ~ "lynching_violence",
str_detect(variable, "sheriff state court juri governor order offic prison judg deputi") ~ "legal",
str_detect(variable, "bodi fire shot hang hous tree found street rope door") ~ "lynch_mob",
))
#Fact check 308597 rows tabulated
sum(articles_decades$n)
11223-9589
# plot topic proportions per decade as bar plot
ggplot(vizDataFrame, aes(x=decade, y=value, fill=category)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "decade") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values=c("#9933FF",
"#33FFFF",
"red",
"yellow",
"darkblue",
"green"))+
#                           "blue"))+
#                           #"pink",
#                           #"gray",
#                           #"orange")) +
labs(title = "Common Narratives in Lynching News Coverage",
subtitle = "Six Probable Topics in 9,589 extracted articles",
caption = "Aggregate mean topic proportions per decade. Graphic by (redated - peer review) & (redated - peer review), 6-23-2024")
#ggsave("../output/mainstrean_topics_june27.png",device = "png",width=9,height=6, dpi=2000)
ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_15_mainstrean_topics_june23_2024.png"),device = "png",width=9,height=6, dpi=800)
theta2 <- as.data.frame(theta)
cause <- theta2 %>%
#renaming for a general topic
rename(cause = '4') %>%
top_n(20, cause ) %>%
arrange(desc(cause )) %>%
select(cause )
# Apply rownames_to_column
cause  <- tibble::rownames_to_column(cause , "story_id")
cause $story_id <- gsub("X", "", cause $story_id)
head(cause$story_id, 20)
#Checks out June 23
#add categories
#Updated June 27. See notes: https://docs.google.com/document/d/1BvYAye_8biVUaJchm1PB1Z0hrycBvdX0jBwlB38xKsc/edit
vizDataFrame <- vizDataFrame %>%
mutate(category = case_when(
str_detect(variable,  "counti citi night mile jail day town morn march juli") ~ "lynchings",
str_detect(variable, "law crime peopl lynch great excit state good citizen countri") ~ "critizing_lynching",
str_detect(variable, "lynch mob negro jail men hang night crowd prison attempt") ~ "negro_lynching",
str_detect(variable, "negro murder white lynch man kill year assault charg mrs") ~ "female_victim",
str_detect(variable, "sheriff state court juri governor order offic prison judg deputi") ~ "legal",
str_detect(variable, "bodi fire shot hang hous tree found street rope door") ~ "lynch_mob",
))
# plot topic proportions per decade as bar plot
ggplot(vizDataFrame, aes(x=decade, y=value, fill=category)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "decade") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values=c("#9933FF",
"#33FFFF",
"red",
"yellow",
"darkblue",
"green"))+
#                           "blue"))+
#                           #"pink",
#                           #"gray",
#                           #"orange")) +
labs(title = "Common Narratives in Lynching News Coverage",
subtitle = "Six Probable Topics in 9,589 extracted articles",
caption = "Aggregate mean topic proportions per decade. Graphic by (redated - peer review) & (redated - peer review), 6-23-2024")
ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_15_mainstrean_topics_june23_2024.png"),device = "png",width=9,height=6, dpi=800)
