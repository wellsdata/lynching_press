summarize(sum=n)
white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
group(n) %>%
summarize(sum=n)
white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(sum=n)
white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
(sum=n)
jim_crow_total <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(total_jim_crow = sum(n))
jim_crow_total <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(total_jim_crow = sum())
View(jim_crow_total)
white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(sum=n)
white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow'))
white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(n)
jim_crow_total <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(total_jim_crow = sum(n))
jim_crow_total <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(total_jim_crow = sum(n, na.rm = TRUE))
jim_crow_count <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(total_jim_crow = sum(n, na.rm = TRUE))
white_bigrams <- read_csv("https://osf.io/download/w6ysu/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
as.data.frame()
jim_crow_count <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(total_jim_crow = sum(n, na.rm = TRUE))
jim_crow_count <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(total_jim_crow = sum(COUNT, na.rm = TRUE))
head(white_bigrams)
jim_crow_count <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(total_jim_crow = sum(n, na.rm = TRUE))
jim_crow_count <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
summarize(total_jim_crow = sum(n))
jim_crow_count <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow'))
View(jim_crow_count)
jim_crow_count1 <- jim_crow_count %>%
summarize(total_jim_crow = sum(n))
glimpse(jim_crow_count)
jim_crow_count <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
ungroup()
jim_crow_count1 <- jim_crow_count %>%
summarize(total_jim_crow = sum(n))
jim_crow_count1 <- jim_crow_count %>%
summarize(total_jim_crow = sum(n, na.rm = TRUE))
jim_crow_count1 <- jim_crow_count %>%
summarize(total_jim_crow = n())
View(jim_crow_count1)
View(jim_crow_total)
jim_crow_count1 <- jim_crow_count %>%
summarize(total_jim_crow = sum(n))
jim_crow_count1 <- jim_crow_count %>%
dplyr::summarize(total_jim_crow = sum(n))  # Specify dplyr's summarize()
library(tidyverse)
library(stringr)
white_bigrams <- read_csv("https://osf.io/download/w6ysu/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
as.data.frame()
jim_crow_count <- white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow'))
View(jim_crow_count)
jim_crow_count1 <- jim_crow_count %>%
summarize(total_jim_crow = sum(n))
View(jim_crow_count1)
total_bigrams <- white_bigrams %>%
summarize(total_bigrams = sum(n))
percentage_jim_crow <- (jim_crow_total$total_jim_crow / total_bigrams$total_bigrams) * 100
jim_crow_total <- jim_crow_count %>%
summarize(total_jim_crow = sum(n))
# Calculate the percentage
percentage_jim_crow <- (jim_crow_total$total_jim_crow / total_bigrams$total_bigrams) * 100
# Print the result
percentage_jim_crow
black_bigrams <- read_csv("https://osf.io/download/5wjzk/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
bp_jim_crow_count <- black_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>%
ungroup()
bp_jim_crow_total <- bp_jim_crow_count %>%
summarize(total_jim_crow = sum(n))
bp_total_bigrams <- black_bigrams %>%
summarize(total_bigrams = sum(n))
# Calculate the percentage
bp_percentage_jim_crow <- (bp_jim_crow_total$total_jim_crow / bp_total_bigrams$total_bigrams) * 100
bp_percentage_jim_crow
# Calculate the percentage
bp_per_10000_jim_crow <- (bp_jim_crow_total$total_jim_crow / bp_total_bigrams$total_bigrams) * 10000
bp_per_10000_jim_crow
# Calculate the percentage
per_10000_jim_crow <- (jim_crow_total$total_jim_crow / total_bigrams$total_bigrams) * 10000
# Print the result
per_100000_jim_crow
# Print the result
per_10000_jim_crow
6.3/.72
39/9
library(tidyverse)
#install.packages("sampler")
library(sampler)
#install.packages("rio")
library(rio)
#install.packages("kableExtra")
#install.packages("formattable")
library(formattable)
library(kableExtra)
library(knitr)
library(here)
#Index of 11,223 articles of text extracted from 60,000 lynching articles: 18.7% of all 60,042 search captured
extracted_articles_index_june_22_2024 <- read_csv("https://osf.io/download/z39ku/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
as.data.frame()
#60,042 Library of Congress articles on lynching captured.
index <- read_csv("https://osf.io/download/hda4v/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
#black press articles only
black_press_extracted_text_june_22_2024 <- read_csv("https://osf.io/download/t75k2/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
#Here is a chart, Figure 1, that describes lynching search results by year, counting news pages with at least one lynching story in the LOC database.
count_year <- extracted_articles_index_june_22_2024 %>%
count(year) %>%
group_by(year) %>%
#Sandwich it onto a simple ggplot
ggplot(aes(x = year, y = n, fill = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(labels = c(seq(1800, 1960, 10)), breaks = seq(1800, 1960, 10)) +
labs(title = "Lynching Coverage By Year, 1805-1963",
subtitle = "Count of Lynching Stories Examined",
caption = "n=11,223  articles. Peak page count: 1903: 482 pages. Graphic by (redacted - peer review), 10/13/2024",
y="Count of Pages",
x="Year")
ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_1_coverage_year_10-13_2024.png"),device = "png",width=9,height=6, dpi=800)
#count with pct total
count_year <- extracted_articles_index_june_22_2024 %>%
count(year) %>%
mutate(pct = formattable::percent(n/sum(n),1))
View(count_year)
#Table from Pg. 10 top 10 newspapers
total_count <- extracted_articles_index_june_22_2024 %>%
count() %>%
pull(n)
newspaper <- extracted_articles_index_june_22_2024 %>%
select(newspaper_name, newspaper_state) %>%
group_by(newspaper_name, newspaper_state) %>%
count(name = "n") %>%
mutate(pct = formattable::percent(n/total_count, 1)) %>%
arrange(desc(pct)) %>%
ungroup()
newspaper_top <- newspaper %>%
top_n(20,pct) %>%
as.data.frame()
newspaper_top
#problem with kable until I installed webshot::install_phantomjs()
#top 20 newspapers by page count
newspaper_top %>%
kable() %>%
kable_styling("striped") %>%
save_kable("../output_images_tables/top_newspapers_10_13_2024.png")
#total by state
state <- extracted_articles_index_june_22_2024 %>%
count(newspaper_state) %>%
group_by(newspaper_state) %>%
ungroup()
state <- state %>%
mutate(pct_total_pages =(n/sum(n))) %>%
arrange(desc(pct_total_pages))
state$pct_total_pages <-formattable::percent(state$pct_total_pages, 1)
state
#top states by page count
state %>%
kable() %>%
kable_styling("striped") %>%
save_kable("../output_images_tables/top_states_10_13_2024.png")
#Classification based on https://www.census.gov/programs-surveys/economic-census/guidance-geographies/levels.html#par_textimage_34
extracted_articles_index_june_22_2024 <- extracted_articles_index_june_22_2024 %>%
mutate(region = case_when(newspaper_state=="South Carolina" ~ "South",
newspaper_state=="Texas" ~ "South",
newspaper_state=="Louisiana" ~ "South",
newspaper_state=="Tennessee" ~ "South",
newspaper_state=="Mississippi" ~ "South",
newspaper_state=="Arkansas" ~ "South",
newspaper_state=="Alabama" ~ "South",
newspaper_state=="Georgia" ~ "South",
newspaper_state=="Virginia" ~ "South",
newspaper_state=="Florida" ~ "South",
newspaper_state=="North Carolina" ~ "South",
newspaper_state=="Maryland" ~ "South",
newspaper_state=="Delaware" ~ "South",
newspaper_state=="West Virginia" ~ "South",
newspaper_state=="Kentucky" ~ "South",
newspaper_state=="Missouri" ~ "Midwest",
newspaper_state=="Maine" ~ "Northeast",
newspaper_state=="New York" ~ "Northeast",
newspaper_state=="New Hampshire" ~ "Northeast",
newspaper_state=="Vermont" ~ "Northeast",
newspaper_state=="Massachusetts" ~ "Northeast",
newspaper_state=="Connecticut" ~ "Northeast",
newspaper_state=="Rhode Island" ~ "Northeast",
newspaper_state=="Pennsylvania" ~ "Northeast",
newspaper_state=="New Jersey" ~ "Northeast",
newspaper_state=="Ohio" ~ "Midwest",
newspaper_state=="Indiana" ~ "Midwest",
newspaper_state=="Kansas" ~ "Midwest",
newspaper_state=="Michigan" ~ "Midwest",
newspaper_state=="Wisconsin" ~ "Midwest",
newspaper_state=="Minnesota" ~ "Midwest",
newspaper_state=="Iowa" ~ "Midwest",
newspaper_state=="California" ~ "West",
newspaper_state=="Nevada" ~ "West",
newspaper_state=="Oregon" ~ "West",
newspaper_state=="Illinois" ~ "Midwest",
newspaper_state=="Nebraska" ~ "Midwest",
newspaper_state=="Colorado" ~ "West",
newspaper_state=="North Dakota" ~ "Midwest",
newspaper_state=="South Dakota" ~ "Midwest",
newspaper_state=="Montana" ~ "West",
newspaper_state=="Washington" ~ "West",
newspaper_state=="Idaho" ~ "West",
newspaper_state=="Wyoming" ~ "West",
newspaper_state=="Utah" ~ "West",
newspaper_state=="Oklahoma" ~ "South",
newspaper_state=="New Mexico" ~ "West",
newspaper_state=="Arizona" ~ "West",
newspaper_state=="Alaska" ~ "West",
newspaper_state=="Hawaii" ~ "West",
newspaper_state=="District of Columbia" ~ "South",))
### Border Designation
extracted_articles_index_june_22_2024 <- extracted_articles_index_june_22_2024 %>%
mutate(border = case_when(newspaper_state=="Maryland" ~ "Border",
newspaper_state=="Delaware" ~ "Border",
newspaper_state=="West Virginia" ~ "Border",
newspaper_state=="Kentucky" ~ "Border",
newspaper_state=="Missouri" ~ "Border",
.default = "Not_Border"))
#total by region
region <- extracted_articles_index_june_22_2024 %>%
group_by(region) %>%
count() %>%
ungroup()
region <- region %>%
na.omit() %>%
rename(total = n)
region <- region  %>%
mutate(pct_total_pages = round(total/sum(total),2)) %>%
mutate(pct = formattable::percent(pct_total_pages,0)) %>%
arrange(desc(pct_total_pages))
region %>%
ggplot(aes(x = region, y = pct, fill = pct)) +
geom_col(position = "dodge") +
theme(legend.position = "none", plot.subtitle = element_text(color = "blue", size = 8, face = "italic")) +
scale_y_continuous(labels = scales::percent) +
geom_text(aes(label = scales::percent(pct_total_pages)), size = 4, hjust=.5, vjust=0) +
labs(title = "Regional Distribution of Lynching Coverage, 1805-1963",
subtitle = "Newspapers by Census Region",
caption = "Newspapers by region with lynching coverage. n=11,223  articles. Graphic by (redacted - peer review), 10/13/2024",
y="Pct of Pages",
x="Region")
ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_2_regional_coverage_10_13_2024.png"),device = "png",width=9,height=6, dpi=800)
tolnay_beck <- read_csv("https://osf.io/download/vb8wa/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
# This contains
# tolnay_beck	5871 confirmed and probable
tolnay_beck %>%
count(status)
tolnay_beck <- tolnay_beck %>%
mutate(
status_clean = str_to_lower(status))
tolnay_beck$status_clean <- stringr::str_trim(tolnay_beck$status_clean)
tolnay_beck %>%
count(status_clean)
tolnay_beck <- tolnay_beck %>%
mutate(
status_clean = case_when(
status_clean == 'coincident death' ~ 'coincidental death',
status_clean == 'possiible lynching' ~ 'possible lynching',
TRUE ~ status_clean
))
install.packages('htmlTable')
library(htmlTable)
tolnay_beck$decade <- paste0(substr(tolnay_beck$year, 0, 3), "0")
tolnay_counts <- tolnay_beck %>%
count(status_clean) %>%
rename(Total = n, Type = status_clean) %>%
mutate(Percent_Total = round(Total/5871, 3)) %>%
mutate(Percent_Total = formattable::percent(Percent_Total)) %>%
arrange(desc(Total))
tolnay_decades <- tolnay_beck %>%
count(decade) %>%
rename(Total = n) %>%
mutate(Percent_Total = round(Total/5871, 3)) %>%
mutate(Percent_Total = formattable::percent(Percent_Total)) %>%
arrange(desc(Total))
#install.packages("rempsyc")
library(rempsyc)
nice_table(tolnay_counts, short = TRUE)
library(kableExtra)
kbl(tolnay_counts) %>%
kable_paper(full_width = F) %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
tolnay_counts %>%
# arrange(desc(n)) %>%
#kbl(caption = "Lynching Totals", font_size = 30) %>%
kbl(caption = "Lynching by type, Tolnay, Beck & Bailey list of 5,872 cases, 1865-2020", font_size = 24) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
#Tolnay Beck Lynch Totals 4_14_2023.png
install.packages("htmlTable")
tolnay_beck_year <- tolnay_beck %>%
group_by(year) %>%
count(year) %>%
rename(total = n) %>%
mutate(pct_total = formattable::percent(total/5871, 3))
library(tidyverse)
#install.packages("sampler")
library(sampler)
#install.packages("rio")
library(rio)
#install.packages("kableExtra")
#install.packages("formattable")
library(formattable)
library(kableExtra)
library(knitr)
library(here)
tolnay_events <- tolnay_beck %>%
select(year, status_clean) %>%
group_by(year) %>%
count(status_clean)
tolnay_events %>%
pivot_wider(names_from = status_clean, values_from = n)
View(tolnay_events)
tolnay_beck_year <- tolnay_beck %>%
group_by(year) %>%
count(year) %>%
rename(total = n) %>%
mutate(pct_total = formattable::percent(total/5871, 3))
tolnay_beck_year
lynching_year <- extracted_articles_index_june_22_2024 %>%
group_by(year) %>%
count(year) %>%
rename(total = n) %>%
mutate(pct_total = formattable::percent(total/11223, 3))
lynching_year
combo2 <- lynching_year %>%
right_join(tolnay_beck_year, by="year") %>%
filter(year <="1963")
combo2 <- combo2 %>%
rename(news_total = total.x, news_pct = pct_total.x, lynch_total = total.y, lynch_pct = pct_total.y) %>%
mutate(gap = news_pct - lynch_pct) %>%
mutate(ratio = news_total/lynch_total)
# write.csv(combo2, "../output_images_tables/news_lynching_combo2_june27_2024.csv")
#plot it
pl <- ggplot(data = combo2, aes(x = year))
pl <- pl + geom_line(aes(y=lynch_pct), colour = "blue")
pl <- pl + geom_line(aes(y=news_pct), colour = "red")
pl <- pl + scale_y_continuous(labels = scales::percent)
pl <- pl + scale_x_continuous(breaks=c(1860, 1865, 1870, 1875, 1880, 1885, 1890, 1895, 1900, 1905, 1910, 1915, 1920, 1925, 1930, 1935, 1940, 1945, 1950, 1955, 1960))
pl <- pl + theme(axis.text.x = element_text(angle=90))
pl <- pl + labs(x = "Year", y = "Pct of whole")
pl <- pl + labs(title = "Newspaper Coverage vs Lynchings, 1865-1963",
subtitle = "Newspaper Articles (Red) vs. Actual Victims (Blue), pct of whole",
caption = "Tolnay_Beck Victims n = 5039. Media n = 11,223  articles. Graphic by (redacted - peer review), 10/13/2024")
ggsave("../output_images_tables/Article_Images/Figure_5_ida_graph_10_13_2024.png", device = "png",width=9,height=6, dpi=800)
pl
#focus on the change in 1890 - 1920
combo3 <- combo2 %>%
filter(year >= "1890" & year <= "1920") %>%
select(year, news_pct, lynch_pct)
plx <- ggplot(data = combo3, aes(x = year))
plx <- plx + geom_line(aes(y=lynch_pct), colour = "blue")
plx <- plx + geom_line(aes(y=news_pct), colour = "red")
plx <- plx + scale_x_continuous(breaks=c(1890:1920))
plx <- plx + scale_y_continuous(labels = scales::percent)
plx <- plx + theme(axis.text.x = element_text(angle=90))
plx <- plx + labs(x = "Year", y = "Pct of whole")
plx <- plx + labs(title = "Newspaper Coverage vs Lynchings, 1890-1920",
subtitle = "Newspaper Articles (Red) vs. Actual Victims (Blue), pct of whole",
caption = "Tolnay_Beck Victims n = 5039. Media n = 11,223  articles. Graphic by (redacted - peer review), 10/13/2024")
ggsave("../output_images_tables/Article_Images/Figure_6_detail_ida_graph_10_13_2024.png", device = "png",width=9,height=6, dpi=800)
plx
### Load 11,223 extracted articles in a df
#303184 rows, articles span multiple rows for tokenization
extracted_text_june_22_2024 <- read_csv("https://osf.io/download/p32he/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
#subset 9589 mainstream white owned paper articles to eliminate Black newspapers
white_lynch <- extracted_text_june_22_2024 %>%
filter(black_press != "Y" | is.na(black_press))
#subset the 1634 Black press news articles
onlybptext <- extracted_text_june_22_2024 %>%
filter(black_press == "Y")
white <- extracted_articles_index_june_22_2024 %>%
filter(black_press =="N")
#303184 rows, articles span multiple rows for tokenization
extracted_text_june_22_2024 <- read_csv("https://osf.io/download/p32he/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
#subset 9590 mainstream white owned paper articles to eliminate Black newspapers
lynch <- extracted_articles_index_june_22_2024 |>
mutate(black_press = str_squish(black_press)) |>
mutate(black_press = case_when(
black_press != "Y" | is.na(black_press) ~ "N",
black_press=="Y" ~ "Y",
TRUE ~ black_press
))
lynch |>
count(black_press)
lynch1 <- lynch %>%
filter(black_press == "N")
lynch |>
count(black_press)
#graph distribution of white press by year
white_lynch %>%
count(year) %>%
ggplot(aes(x = year, y = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(labels = c(seq(1860, 1970, 10)), breaks = seq(1860, 1970, 10)) +
labs(title = "White newspaper articles",
subtitle = "Based in  9590 extracted articles, 1805-1963",
caption = "Graphic by (redacted - peer review), 10-13-2024",
y="number",
x="year")
#graph distribution of bp press by year
black_press_master_oct_11_2024 <- extracted_articles_index_june_22_2024 %>%
filter(black_press =="Y") |>
count(year) %>%
ggplot(aes(x = year, y = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(labels = c(seq(1850, 2000, 10)), breaks = seq(1850, 2000, 10)) +
labs(title = "Black newspaper articles",
subtitle = "Based in 1633 extracted articles, 1805-1963",
caption = "Graphic by (redacted - peer review), 10-13-2024",
y="number",
x="year")
#graph distribution of bp press by year
extracted_articles_index_june_22_2024 |>
filter(black_press =="Y") |>
count(year) %>%
ggplot(aes(x = year, y = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(labels = c(seq(1850, 2000, 10)), breaks = seq(1850, 2000, 10)) +
labs(title = "Black newspaper articles",
subtitle = "Based in 1633 extracted articles, 1805-1963",
caption = "Graphic by (redacted - peer review), 10-13-2024",
y="number",
x="year")
extracted_articles_index_june_22_2024 |>
count(year) %>%
ggplot(aes(x = year, y = n, fill=black_press)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(labels = c(seq(1850, 2000, 10)), breaks = seq(1850, 2000, 10)) +
labs(title = "White vs Black newspaper articles",
subtitle = "1805-1963",
caption = "Graphic by (redacted - peer review), 10-13-2024",
y="number",
x="year")
names(extracted_articles_index_june_22_2024)
extracted_articles_index_june_22_2024 |>
count(year, black_press) %>%
ggplot(aes(x = year, y = n, fill=black_press)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(labels = c(seq(1850, 2000, 10)), breaks = seq(1850, 2000, 10)) +
labs(title = "White vs Black newspaper articles",
subtitle = "1805-1963",
caption = "Graphic by (redacted - peer review), 10-13-2024",
y="number",
x="year")
extracted_articles_index_june_22_2024 |>
count(year, black_press)
extracted_articles_index_june_22_2024 |>
count(year, black_press) %>%
filter(year < 1970)
extracted_articles_index_june_22_2024 |>
count(year, black_press) %>%
filter(year < 1970) |>
ggplot(aes(x = year, y = n, fill=black_press)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(labels = c(seq(1800, 1970, 10)), breaks = seq(1800, 1970, 10)) +
labs(title = "White vs Black newspaper articles",
subtitle = "1805-1963",
caption = "Graphic by (redacted - peer review), 10-13-2024",
y="number",
x="year")
extracted_articles_index_june_22_2024 |>
count(year, black_press) %>%
filter(year < 1970) |>
ggplot(aes(x = year, y = n, fill=black_press)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(labels = c(seq(1800, 1970, 10)), breaks = seq(1800, 1970, 10)) +
labs(title = "White vs Black newspaper articles",
subtitle = "Study sample shows Black papers become significant after 1900",
caption = "n=11,223 articles. Graphic by (redacted - peer review), 10-13-2024",
y="number",
x="year")
