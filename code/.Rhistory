output_file <- file.path(output_dir, paste0("moley_extracted", i, ".txt"))
writeLines(documents[[i]], output_file)
}
cat("Files created:", length(documents), "\n")
library(tidyverse)
library(pdftools)
library(tidytext)
text <- pdf_text("moley_final.PDF")
moley_data <- data.frame(
page = seq_along(text),
text = text,
stringsAsFactors = FALSE
)
write_csv(moley_data, "moley_final.csv")
library(tidyverse)
library(tidytext)
text_stats <- moley_data %>%
summarise(
total_pages = n(),
total_characters = sum(nchar(text)),
avg_characters_per_page = mean(nchar(text))
)
key_terms <- moley_data %>%
summarise(
new_deal_mentions = sum(str_count(tolower(text), "new deal")),
reform_mentions = sum(str_count(tolower(text), "reform")),
liberalism_mentions = sum(str_count(tolower(text), "liberalism")),
conservatism_mentions = sum(str_count(tolower(text), "conservatism"))
)
cat("\nKey Term Frequencies:\n")
print(key_terms)
doc_lengths <- moley_data %>%
mutate(
word_count = str_count(text, "\\w+")
) %>%
summarise(
min_words = min(word_count),
max_words = max(word_count),
mean_words = mean(word_count),
median_words = median(word_count),
sd_words = sd(word_count)
)
cat("\nDocument Length Statistics:\n")
print(doc_lengths)
View(text_stats)
View(key_terms)
documents_df <- data.frame(documents = documents)
nrow(documents_df)
ncol(documents_df)
library(tidyverse)
library(tidytext)
new_deal_mentions <- moley_data %>%
mutate(year = str_extract(text, "19[0-9]{2}")) %>%
filter(!is.na(year)) %>%
mutate(
year = as.numeric(year),
new_deal_count = str_count(tolower(text), "new deal")
) %>%
group_by(year) %>%
summarize(frequency = sum(new_deal_count))
ggplot(new_deal_mentions, aes(x = year, y = frequency)) +
geom_point(color = "darkblue", size = 3) +
geom_smooth(method = "loess", color = "red", se = TRUE) +
theme_minimal() +
labs(
title = 'Trend in "New Deal" Mentions',
subtitle = "From 1920 to 2000",
x = "Year",
y = 'Number of "New Deal" Mentions',
caption = "Source: Moley Documents Analysis. By: Teona Goderdzishvili"
) +
theme(
plot.title = element_text(face = "bold", size = 14),
plot.subtitle = element_text(size = 12),
axis.title = element_text(face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
View(moley_data)
library(tidyverse)
library(pdftools)
library(quanteda)
library(tidytext)
library(rio)
# Set the path to your .txt file
file_path <- "../Summary_HW/ProQuestDocuments-2024-11-17.txt"
# Read the .txt file line by line
text <- readLines(file_path)
# Convert the lines into a dataframe, one line per row
gay_dv_articles <- data.frame(text)
View(gay_dv_articles)
# Step 2: Combine lines into one single string
text_combined <- paste(text, collapse = "\n")
text_combined <- sub(".*Table of contents", "Table of contents", text_combined)
# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "____________________________________________________________")[[1]]
# Step 4: Write each section to a new file
output_dir <- "../Summary_HW/article_data_dv/"
for (i in seq_along(documents)) {
output_file <- file.path(output_dir, paste0("dv_", i, ".txt"))
writeLines(documents[[i]], output_file)
}
cat("Files created:", length(documents), "\n")
moley_index <- read_lines("../Summary_HW/article_data_dv/dv_1.txt")
# Extract lines 16 to 58
extracted_lines <- moley_index[3:46]
# Print the extracted lines to the console
cat(extracted_lines, sep = "\n")
extracted_lines <- extracted_lines |>
as.data.frame()
# Step 1: Trim spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
mutate(
# Trim leading and trailing spaces before detection
trimmed_line = str_trim(extracted_lines),
# Detect titles (start with a number and a period)
is_title = str_detect(trimmed_line, "^\\d+\\. "),
# Detect dates (e.g., "Aug 14, 2024")
is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
)
# Step 2: Shift dates to align with corresponding titles
aligned_data <- cleaned_data |>
mutate(
date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
) |>
filter(is_title) |>
select(trimmed_line, date)  # Keep only the relevant columns
# Step 3: Rename columns for clarity
final_data <- aligned_data |>
rename(
title = trimmed_line,
date = date
)
#Step 4: Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")
#Step 5: Format date, clean headline
final_data <- final_data |>
mutate(date = as.Date(date2,format = "%b %d, %Y")) |>
mutate(title =str_remove(title, "^\\d+\\. ")) |>
subset(select = -(date2)) |>
mutate(index = row_number()) |>
select(index, date, title, publication)
write_csv(final_data, "../Summary_HW/dv_index_data.csv")
files <- list.files("../Summary_HW/article_data_dv/", pattern="dv.*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
#create an index with the file name
mutate(index = str_extract(filename, "\\d+")) |>
mutate(index = as.numeric(index))
final_index <- final_data |>
inner_join(files, c("index")) |>
mutate(filepath = paste0("../Summary_HW/article_data_dv/", filename))
head(final_index)
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
# Create empty tibble to store results
articles_df <- tibble()
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
lapply(row_values, create_article_text)
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
articles_df <- articles_df %>%
slice(-c(3:46)) |>
#gets rid of blank rows
filter(trimws(sentence) != "")
write.csv(articles_df, "../Summary_HW/dvlinkedindex.csv")
View(articles_df)
clean_df <- articles_df %>%
mutate(text = str_squish(sentence)) %>% #gets rid of leading and trailing spaces + double spaces
mutate(text = tolower(text)) %>%
mutate(text = str_replace(text, "startofarticle", "")) %>%
mutate(text = str_replace(text, "____________________________________________________________", "")) %>%
mutate(text = gsub("issn:\\s+\\S+", "", text)) %>%
mutate(text = gsub("http[^ ]*", "http", text)) %>%
mutate(text = str_replace_all(text, c(
"copyright" = "",
"database: proquest central" = "",
"language of publication: english" = "",
"document url:\\s+\\S+" = "",
"proquest document id:\\s+\\S+" = "",
"publication subject:\\s+\\S+" = "",
"publication date:\\s+\\S+" = "",
"publication-type:\\s+\\S+" = "",
"pages:\\s+\\S+" = "",
"publication info" = "",
"last updated:\\s+\\S+" = "",
"interest periodicals--united states" = "",
"all rights resesrved" = "",
"load-date" = "",
"all rights reserved" = "",
"new york times" = "",
"language: english" = "",
"author:\\s+\\S+" = "",
"abstract:\\s+\\S+" = "",
"links:\\s+\\S+" = "",
"publication title:\\s+\\S+" = "",
"volume:\\s+\\S+" = "",
"issue:\\s+\\S+" = "",
"publication year:\\s+\\S+" = "",
"publisher: windy city media group" = "",
"place of publication: chicago, ill." = "",
"country of publication: united states, chicago, ill." = "",
"document type:\\s+\\S+" = "",
"database: genderwatch" = "",
"identifier / keyword: y; genderwatch" = "",
"source type: newspaper" = "",
"windy city times" = "",
"umcp.primo.exlibrisgroup.com\\s+\\S+" = "",
"usmai-umcp.primo.exlibrisgroup.com" = "",
"id	doi" = "",
"Links: https://usmai-umcp.primo.exlibrisgroup.com/" = "",
"windy city media group" = "",
"city times; chicago, ill." = "",
"http" = ""
)))
View(clean_df)
clean_df <- articles_df %>%
mutate(text = str_squish(sentence)) %>% #gets rid of leading and trailing spaces + double spaces
mutate(text = tolower(text)) %>%
mutate(text = str_replace(text, "startofarticle", "")) %>%
mutate(text = str_replace(text, "____________________________________________________________", "")) %>%
mutate(text = gsub("issn:\\s+\\S+", "", text)) %>%
mutate(text = gsub("http[^ ]*", "http", text)) %>%
mutate(text = str_replace_all(text, c(
"copyright" = "",
"database: proquest central" = "",
"language of publication: english" = "",
"document url:\\s+\\S+" = "",
"proquest document id:\\s+\\S+" = "",
"publication subject:\\s+\\S+" = "",
"publication date:\\s+\\S+" = "",
"publication-type:\\s+\\S+" = "",
"pages:\\s+\\S+" = "",
"publication info" = "",
"last updated:\\s+\\S+" = "",
"interest periodicals--united states" = "",
"all rights resesrved" = "",
"load-date" = "",
"all rights reserved" = "",
"new york times" = "",
"language: english" = "",
"author:\\s+\\S+" = "",
"abstract:\\s+\\S+" = "",
"links:\\s+\\S+" = "",
"publication title:\\s+\\S+" = "",
"volume:\\s+\\S+" = "",
"issue:\\s+\\S+" = "",
"publication year:\\s+\\S+" = "",
"publisher: windy city media group" = "",
"place of publication: chicago, ill." = "",
"country of publication: united states, chicago, ill." = "",
"document type:\\s+\\S+" = "",
"database: genderwatch" = "",
"identifier / keyword: y; genderwatch" = "",
"source type: newspaper" = "",
"windy city times" = "",
"umcp.primo.exlibrisgroup.com\\s+\\S+" = "",
"usmai-umcp.primo.exlibrisgroup.com" = "",
"id	doi" = "",
"Links: https://usmai-umcp.primo.exlibrisgroup.com/" = "",
"windy city media group" = "",
"city times; chicago, ill." = "",
"http" = ""
)))
#Tokenize the data
bitoken_clean_df <- clean_df %>%
unnest_tokens(bigram, text, token="ngrams", n=2)
bigrams_separated <- bitoken_clean_df %>%
separate(bigram, c("word1", "word2"), sep = " ")
#remove stop words and
data(stop_words)
bigrams_separated <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(word1  != "https") %>%
filter(word2  != "https") %>%
filter(!grepl('[0-9]',word1)) %>%
filter(!grepl('[0-9]',word2))
View(bitoken_clean_df)
# number of lines
count_lines <- clean_df %>%
summarise(count_lines = n())
# number of lines per article
count_lines_index <- clean_df %>%
group_by(index) %>%
summarise(count_lines = n())
# number of words per article
count_df <- clean_df %>%
group_by(index) %>%
summarise(count_words = str_count(text, '\\S+')) %>%
summarise(count_lines = sum(count_words))
View(count_df)
View(count_lines)
View(count_lines_index)
head(clean_df, 30)
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bigram_ct <- bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
top_20_bigrams <- head(bigram_ct, 20)
top_20_bigrams
# bigram needs to be one column
bigram_ct_toplot <- head(bigram_ct,20) %>%
mutate(bigram = paste0(word1, " ", word2))
bi_plot <- ggplot(data=bigram_ct_toplot, aes(fill = n, reorder(bigram, n), reorder(n, bigram))) +
geom_col() +
labs(y = NULL,
x = "Bigram") +
coord_flip() +
ggtitle("Top 20 Bigrams in Articles about Domestic Violence published in the Windy City") +
theme(legend.position="none")
plot(bi_plot)
# install.packages("tidyverse")
# install.packages("rvest")
# install.packages("janitor")
library(tidyverse)
library(rvest)
library(janitor)
library(readxl)
library(ggplot2)
# Define the function to scrape a Daily Wire article
scrape_article <- function(url) {
# Read the HTML content of the page
page <- tryCatch(
read_html(url),
error = function(e) {
message(paste("Error reading:", url))
return(NULL)
}
)
# Return NULL if page could not be loaded
if (is.null(page)) return(NULL)
# Extract the headline (assuming it's in an <h1> tag)
headline <- page %>%
html_element("h1") %>%
html_text(trim = TRUE)
# Extract the article text (all <p> elements)
article_text <- page %>%
html_elements("p") %>%
html_text(trim = TRUE) %>%
paste(collapse = "\n")
# Extract the publication date (assuming it's in a <time> or meta tag)
date <- page %>%
html_element("time") %>%
html_attr("datetime") # Extract datetime attribute if available
# If <time> is not used, try a common meta tag for publication date
if (is.na(date) || is.null(date)) {
date <- page %>%
html_element("meta[property='article:published_time']") %>%
html_attr("content") # Extract content attribute
}
# If still no date, assign NA
if (is.na(date) || is.null(date)) {
date <- NA
}
# Combine headline, date, and article into one string for reference
full_text <- paste("Headline:\n", headline, "\n\nDate:\n", date, "\n\nArticle Text:\n", article_text)
return(list(full_text = full_text, date = date))
}
# Read the URLs from the Excel file
urls <- read_excel("Daily_Wire_Articles.xlsx", sheet = 1)$URL
# Specify the folder for saving articles
output_folder <- "extracted_text"
# Ensure the folder exists (create it if it doesn't)
if (!dir.exists(output_folder)) {
dir.create(output_folder)
}
# Initialize a data frame to store article metadata
article_metadata <- data.frame(
URL = character(),
Date = character(),
stringsAsFactors = FALSE
)
# Loop over URLs and save each article as a text file
for (url in urls) {
article_data <- scrape_article(url)
# Skip if article content is NULL
if (is.null(article_data)) next
# Extract the content and date
article_content <- article_data$full_text
article_date <- article_data$date
# Create a filename from the URL (sanitize to avoid invalid characters)
filename <- paste0(gsub("[^a-zA-Z0-9]", "_", basename(url)), ".txt")
# Save the article content to a text file in the specified folder
file_path <- file.path(output_folder, filename)
writeLines(article_content, con = file_path)
# Append metadata to the data frame
article_metadata <- rbind(article_metadata, data.frame(URL = url, Date = article_date))
message(paste("Saved:", file_path))
}
View(article_metadata)
urls_list <- urls %>%
as.data.frame() %>%
rename(site = 1)
View(urls_list)
# Define the function to extract text and date from a limited set of URLs in urls_list
extract_text_from_head_urls <- function(urls_list, n = 29) {
# Select the first 'n' URLs from the urls_list data frame
urls_to_process <- head(urls_list$site, n)
# Initialize an empty data frame to store results
results_df <- data.frame(
url = character(),
headline = character(),
article_text = character(),
date = character(),
stringsAsFactors = FALSE
)
# Loop through each URL in the selected URLs
for (url in urls_to_process) {
# Read the HTML content of the page
page <- tryCatch({
read_html(url)
}, error = function(e) {
message("Error reading URL: ", url)
return(NULL)
})
# Skip if page could not be read
if (is.null(page)) next
# Extract the headline
headline <- page %>%
html_element("h1") %>%
html_text(trim = TRUE)
# Extract the article text (all <p> elements)
article_text <- page %>%
html_elements("p") %>%
html_text(trim = TRUE) %>%
paste(collapse = "\n")
# Extract the publication date
date <- page %>%
html_element("time") %>%
html_attr("datetime") # Extract datetime attribute if available
# If <time> is not used, try a common meta tag for publication date
if (is.na(date) || is.null(date)) {
date <- page %>%
html_element("meta[property='article:published_time']") %>%
html_attr("content") # Extract content attribute
}
# If still no date, assign NA
if (is.na(date) || is.null(date)) {
date <- NA
}
# Append the results to the data frame
results_df <- results_df %>%
add_row(url = url, headline = headline, article_text = article_text, date = date)
}
return(results_df)
}
# Example usage with your urls data frame
# Assuming urls_list has a column 'site' containing the URLs
# urls_list <- data.frame(site = c("https://example.com/article1", "https://example.com/article2"))
compiled_text_df <- extract_text_from_head_urls(urls_list, n = 29)
# View the compiled results
print(compiled_text_df)
View(compiled_text_df)
WordCount <- compiled_text_df %>%
select(article_text) %>%
mutate(WordCount = str_count(article_text, "\\w+")) %>%
arrange(desc(WordCount))
WordCount
ggplot(WordCount, aes(x = WordCount)) +
geom_histogram(binwidth = 50, fill = "blue", color = "black") +
labs(
title = "Distribution of Word Counts in Articles",
x = "Word Count",
y = "Frequency"
) +
theme_minimal()
View(WordCount)
WordCount <- compiled_text_df %>%
select(article_text, date) %>%
mutate(WordCount = str_count(article_text, "\\w+")) %>%
arrange(desc(WordCount))
WordCount
View(WordCount)
nrow(compiled_text_df)
ncol(compiled_text_df)
# Create the histogram
ggplot(compiled_text_df, aes(x = date)) +
geom_bar(binwidth = 7, fill = "blue", color = "black") + # Weekly bins
labs(
title = "Distribution of Articles about Kamala Harris Over Time",
caption =  "Graphics by Nini Mtchedlishvili",
x = "Date",
y = "Number of Articles"
) +
theme_minimal()+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(tidyverse)
tolnay_beck <- read_csv("https://osf.io/download/vb8wa/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
# This contains
# tolnay_beck	5871 confirmed and probable
View(tolnay_beck)
