---
title: "Main Article Graphics"
author: "(redacted for peer review)"
date: "2024-10-13"
output:
  word_document: default
  pdf_document: default
---


Anonymized data and code stored at OSF: https://osf.io/7kpr4/?view_only=6c106acd6cb54f6f849e8c6f9098809f

This notebook has the code for many graphics (or links to them) for "Lynching and the Press, 1805-1963."	


```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
#install.packages("sampler")
library(sampler)
#install.packages("rio")
library(rio)
#install.packages("kableExtra")
#install.packages("formattable")
library(formattable)
library(kableExtra)
library(knitr)
library(here)
```

Next, we import an index of lynching coverage -- 60,042 pages -- captured by these search terms.

# Import Data
```{r}
#Index of 11,194 articles of text extracted from 60,042 lynching articles: 18.6% of all 60,042 search captured

#11,194 article index, with 9,590 predominantly white press and 1,605 black press
extracted_articles_index_oct_16_2024 <- read.csv("https://osf.io/download/uxw3a/?view_only=6c106acd6cb54f6f849e8c6f9098809f")

#black press only index, 1,604 articles
black_index_master_oct_16_2024 <- read.csv("https://osf.io/download/egkqu/?view_only=6c106acd6cb54f6f849e8c6f9098809f")


#60,042 Library of Congress articles on lynching captured.
index <- read_csv("https://osf.io/download/hda4v/?view_only=6c106acd6cb54f6f849e8c6f9098809f")

#298,564 rows, for 11,194 articles span multiple rows for tokenization
extracted_text_oct_16_2024 <- read_csv("https://osf.io/download/gw5dk/?view_only=6c106acd6cb54f6f849e8c6f9098809f")

#black press extracted text: 1,604 articles
black_press_extracted_text_oct_16_2024 <- read_csv("https://osf.io/download/95a3v/?view_only=6c106acd6cb54f6f849e8c6f9098809f")


```

# Sample 
A stratified random sample by year using proportional allocation.

```{r}
#sample code: ssampcalc(df, n, strata, over=0)

x <- ssampcalc(index, n=60042, strata=year, over=0.5)
x <- janitor::clean_names(x)

x$decade <- paste0(substr(x$year, 0, 3), "0")

library(kableExtra)

sample_decade <- x %>% 
  group_by(decade) %>% 
  count() %>% 
  ungroup() %>% 
  mutate(pct_total = n/(sum(n))) %>% 
  mutate(pct_total = formattable::percent(pct_total,2)) %>% 
  rename(total = n) %>% 
  kable() %>%
  kable_styling("striped") %>%
  column_spec(1, width = ".5em") %>%
  column_spec(2, width = "1em") %>%
  column_spec(3, width = "1em") %>% 
  add_header_above(c("LOC Lynching Articles by Decade, n=60,042" = 3)) %>% 
  save_kable("../output_images_tables/sample_decade_6_26_2024.png")

#See at OSF: https://osf.io/download/mpydr/?view_only=None

#Fact check
#sum(x$nh)
#nh is the total pages per year
#wt[,1] is the percentage of the total corpus (60042 pages)
#Shows peak media coverage of lynching activity between 1893-1910
#Peak year for coverage was 1903, with 2971 pages and 4.94% of all pages


x <- ssampcalc(index, n=60042, strata=year, over=0.5)
x <- janitor::clean_names(x)
x

```

# Figure 1: Analyzing the Lynching Data

```{r}

#Here is a chart, Figure 1, that describes lynching search results by year, counting news pages with at least one lynching story in the LOC database.
count_year <- extracted_articles_index_oct_16_2024 %>% 
count(year) %>% 
   group_by(year) %>% 
#Sandwich it onto a simple ggplot
  ggplot(aes(x = year, y = n, fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  scale_x_continuous(labels = c(seq(1800, 1960, 10)), breaks = seq(1800, 1960, 10)) +
  labs(title = "Lynching Coverage By Year, 1805-1963", 
       subtitle = "Count of Lynching Stories Examined",
       caption = "n=11,194  articles. Peak page count: 1903: 482 pages. Graphic by (redacted - peer review), 10/16/2024",
       y="Count of Pages",
       x="Year")

ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_1_coverage_year_10-16_2024.png"),device = "png",width=9,height=6, dpi=800)

```

```{r}
#count with pct total
count_year <- extracted_articles_index_oct_16_2024 %>% 
  count(year) %>% 
  mutate(pct = formattable::percent(n/sum(n),1)) 
  
```


Here is the list of all newspapers we captured. I'm just supplying the top 20 results.
### Total by newspaper
```{r}

#Table from Pg. 10 top 10 newspapers
total_count <- extracted_articles_index_oct_16_2024 %>%
  count() %>%
  pull(n)

newspaper <- extracted_articles_index_oct_16_2024 %>% 
  select(newspaper_name, newspaper_state) %>% 
   group_by(newspaper_name, newspaper_state) %>% 
   count(name = "n") %>% 
  mutate(pct = formattable::percent(n/total_count, 1)) %>% 
  arrange(desc(pct)) %>% 
  ungroup()


newspaper_top <- newspaper %>% 
  top_n(20,pct) %>% 
  as.data.frame()

newspaper_top

#problem with kable until I installed webshot::install_phantomjs()

#top 20 newspapers by page count
newspaper_top %>%
  kable() %>%
  kable_styling("striped") %>%
  save_kable("../output_images_tables/top_newspapers_10_16_2024.png")

```

### Total publications by state
```{r}
#total by state
state <- extracted_articles_index_oct_16_2024 %>% 
count(newspaper_state) %>% 
   group_by(newspaper_state) %>% 
  ungroup()

state <- state %>% 
  mutate(pct_total_pages =(n/sum(n))) %>% 
  arrange(desc(pct_total_pages))
         
 
state$pct_total_pages <-formattable::percent(state$pct_total_pages, 1)
state

#top states by page count
state %>%
  kable() %>%
  kable_styling("striped") %>%
  save_kable("../output_images_tables/top_states_10_16_2024.png")
```


### Regional classification for newspaper

```{r}
#Classification based on https://www.census.gov/programs-surveys/economic-census/guidance-geographies/levels.html#par_textimage_34
extracted_articles_index_oct_16_2024 <- extracted_articles_index_oct_16_2024 %>% 
  mutate(region = case_when(newspaper_state=="South Carolina" ~ "South",
                           newspaper_state=="Texas" ~ "South",
                            newspaper_state=="Louisiana" ~ "South",
                            newspaper_state=="Tennessee" ~ "South",
                            newspaper_state=="Mississippi" ~ "South",
                            newspaper_state=="Arkansas" ~ "South",
                            newspaper_state=="Alabama" ~ "South",
                            newspaper_state=="Georgia" ~ "South",
                            newspaper_state=="Virginia" ~ "South",
                            newspaper_state=="Florida" ~ "South",
                            newspaper_state=="North Carolina" ~ "South",
                            newspaper_state=="Maryland" ~ "South",
                            newspaper_state=="Delaware" ~ "South",
                            newspaper_state=="West Virginia" ~ "South",
                            newspaper_state=="Kentucky" ~ "South",
                            newspaper_state=="Missouri" ~ "Midwest",
                            newspaper_state=="Maine" ~ "Northeast",
                            newspaper_state=="New York" ~ "Northeast",
                            newspaper_state=="New Hampshire" ~ "Northeast",
                            newspaper_state=="Vermont" ~ "Northeast",
                            newspaper_state=="Massachusetts" ~ "Northeast",
                            newspaper_state=="Connecticut" ~ "Northeast",
                            newspaper_state=="Rhode Island" ~ "Northeast",
                            newspaper_state=="Pennsylvania" ~ "Northeast",
                            newspaper_state=="New Jersey" ~ "Northeast",
                            newspaper_state=="Ohio" ~ "Midwest",
                            newspaper_state=="Indiana" ~ "Midwest",
                            newspaper_state=="Kansas" ~ "Midwest",
                            newspaper_state=="Michigan" ~ "Midwest",
                             newspaper_state=="Wisconsin" ~ "Midwest",
                             newspaper_state=="Minnesota" ~ "Midwest",
                             newspaper_state=="Iowa" ~ "Midwest",
                             newspaper_state=="California" ~ "West",
                             newspaper_state=="Nevada" ~ "West",
                             newspaper_state=="Oregon" ~ "West",
                            newspaper_state=="Illinois" ~ "Midwest",
                            newspaper_state=="Nebraska" ~ "Midwest",
                            newspaper_state=="Colorado" ~ "West",
                            newspaper_state=="North Dakota" ~ "Midwest",
                            newspaper_state=="South Dakota" ~ "Midwest",
                            newspaper_state=="Montana" ~ "West",
                            newspaper_state=="Washington" ~ "West",
                            newspaper_state=="Idaho" ~ "West",
                            newspaper_state=="Wyoming" ~ "West",
                            newspaper_state=="Utah" ~ "West",
                            newspaper_state=="Oklahoma" ~ "South",
                            newspaper_state=="New Mexico" ~ "West",
                            newspaper_state=="Arizona" ~ "West",
                            newspaper_state=="Alaska" ~ "West",
                            newspaper_state=="Hawaii" ~ "West",
                            newspaper_state=="District of Columbia" ~ "South",))

```


```{r}
### Border Designation
extracted_articles_index_oct_16_2024 <- extracted_articles_index_oct_16_2024 %>% 
  mutate(border = case_when(newspaper_state=="Maryland" ~ "Border",
                            newspaper_state=="Delaware" ~ "Border",
                            newspaper_state=="West Virginia" ~ "Border",
                            newspaper_state=="Kentucky" ~ "Border",
                            newspaper_state=="Missouri" ~ "Border",
                               .default = "Not_Border"))

```


# Figure 2: State-region totals

```{r}

#total by region
region <- extracted_articles_index_oct_16_2024 %>% 
  group_by(region) %>% 
  count() %>% 
  ungroup()

region <- region %>% 
  na.omit() %>% 
  rename(total = n) 


region <- region  %>% 
   mutate(pct_total_pages = round(total/sum(total),2)) %>% 
  mutate(pct = formattable::percent(pct_total_pages,0)) %>% 
  arrange(desc(pct_total_pages))


region %>% 
  ggplot(aes(x = region, y = pct, fill = pct)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none", plot.subtitle = element_text(color = "blue", size = 8, face = "italic")) +
    scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label = scales::percent(pct_total_pages)), size = 4, hjust=.5, vjust=0) + 
  labs(title = "Regional Distribution of Lynching Coverage, 1805-1963", 
       subtitle = "Newspapers by Census Region",
       caption = "Newspapers by region with lynching coverage. n=11,194  articles. Graphic by (redacted - peer review), 10/16/2024",
       y="Pct of Pages",
       x="Region")



ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_2_regional_coverage_10_16_2024.png"),device = "png",width=9,height=6, dpi=800)
```



### Tolnay Beck Bailey Victim Data - Compare Trends with News Coverage

The University of Illinois holds the Tolnay_Bailey_Victim_Data of all known lynching cases from 1882-1929.

\#<https://uofi.app.box.com/s/ffmqd2rjxrdt1tvxl38d/file/110041209555>

There are 2,249 lynching cases in this database, again from 1882-1929

There is also the Tolnay, Beck & Bailey list of 5,872 cases, 1865-2020, probable and confirmed lynchings.
https://app.box.com/s/99ggc6epn4rdvritke0h/file/992017683748

**Our newspaper research reaches back much earlier than this dataset.** 

**Below, we compare the frequency of news coverage to the Tolnay victims list.**

```{r}
tolnay_beck <- read_csv("https://osf.io/download/vb8wa/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>% 
  as.data.frame()

tolnay_beck <- janitor::clean_names(tolnay_beck)
# This contains
# tolnay_beck	5871 confirmed and probable	

```

```{r}
tolnay_beck %>% 
  count(status)

```

```{r}
tolnay_beck <- tolnay_beck %>% 
  mutate(
    status_clean = str_to_lower(status)) 
  

tolnay_beck$status_clean <- stringr::str_trim(tolnay_beck$status_clean)

tolnay_beck %>% 
  count(status_clean) 
```

```{r}
tolnay_beck <- tolnay_beck %>% 
  mutate(
    status_clean = case_when(
      status_clean == 'coincident death' ~ 'coincidental death',
      status_clean == 'possiible lynching' ~ 'possible lynching',
      TRUE ~ status_clean
    ))

#install.packages('htmlTable')
library(htmlTable)

tolnay_beck$decade <- paste0(substr(tolnay_beck$year, 0, 3), "0")

tolnay_counts <- tolnay_beck %>% 
 count(status_clean) %>%
  rename(Total = n, Type = status_clean) %>% 
  mutate(Percent_Total = round(Total/5871, 3)) %>% 
  mutate(Percent_Total = formattable::percent(Percent_Total)) %>% 
  arrange(desc(Total))

tolnay_decades <- tolnay_beck %>% 
 count(decade) %>% 
  rename(Total = n) %>% 
  mutate(Percent_Total = round(Total/5871, 3)) %>% 
  mutate(Percent_Total = formattable::percent(Percent_Total)) %>% 
  arrange(desc(Total))

#install.packages("rempsyc")
library(rempsyc)

nice_table(tolnay_counts, short = TRUE)

library(kableExtra)
kbl(tolnay_counts) %>%
  kable_paper(full_width = F) %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em", background = "yellow")


tolnay_counts %>%
  # arrange(desc(n)) %>% 
 #kbl(caption = "Lynching Totals", font_size = 30) %>%
  kbl(caption = "Lynching by type, Tolnay, Beck & Bailey list of 5,872 cases, 1865-2020", font_size = 24) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em", background = "yellow") 

#Tolnay Beck Lynch Totals 4_14_2023.png
 
```

```{r}
tolnay_events <- tolnay_beck %>% 
  select(year, status_clean) %>% 
group_by(year) %>% 
  count(status_clean)

tolnay_events %>% 
  pivot_wider(names_from = status_clean, values_from = n)



```
# Figure 3: Lynching Totals, Tolnay & Beck, 2022

```{r}

tolnay_beck <- tolnay_beck %>% 
  mutate(
    status_clean = case_when(
      status_clean == 'coincident death' ~ 'coincidental death',
      status_clean == 'possiible lynching' ~ 'possible lynching',
      TRUE ~ status_clean
    ))

tolnay_counts <- tolnay_beck %>% 
  count(status_clean) %>% 
  mutate(pct_total = round(n/5871, 3)) %>% 
  mutate(pct_total = formattable::percent(pct_total, 1)) |> 
  rename(Category = status_clean, Number = n, Pct_Total = pct_total)  

library(rempsyc)
nice_table(tolnay_counts, short = TRUE)

library(kableExtra)
kbl(tolnay_counts) %>%
  kable_paper(full_width = F) %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em", background = "yellow")


tolnay_graphic <- tolnay_counts %>%
  arrange(desc(Number)) %>% 
  kbl(caption = "Lynching Totals, Tolnay & Beck, 2022", font_size = 30) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em", background = "yellow") 

library(webshot)
# Save the kable table as an image
temp_html <- tempfile(fileext = ".html")
save_kable(tolnay_graphic, file = temp_html)
img_file <- tempfile(fileext = ".png")
webshot(temp_html, file = img_file, zoom = 2) # Adjust zoom to control resolution

# Convert the image to a ggplot object
img <- grid::rasterGrob(png::readPNG(img_file), interpolate = TRUE)

# Save the ggplot object as a high-resolution image
ggsave("tolnay_graphic.png", plot = ggplot() + annotation_custom(img, xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf), dpi = 800, width = 10, height = 10)
```

# Figure 4: Lynching Totals graphic over time

```{r}

tolnay_beck %>% 
  filter(status_clean =="lynching") |> 
count(year) %>% 
   group_by(year) %>% 
#Sandwich it onto a simple ggplot
  ggplot(aes(x = year, y = n, fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  scale_x_continuous(labels = c(seq(1860, 2020, 10)), breaks = seq(1860, 2020, 10)) +
  labs(title = "Actual Lynchings, 1865-2020", 
       subtitle = "Count of Actual Lynchings. Tolnay-Beck Data",
       caption = "n=5,039 lynchings. Graphic by (redacted - peer review), 10/11/2024",
       y="Count",
       x="Year")
#Actual Lynchings Tolnay 4_14_2023
ggsave("../output_images_tables/Article_Images/Figure_4_tolnay_graphic_10_16_2024.png", dpi = 800, width = 10, height = 10)
```



Year Totals - seguin_tolnay
```{r}
tolnay_beck_year <- tolnay_beck %>% 
  group_by(year) %>% 
  count(year) %>% 
  rename(total = n) %>% 
  mutate(pct_total = formattable::percent(total/5871, 3))
tolnay_beck_year
```

Year Totals - Lynching Articles

```{r}
lynching_year <- extracted_articles_index_oct_16_2024 %>% 
  group_by(year) %>% 
  count(year) %>% 
  rename(total = n) %>% 
  mutate(pct_total = formattable::percent(total/11194, 3))
lynching_year
```

We capture a stratified sample by year of the tolnay_beck victims.

It shows peak lynching activity between 1884-1895 and a significant drop off after 1922.

Our newspaper research has broadly similar findings.

# Figure 5: Ida Wells Graphic 

```{r}
combo2 <- lynching_year %>% 
  right_join(tolnay_beck_year, by="year") %>% 
  filter(year <="1963")

combo2 <- combo2 %>% 
  rename(news_total = total.x, news_pct = pct_total.x, lynch_total = total.y, lynch_pct = pct_total.y) %>% 
  mutate(gap = news_pct - lynch_pct) %>% 
  mutate(ratio = news_total/lynch_total)

# write.csv(combo2, "../output_images_tables/news_lynching_combo2_june27_2024.csv")

#plot it
pl <- ggplot(data = combo2, aes(x = year))
pl <- pl + geom_line(aes(y=lynch_pct), colour = "blue")
pl <- pl + geom_line(aes(y=news_pct), colour = "red")
pl <- pl + scale_y_continuous(labels = scales::percent)
pl <- pl + scale_x_continuous(breaks=c(1860, 1865, 1870, 1875, 1880, 1885, 1890, 1895, 1900, 1905, 1910, 1915, 1920, 1925, 1930, 1935, 1940, 1945, 1950, 1955, 1960)) 
pl <- pl + theme(axis.text.x = element_text(angle=90)) 
pl <- pl + labs(x = "Year", y = "Pct of whole")
pl <- pl + labs(title = "Newspaper Coverage vs Lynchings, 1865-1963", 
        subtitle = "Newspaper Articles (Red) vs. Actual Victims (Blue), pct of whole",
       caption = "Tolnay_Beck Victims n = 5039. Media n = 11,194  articles. Graphic by (redacted - peer review), 10/16/2024")

ggsave("../output_images_tables/Article_Images/Figure_5_ida_graph_10_16_2024.png", device = "png",width=9,height=6, dpi=800)

pl
```

#Figure 6: Detail ida graph 10 16 2024

```{r}
#focus on the change in 1890 - 1920
combo3 <- combo2 %>% 
  filter(year >= "1890" & year <= "1920") %>% 
  select(year, news_pct, lynch_pct)

plx <- ggplot(data = combo3, aes(x = year))
plx <- plx + geom_line(aes(y=lynch_pct), colour = "blue")
plx <- plx + geom_line(aes(y=news_pct), colour = "red")
plx <- plx + scale_x_continuous(breaks=c(1890:1920)) 
plx <- plx + scale_y_continuous(labels = scales::percent)
plx <- plx + theme(axis.text.x = element_text(angle=90)) 
plx <- plx + labs(x = "Year", y = "Pct of whole")
plx <- plx + labs(title = "Newspaper Coverage vs Lynchings, 1890-1920", 
        subtitle = "Newspaper Articles (Red) vs. Actual Victims (Blue), pct of whole",
        caption = "Tolnay_Beck Victims n = 5039. Media n = 11,194  articles. Graphic by (redacted - peer review), 10/16/2024")

ggsave("../output_images_tables/Article_Images/Figure_6_detail_ida_graph_10_16_2024.png", device = "png",width=9,height=6, dpi=800)

plx
```


### Counts of Words and Articles

```{r}
### Load 11,194 extracted articles in a df

#subset 9590 mainstream white owned paper articles to eliminate Black newspapers
white_lynch <- extracted_text_oct_16_2024 %>% 
    filter(black_press == "N")

#subset the 1604 Black press news articles
onlybptext <- extracted_text_oct_16_2024 %>% 
  filter(black_press == "Y")


```


```{r}
#graph distribution of white press by year
white_lynch %>% 
  count(year) %>% 
   ggplot(aes(x = year, y = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  scale_x_continuous(labels = c(seq(1800, 1970, 10)), breaks = seq(1800, 1970, 10)) +
  labs(title = "White newspaper articles",
       subtitle = "Based in  9590 extracted articles, 1805-1963",
       caption = "Graphic by (redacted - peer review), 10-16-2024",
       y="number",
       x="year")


```

```{r}
#graph distribution of bp press by year
onlybptext |> 
  count(year) %>% 
   ggplot(aes(x = year, y = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  scale_x_continuous(labels = c(seq(1850, 1970, 10)), breaks = seq(1850, 1970, 10)) +
  labs(title = "Black newspaper articles",
       subtitle = "Based in 1604 extracted articles, 1805-1963",
       caption = "Graphic by (redacted - peer review), 10-16-2024",
       y="number",
       x="year")


```

#Figure 7: White v Black paper count in sample
```{r}
extracted_articles_index_oct_16_2024 |> 
  count(year, black_press) %>% 
  filter(year < 1970) |> 
  ggplot(aes(x = year, y = n, fill=black_press)) +
  geom_col(position = "dodge") + 
  scale_fill_manual(values = c("Y" = "red", "N" = "blue")) +
  theme(legend.position = "none") +
  scale_x_continuous(labels = c(seq(1800, 1970, 10)), breaks = seq(1800, 1970, 10)) +
  labs(title = "White vs Black newspaper articles",
       subtitle = "Black papers gain significance in study sample after 1900",
       caption = "Red bars: Black press sample. Blue: white papers. n=11,194 articles. Graphic by (redacted - peer review), 10-16-2024",
       y="number",
       x="year")

ggsave("../output_images_tables/Article_Images/Figure_7_black_white_sample_years_10_16_2024.png", device = "png",width=9,height=6, dpi=800)
```

### Building lynch DF - All Stories: Count words by story
```{r}
xxx <- stringi::stri_count_words(extracted_text_oct_16_2024$sentence, "\\w+") %>%
  as.data.frame() %>%
  rename(words = ".")
# 
lynch <- cbind(extracted_text_oct_16_2024, xxx)
# 
y <- lynch %>% 
   select(file_id, sentence, words, year, newspaper_name, url)
# 
# # append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")


##Average Word Count in Lynching News Coverage by decade
z <- y %>% 
  select(file_id, newspaper_name, url, words, decade) %>% 
  group_by(file_id, decade) %>% 
  summarize(total=sum(words))

mean(z$total, na.rm=TRUE)

#average words per decade and total article count
lynch_word_decade <- z %>% 
  select(decade, total) %>% 
  group_by(decade) %>% 
  summarise(
    avg_words = mean(total, na.rm = TRUE),
    num_articles = n()
  ) %>% 
  mutate(avg_words = round(avg_words, 0)) %>% 
   filter(!decade>1960)
```
# Figure 8 Avg_word_count

```{r}
lynch_word_decade %>% 
  ggplot(aes(x = decade, y = avg_words,fill = avg_words)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "Average Word Count in Lynching News Coverage",
       subtitle = "Based in 11,194 extracted articles, 1805-1963",
       caption = "Graphic by (redacted - peer review), 10-16-2024",
       y="Average Article Word Count",
       x="Decade")

ggsave("../output_images_tables/Article_Images/Figure8_avg_word_count_10_16_2024.png",device = "png",width=9,height=6, dpi=800)


```


### Black press word count by decade

```{r}
b <- stringi::stri_count_words(black_press_extracted_text_oct_16_2024$sentence, "\\w+") %>%
    as.data.frame() %>%
    rename(words = ".")

black_press_extracted_text_oct_16_2024 <- cbind(black_press_extracted_text_oct_16_2024, b)

bb <- black_press_extracted_text_oct_16_2024 %>% 
   select(file_id, sentence, words, year, newspaper_name, url)
# 
# # append decade information for aggregation
bb$decade <- paste0(substr(bb$year, 0, 3), "0")

##Average Word Count in Black Press Lynching News Coverage by decade
zz <- bb %>% 
  select(file_id, newspaper_name, url, words, decade) %>% 
  group_by(file_id, decade) %>% 
  summarize(total=sum(words))

mean(zz$total, na.rm=TRUE)
#[1] 309.5641
#Average word count of Black owned newspapers was 310 words

#average words per decade and total article count
bp_lynch_word_decade <- zz %>% 
  select(decade, total) %>% 
  group_by(decade) %>% 
  summarise(
    avg_words = mean(total, na.rm = TRUE),
    num_articles = n()
  ) %>% 
  mutate(avg_words = round(avg_words, 0)) %>% 
   filter(!decade>1960)

```

### White press word count by decade

```{r}
#subset 9590 mainstream white owned paper articles to eliminate Black newspapers
white_press <- extracted_text_oct_16_2024  %>% 
    filter(black_press == "N")

w <- stringi::stri_count_words(white_press$sentence, "\\w+") %>%
    as.data.frame() %>%
    rename(words = ".")

white_press <- cbind(white_press, w)

ww <- white_press %>% 
   select(file_id, sentence, words, year, newspaper_name, url)
# 
# # append decade information for aggregation
ww$decade <- paste0(substr(ww$year, 0, 3), "0")

##Average Word Count in white Press Lynching News Coverage by decade
www <- ww %>% 
  select(file_id, newspaper_name, url, words, decade) %>% 
  group_by(file_id, decade) %>% 
  summarize(total=sum(words))

mean(www$total, na.rm=TRUE)
#Average word count of white owned newspapers was 138 words (137.5727)

#average words per decade and total article count
w_lynch_word_decade <- www %>% 
  select(decade, total) %>% 
  group_by(decade) %>% 
  summarise(
    avg_words = mean(total, na.rm = TRUE),
    num_articles = n()
  ) %>% 
  mutate(avg_words = round(avg_words, 0)) %>% 
   filter(!decade>1960)

```
#Figure 9: Page Placement

```{r}
extracted_articles_index_oct_16_2024$decade <- paste0(substr(extracted_articles_index_oct_16_2024$year, 0, 3), "0")

pageplacement <- extracted_articles_index_oct_16_2024 %>% 
  # mutate(page = str_replace(page, "seq-", "")) %>% 
  group_by(page) %>% 
  count(page) %>% 
  ungroup()

pageplacement <- pageplacement %>% 
  mutate(pct =(n/sum(n)))


pageplacement$pct <-formattable::percent(pageplacement$pct, 1)


pageplacement %>% 
  top_n(10,pct) %>% 
  mutate(page = reorder(page, -pct)) %>%
  ggplot(aes(x = page, y = pct, fill = pct)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  #scale_x_continuous(breaks=c(1:15)) +
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label= pct, x= page, y= pct), hjust=.5, vjust=0) +
  labs(title = "Page Placement, Lynching Coverage, 1800-1963", 
       subtitle = "Page Number Placement of Lynching Stories",
       caption = "Page 1 stories were 34% of 11,194  articles. Graphic by (redcated) , 10/16/2024",
       y="Pct of Pages",
       x="Page Number")

ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_9_page_placement_10_16_2024.png"),device = "png",width=9,height=6, dpi=800)
```




#Page One Analysis
Next, we count the number of articles by page number. About 27% of all lynching articles were on Page One.

```{r}
#uses the main index of 11,194  extracted articles

black_index_master_oct_16_2024$decade <- paste0(substr(black_index_master_oct_16_2024$year, 0, 3), "0")

bp_pageplacement <- black_index_master_oct_16_2024 %>% 
  # mutate(page = str_replace(page, "seq-", "")) %>% 
  group_by(page) %>% 
  count(page) %>% 
  ungroup() %>% 
  mutate(pct =(n/sum(n))) %>% 
  mutate(pct = formattable::percent(pct, 1)) 


#bp page 1 by decade
bp_index_pages <- black_index_master_oct_16_2024 %>% 
  mutate(page_one = ifelse(page > 1, FALSE, TRUE))

bp_pages_decade <- bp_index_pages %>% 
  group_by(page_one, decade) %>%
  count(page_one) %>% 
   ungroup()


bp_pages_decade2 <- bp_pages_decade %>%
  group_by(decade, page_one) %>%
  summarise(n = sum(n)) %>%
  mutate(percentage = round(n / sum(n) * 100, 1)) 


```
# Figure 10: Page 1 stories by decade

```{r}
#Using the 11,194  extracted articles, we determine the number of page 1 stories per decade. 

extracted_articles_index_oct_16_2024$decade <- paste0(substr(extracted_articles_index_oct_16_2024$year, 0, 3), "0")

x_extracted_articles_index_oct_16_2024 <- extracted_articles_index_oct_16_2024 %>% 
  select(newspaper_name, newspaper_state, sn, year, month, day, decade, edition,page, filepath)

index_pages <- x_extracted_articles_index_oct_16_2024 %>% 
  mutate(page_one = ifelse(page > 1, FALSE, TRUE))

pages_decade <- index_pages %>% 
  group_by(page_one, decade) %>%
  count(page_one) %>% 
   ungroup()


pages_decade <- pages_decade %>%
  group_by(decade, page_one) %>%
  summarise(n = sum(n)) %>%
  mutate(percentage = round(n / sum(n) * 100, 1)) 

#write.csv(pages_decade, "../output/pages_decade_6_18.csv")

pages_decade %>% 
  filter(!is.na(page_one)) %>%
  #filter(decade > "1820") %>% 
  filter(decade < "1970") %>% 
  #remove outliers
  mutate(page = case_when(
    str_detect(page_one, "TRUE") ~ "PageOne",
    str_detect(page_one, "FALSE") ~ "Inside")) %>% 
  ggplot(aes(x = decade, y = percentage, fill = page)) +
  geom_col(position = "dodge") + 
  scale_fill_manual(values = c("PageOne" = "red", "Inside" = "lightblue")) + 
  labs(title = "Percentage Page One Lynching Stories, 1830-1960", 
       subtitle = "Page One Stories By Decade",
       caption = "Page 1 lynching stories peaked at 45% in the 1920s. n=11,194 articles. Graphic by (redacted - peer review), 10/16/2024",
       y="Pct of Pages",
       x="Decade")


ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_10_pages_decade_10_16_2024.png"),device = "png",width=9,height=6, dpi=800)


```

### All pages by decade

```{r}
extracted_articles_index_oct_16_2024$decade <- paste0(substr(extracted_articles_index_oct_16_2024$year, 0, 3), "0")

count_decade <- extracted_articles_index_oct_16_2024 %>% 
  group_by(decade) %>%
  count() %>% 
    ungroup() %>% 
  mutate(pct_total = n/(sum(n))) %>% 
  mutate(pct_total = formattable::percent(pct_total,2)) %>% 
  rename(total = n) %>% 
  kable() %>%
  kable_styling("striped", full_width = F, position = "left") %>%
  column_spec(1, width = "0.001em") %>%
  column_spec(2, width = "0.001em") %>%
  column_spec(3, width = "0.001em") %>% 
  add_header_above(c("Lynching Articles by Decade, n=11,194 " = 3)) %>% 
  save_kable("../output_images_tables/viewed_sample_decade_10_16_2024.png")
```

##Mob analysis in white papers

```{r}

mob_cleaned <- read_csv("../output_images_tables/mob_cleaned_10_20_2024.csv")

mob_summary <- mob_cleaned %>% 
  group_by(decade) %>% 
  summarize(
    total_hostile = sum(hostile, na.rm = TRUE),
    total_neutral = sum(neutral, na.rm = TRUE),
    total_orderly = sum(orderly, na.rm = TRUE),
    total_justice = sum(justice, na.rm = TRUE),
    total_identified = sum(identified, na.rm = TRUE),
    total_unsure = sum(unsure, na.rm = TRUE),
    grand_total = n() 
    ) %>% 
        mutate(
    percent_hostile = round(total_hostile / grand_total * 100),
    percent_neutral = round(total_neutral / grand_total * 100),
    percent_orderly = round(total_orderly / grand_total * 100),
    percent_justice = round(total_justice / grand_total * 100),
    percent_identified = round(total_identified / grand_total * 100),
    percent_unsure = round(total_unsure / grand_total * 100)
  )

write.csv(mob_summary, "/Users/robwells/Code/lynching_press/output_images_tables/mob_summary_10_20_2024.csv")

```

# Figure 11: Mob coverage, characterization by decade Analyzing by Decade

```{r}

# Reshape the data to long format
mob_long <- mob_summary %>%
  select(decade,percent_hostile,percent_orderly, percent_neutral) |> 
  filter(decade > 1820) |> 
  pivot_longer(
    cols = starts_with("percent_"), 
    names_to = "story_type", 
    values_to = "percentage"
  ) 

# Plot with side-by-side bars
mob_long %>%
  ggplot(aes(x = factor(decade), y = percentage, fill = story_type)) +
  geom_col(position = "dodge", stat = "identity") +
    geom_text(
    data = subset(mob_long, story_type == "percent_neutral"),  # Filter only neutral bars
    aes(label = paste0(percentage, "%")), 
 #   position = position_dodge(width = 0.5),  # Align with bars
 #   vjust = -0.1,
   hjust = -.3,# Slightly above the bar
    angle = 90      # Rotate the text 90 degrees
  ) +
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  scale_fill_manual(
    values = c("percent_hostile" = "red", 
               "percent_neutral" = "blue",
                "percent_orderly" = "#458B00"),
    labels = c("Hostile", "Neutral", "Orderly")
  ) +
  labs(
    title = "Lynch mobs often portrayed neutrally in white newspapers",
    subtitle = "Percent of white newspaper coverage portrayals of lynch mobs",
    caption = "1800s removed, lack of data. n=3,147 articles. Graphic by (redacted - peer review), 10/20/2024",
    y = "Pct of Pages",
    x = "Decade",
    fill = "Story Type"
  ) 


ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_11_mob_coverage_10_20_2024.png"),device = "png",width=9,height=6, dpi=800)

```


#Figure 12: Lynching Expected. (1902, July 23). Alexandria Gazette. 



#Figure 13: Newspaper, lynch mobs and justice narrative. 


```{r}

# Reshape the data to long format
justice <- mob_summary %>%
  select(decade,percent_justice) |> 
  filter(decade > 1820) |> 
  pivot_longer(
    cols = starts_with("percent_"), 
    names_to = "story_type", 
    values_to = "percentage"
  ) 

# Plot with side-by-side bars
justice %>%
  ggplot(aes(x = factor(decade), y = percentage, fill = percentage)) +
  geom_col(position = "dodge", stat = "identity") +
    geom_text(
    data = subset(justice, story_type == "percent_justice"),  
    aes(label = paste0(percentage, "%")), 
 #   position = position_dodge(width = 0.5),  # Align with bars
    vjust = -0.1,
   hjust = .5,# Slightly above the bar
    #angle = 90      # Rotate the text 90 degrees
  ) +
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  theme(legend.position = "none") +
  labs(
    title = "Lynching as 'justice' drops after 1830s in white newspapers",
    subtitle = "Predominantly white news coverage equating lynch mobs with justice",
    caption = "1800s removed, lack of data. n=3,147 articles. Graphic by (redacted - peer review), 10/20/2024",
    y = "Pct of Pages",
    x = "Decade") 


ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_13_mob_justice_10_20_2024.png"),device = "png",width=9,height=6, dpi=800)

```


#Figure 14: Common narratives in Black press coverage of lynching

in Topic Modeling_10_19_2024.rmd, Line 687

#Figure 15 mainstrean_topics_june27.png

in Topic Modeling_10_19_2024.rmd, Line 368


#Figure 16: Compare White / Black Article Counts

```{r}
wp_bp_comparison <- extracted_articles_index_oct_16_2024 |> 
  mutate(black_press = str_squish(black_press)) |>
  mutate(black_press = case_when(
    black_press != "Y" | is.na(black_press) ~ "N",
    black_press=="Y" ~ "Y",
    TRUE ~ black_press
  )) |>
  mutate(decade = paste0(substr(year, 1, 3), "0")) |> 
  select(decade, file_id, black_press) |> 
  group_by(decade) |> 
  count(black_press) |> 
  pivot_wider(names_from = black_press, values_from = n) |> 
  rename(White_count = N, Black_count = Y, Decade = decade) |> 
  mutate(Black_pct = round(Black_count / (Black_count + White_count)*100,1))

library(kableExtra)
wp_bp_comparison |> 
 kable() %>%
  kable_styling("striped", full_width = F, position = "left") %>%
  column_spec(1, width = "0.001em") %>%
  column_spec(2, width = "0.001em") %>%
  column_spec(3, width = "0.001em") %>% 
  column_spec(4, width = "0.001em") %>% 
  add_header_above(c("White v. Black Press Article Sample, n=11,194 " = 4)) %>% 
  save_kable("../output_images_tables/Article_Images/figure_16_wp_bp_sample_decade_10_19_2024.png")

```

#Figure 16: Common two-word phrases in Black, white press by decade, 1900-1930s. 

### See Bigrams_7_2_2024.rmd for details on how the bigrams were created

```{r}

black_bigrams <- read_csv("https://osf.io/download/5wjzk/?view_only=6c106acd6cb54f6f849e8c6f9098809f") 


black_bigrams %>%
  filter(n >= 5) %>%
  filter(decade == "1930s") %>% 
  filter(str_detect(word1, 'mob') & str_detect(word2, 'violence'))


black_bigrams %>%
  filter(n >= 5) %>%
  filter(str_detect(word1, 'anti') & str_detect(word2, 'lynching'))

black_bigrams <- black_bigrams %>%
  na.omit()

bigrams_for_viz <- black_bigrams %>%
  mutate(black_bigrams, x = paste(word1, word2)) 

# comparing to the white press

white_bigrams <- read_csv("https://osf.io/download/w6ysu/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>% 
  as.data.frame()


jim_crow_count <- white_bigrams %>%
  filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>% 
  ungroup()

jim_crow_total <- jim_crow_count %>%
  summarize(total_jim_crow = sum(n))


# Calculate the total occurrences of all bigrams
total_bigrams <- white_bigrams %>%
  summarize(total_bigrams = sum(n))

# Calculate the percentage
per_10000_jim_crow <- (jim_crow_total$total_jim_crow / total_bigrams$total_bigrams) * 10000

# Print the result
per_10000_jim_crow
#Jim Crow was .72 per 100000 white bigrams


#for bp
bp_jim_crow_count <- black_bigrams %>%
  filter(str_detect(word1, 'jim') & str_detect(word2, 'crow')) %>% 
  ungroup()

bp_jim_crow_total <- bp_jim_crow_count %>%
  summarize(total_jim_crow = sum(n))


# Calculate the total occurrences of all bigrams
bp_total_bigrams <- black_bigrams %>%
  summarize(total_bigrams = sum(n))

# Calculate the percentage
bp_percent_10000_jim_crow <- (bp_jim_crow_total$total_jim_crow / bp_total_bigrams$total_bigrams) * 100

bp_per_10000_jim_crow 

#6.3 per 10,000 phrases

6.3/.72

#jim crow was 8.75 times more frequent in Black press than white press
```

#white_bigrams 

```{r}
white_bigrams %>%
  filter(str_detect(word1, 'civil') & str_detect(word2, 'rights'))

white_bigrams %>%
  filter(decade == "1930") %>%
  filter(str_detect(word1, 'anti') & str_detect(word2, 'lynching'))

white_bigrams_for_viz <- white_bigrams %>%
  mutate(white_bigrams, x = paste(word1, word2)) 

white_bigrams %>%
  filter(n >= 5) %>%
  filter(decade == "1930") %>%
  filter(str_detect(word1, 'mob') & str_detect(word2, 'violence'))

black_bigrams %>%
  filter(decade == '1910s')

black_bigrams %>%
  filter(decade == '1920s')

black_bigrams %>%
  filter(decade == '1930s')

black_bigrams %>%
  filter(decade == '1940s')

white_bigrams %>%
  filter(str_detect(word1, 'mob') & str_detect(word2, 'violence'))

black_bigrams %>%
  filter(str_detect(word1, 'jim') & str_detect(word2, 'crow'))

white_bigrams %>%
  filter(str_detect(word1, 'anti') & str_detect(word2, 'lynching'))

black_bigrams %>%
  filter(decade == "1930s") %>%
  filter(str_detect(word1, 'anti') & str_detect(word2, 'lynching'))

black_bigrams %>%
  filter(str_detect(word1, 'civil') & str_detect(word2, 'rights'))

```

#Black Press: Count words by story
```{r}
black <- bp_text

xx <- stringi::stri_count_words(black$sentence, "\\w+") %>% 
  as.data.frame() %>% 
  rename(words = ".")

black <- cbind(black, xx)

#65257 rows only the bp files. 
onlybptext <- filter(black, grepl("bp", filename))
#write.csv(onlybptext,("../data/only_bp_text.csv"))

yy <- black %>% 
  select(filename, sentence, words, year)

# append decade information for aggregation
yy$decade <- paste0(substr(yy$year, 0, 3), "0")


zz <- yy %>% 
  select(filename, words, decade) %>% 
  group_by(filename, decade) %>% 
  summarize(total=sum(words))

#Count words by decade
black_word_decade <- zz %>% 
  select(decade, total) %>% 
  group_by(decade) %>% 
    summarise(
    avg_words = mean(total, na.rm = TRUE),
    num_articles = n() )%>% 
  mutate(avg_words = round(avg_words, 0)) 

library(kableExtra)
# black_articles_decade %>%
  black_word_decade %>%
  kbl(caption = "Black Press Article Totals", font_size = 30) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em", background = "yellow") 

#1045 black press articles. join 714 articles + 358 LOC articles
blackindex_master <- read.csv("../data/blackindex_master.csv")

#Count articles by year
black_articles <- blackindex_master %>% 
  select(filename, year) %>% 
  group_by(year) %>%
  count()


black_articles_decade <- zz %>% 
  select(decade, filename) %>% 
  group_by(decade) %>%
  count() %>% 
  mutate(Pct_Total =formattable::percent(round(n/1045,2)))

library(kableExtra)
black_articles_decade %>%
  kbl(caption = "Black Press Article Totals", font_size = 30) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em", background = "yellow") 

```

# BP word count by decade
```{r}
black_word_decade %>% 
  ggplot(aes(x = decade, y = avg_words,fill = avg_words)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "Black Press Avg Word Count in Lynching News Coverage",
       subtitle = "Based in 1,045 extracted articles, 1892-2002",
       caption = "Graphic by (redacted - peer review), 12-26-2023",
       y="Average Article Word Count",
       x="Decade")

#ggsave("../output_images_tables/FigureX_black_press_avg_word_count_ap_19.png",device = "png",width=9,height=6, dpi=800)


```

# Regular Expression Search Terms

Here is the list and formatting of regular expression terms used to search the Chronicling America API.

```{r}

"lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?colored"    "(murderer|fiend|desperado|brute)\W+((\w+\W+){1,2})?lynch(ed|es|ing)?(\W+|$)"    "coloreds?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?negro"    "mob\W+((\w+\W+){1,2})?(hung|hang(ed|ings?|s)|lynch(ed|es|ing)?)"    "negro(e?s)?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?colored"_num_matches    "(murderer|fiend|desperado|brute)\W+((\w+\W+){1,2})?lynch(ed|es|ing)?(\W+|$)"_num_matches    "coloreds?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_num_matches    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?negro"_num_matches    "mob\W+((\w+\W+){1,2})?(hung|hang(ed|ings?|s)|lynch(ed|es|ing)?)"_num_matches    "negro(e?s)?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_num_matches    Unnamed: 0    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?colored"_start_idx    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?colored"_end_idx    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?colored"_cost    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?colored"_SEARCH_cost_threshold    "coloreds?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_start_idx    "coloreds?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_end_idx    "coloreds?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_cost    "coloreds?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_SEARCH_cost_threshold    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?negro"_start_idx    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?negro"_end_idx    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?negro"_cost    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?negro"_SEARCH_cost_threshold    "negro(e?s)?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_start_idx    "negro(e?s)?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_end_idx    "negro(e?s)?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_cost    "negro(e?s)?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_SEARCH_cost_threshold    "mob\W+((\w+\W+){1,2})?(hung|hang(ed|ings?|s)|lynch(ed|es|ing)?)"_start_idx    "mob\W+((\w+\W+){1,2})?(hung|hang(ed|ings?|s)|lynch(ed|es|ing)?)"_end_idx    "mob\W+((\w+\W+){1,2})?(hung|hang(ed|ings?|s)|lynch(ed|es|ing)?)"_cost    "(murderer|fiend|desperado|brute)\W+((\w+\W+){1,2})?lynch(ed|es|ing)?(\W+|$)"_start_idx    "(murderer|fiend|desperado|brute)\W+((\w+\W+){1,2})?lynch(ed|es|ing)?(\W+|$)"_end_idx    "(murderer|fiend|desperado|brute)\W+((\w+\W+){1,2})?lynch(ed|es|ing)?(\W+|$)"_cost    "mob\W+((\w+\W+){1,2})?(hung|hang(ed|ings?|s)|lynch(ed|es|ing)?)"_SEARCH_cost_threshold    "(murderer|fiend|desperado|brute)\W+((\w+\W+){1,2})?lynch(ed|es|ing)?(\W+|$)"_SEARCH_cost_threshold    batch    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?colored"_CLEANING_cost_threshold    "coloreds?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_CLEANING_cost_threshold    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?negro"_CLEANING_cost_threshold    "negro(e?s)?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_CLEANING_cost_threshold    "mob\W+((\w+\W+){1,2})?(hung|hang(ed|ings?|s)|lynch(ed|es|ing)?)"_CLEANING_cost_threshold    "(murderer|fiend|desperado|brute)\W+((\w+\W+){1,2})?lynch(ed|es|ing)?(\W+|$)"_CLEANING_cost_threshold    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?colored"_passed_cleaning    "coloreds?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_passed_cleaning    "lynchings?\W+of\W+(the\W+)?((\w+\W+){1,2})?negro"_passed_cleaning    "negro(e?s)?\W+((\w+\W+){1,2})?((was|were)\W+)?lynch(ed|es|ing)?(\W+|$)"_passed_cleaning    "mob\W+((\w+\W+){1,2})?(hung|hang(ed|ings?|s)|lynch(ed|es|ing)?)"_passed_cleaning    "(murderer|fiend|desperado|brute)\W+((\w+\W+){1,2})?lynch(ed|es|ing)?(\W+|$)"_passed_cleaning

```




    --30--
