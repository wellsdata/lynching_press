---
title: "Indexes and Lists"
author: "(redacted for peer review)"
date: "2024-06-17"
output: html_document
---


```{r}
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("tigris")
#install.packages("zoo")
library(tigris)
library(stringr)
library(janitor)
library(zoo)
library(lubridate)
library(readtext)

```

#master_article_index_6_18
June 17 data cleaning and compiling
master_article_index_6_17 has 11,396 articles
--1650 from the black press


```{r}
#Index of 11,223 articles of text extracted from 60,000 lynching articles

master_article_index_june_22_2024 <- read_csv("https://osf.io/download/hzyw6/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
  as.data.frame()

#60,042 Library of Congress articles on lynching captured.
main_index <- read_csv("https://osf.io/download/hda4v/?view_only=6c106acd6cb54f6f849e8c6f9098809f")

```


```{r}
# 1633 from the black press
# from master_article_index_6_17 has 11,223 articles
# June 22 data cleaning and compiling

black_press_master_june_22_2024 <- master_article_index_june_22_2024 %>% 
  filter(black_press =="Y")

#write.csv(black_press_master_june_22_2024, "../data/blackindex_master_june_18_2024.csv")

black_press_master_june_22_2024 %>% 
  count(newspaper_name) %>% 
  arrange(desc(n))

```

# Black press index from Chronicling America
```{r}
#243 papers from the 59,967 paper search
black_papers <- read_csv("../data/black_papers.csv") %>% 
  rename(newspaper_name = Title) %>% 
  as.data.frame()

black_papers <- black_papers %>% 
  mutate(black = "Y")

```


# Black Press articles for tokenization
```{r}

black_articles_text_june_18_2024 <- read.csv("../data/extracted_text_june_22_2024.csv")%>% 
  filter(black_press =="Y")

write.csv(black_articles_text_june_18_2024, "../data/black_articles_text_june_18_2024.csv")
```




# Counting the years
```{r}
years_6_18 <- master_index_june_18_2024 %>% 
  count(year) %>% 
  rename(count = n) %>% 
  mutate(pct_whole = round(count/sum(count)*100,2)) %>% 
  arrange(desc(pct_whole)) %>% 
  mutate(rank_new = dense_rank(desc(pct_whole)))

#master_article_index_10_19

old_master_article_index_10.19 <- read.csv("../data/old_article_indexes_lists/master_article_index_10.19.csv")
years_10_19 <- old_master_article_index_10.19 %>% 
  count(year) %>% 
  rename(count = n) %>% 
  mutate(pct_whole = round(count/sum(count)*100,2)) %>% 
  arrange(desc(pct_whole)) %>% 
  rename(pct_whole_old = pct_whole, count_old = count) %>%   mutate(rank_old = dense_rank(desc(pct_whole_old)))


#join and compare dfs


year_compare <- years_10_19 %>% 
  inner_join(years_6_18, by="year") %>% 
  mutate(diff = (count-count_old)) %>% 
  mutate(pct_chg = round(count-count_old)/count_old*100) %>% 
  mutate(pct_chg = round(pct_chg,2))

## fact checking years new vs old
year_compare %>% 
  filter(count_old > count)

#lost five articles in 1851, 1 in 1846. That's it
#Biggest gains in 1871, 1873, 1872, 1917, 1915, 1916

         
#NY, MA, no difference; PA, 1; DE, 6; ME, 7; SC, 8
#Biggest changes were IN, AZ, IA, CO, ID, CT, AL, A, HI, SC, AK

```



# Counting the states
```{r}
totals_6_18 <- master_index_june_18_2024 %>% 
  count(newspaper_state) %>% 
  rename(count = n) %>% 
  mutate(pct_whole = round(count/sum(count)*100,2)) %>% 
  arrange(desc(pct_whole)) %>% 
  mutate(rank_new = dense_rank(desc(pct_whole)))

#Il (8%), AL, IA , IN (5%), AR (4.6%), AZ, VA, OH, WI, GA top newspaper states

totals_10_19 <- old_master_article_index_10.19 %>% 
  count(newspaper_state) %>% 
  rename(count_old = n) %>% 
  mutate(pct_whole = round(count_old/sum(count_old)*100,2)) %>% 
  arrange(desc(pct_whole)) %>% 
  mutate(rank_new = dense_rank(desc(pct_whole)))

```

## join and compare dfs


```{r}
compare <- totals_6_18 %>% 
  inner_join(totals_10_19, by="newspaper_state") %>% 
  mutate(diff = (count-count_old)) %>% 
  mutate(pct_chg = round(count-count_old)/count_old*100) %>% 
  mutate(pct_chg = round(pct_chg,2))
         

#Biggest changes were AZ, IN, IA, CO, ID, AL, CT, CA, HI
#No difference; PA, WV, WY, NY, DE, SC, MA
```


### count of new pre-civil war stories

```{r}
#724 pre-civil war articles
year_compare %>% 
filter(year < "1862") %>% 
summarize(sum(count))


#69 new pre-civil war articles extracted.
year_compare %>% 
filter(year < "1862") %>% 
summarize(sum(diff))


```

# Full Library of Congress Newspaper List
```{r}


#Code from Khushboo Rathore
download_loc <- "../data/newspapers_list.txt"
download.file("https://chroniclingamerica.loc.gov/newspapers.txt", download_loc)

newspaper_list <- read_delim(download_loc, delim = "|")

clean_newspaper_list <- newspaper_list %>% 
  clean_names() %>% 
  select(state, title, lccn, oclc, issn, no_of_issues, first_issue_date, last_issue_date) %>% 
  rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date) %>% 
  mutate_all(str_squish) %>% 
  mutate(earliest_issue = lubridate::mdy(earliest_issue)) %>% 
  mutate(latest_issue = lubridate::mdy(latest_issue))

#write_csv(clean_newspaper_list, "../data/newspaper_list.csv")
```

# Printing Hate Main Index of Newspapers

```{r}
#new_main_index <- read.csv("../data/mainindex_10_30.csv")
#full_list_clean <- read.csv("../data/main_index_with_names_111323.csv")
#dec_28 with cleaned names, all but 1,170 newspapers named
full_list_clean <- read.csv("../data/main_index_dec_28_2023.csv")

```


# Tolnay and Beck Lynching Inventory
```{r}

tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>% 
  as.data.frame()

tolnay_beck <- janitor::clean_names(tolnay_beck)

```


# Text data

```{r}


#152766 rows of text data for tokenization (6448 predominantly white press articles)
articles_oct_19 <- read.csv("../data/articles_oct_19.csv")
#Consists of this folder
lynch1 <- readtext("~/Code/hcij_lynching_phase_two/articles_10_19")

#76020 rows of BP text data for tokenization 
bp_text <- read.csv("../data/black_press_article_text_oct_19.csv")
#from this folder
bp_lynch <- readtext("~/Code/hcij_lynching_phase_two/black_press_all")

#text for just the 714 bp articles
onlybptext <- read.csv("../data/only_bp_text.csv")

```



#---------------------------------------------------------
#Notes - Cleaning the extracted article index June 17 2024
#---------------------------------------------------------

### Index cleaning - June 18 2024
```{r}
master_article_index_6_17 <- master_article_index_6_17 %>% 
  mutate(file_id_all = paste(file_id2, file_name.y)) %>% 
  mutate(file_id_all = str_replace(file_id_all, "NA", "")) %>% 
  subset(select = -c(file_name.x, X, file_name.y, idcol,index, abstract, store_id))


master_article_index_6_17 <- master_article_index_6_17 %>% 
  rename(index = X.1, file_id_old = file_id, month = month.x, file_id = file_id_all, file_id2_old = file_id2)

master_article_index_6_17 <- master_article_index_6_17 %>%
  select( index,  file_id, newspaper_name,  date, date2,  year, month,  day, url,  article_id,sn, newspaper_city,  newspaper_state,  black_press,  black,  pages, page,edition,mod_id, seq,   article_title, collection,authors,  document_type,issn,find_a_copy,  start_page,  filepath, file_id_old, file_id2_old, url_backup )

master_article_index_6_17 <- master_article_index_6_17 %>%
  subset(select = -c(date)) %>% 
  rename(date = date2)

write.csv(master_article_index_6_17, "../data/master_article_index_6_17.csv")

#replace missing newspaper names
newspaper_list <- read.csv("../data/newspaper_list.csv")

newslist1 <- newspaper_list %>% 
  select(state, newspaper, lccn)

master_article_index_XX_6_17 <- master_article_index_6_17 %>% 
  left_join(newslist1, by=c("sn"="lccn"))

#clean newspaper name, extract city and state
test <- master_article_index_XX_6_17 %>% 
  mutate(newspaper = str_replace(newspaper, "\\[volume\\]", "")) %>% 
  mutate(newspaper = gsub("\\d", "", newspaper)) %>% 
   mutate(city_state = str_extract(newspaper, "\\([^\\)]+\\)"),
         city_state = str_replace_all(city_state, "\\(|\\)", ""),
         city = str_split_fixed(city_state, ", ", 2)[, 1],
         state = str_split_fixed(city_state, ", ", 2)[, 2]) %>%
  select(-city_state)


#replace blank city and new entries
test <- test %>% 
  mutate(newspaper_city = coalesce(newspaper_city, city)) %>% 
  mutate(newspaper_city = str_replace(newspaper_city, "[[:punct:]]", ""))

#clean newspaper name
library(tidyr)
test <- separate(data = test, col = newspaper, into = c("newspaper1", "crap"), sep = "\\(", extra = "merge", fill = "right")

test <- test %>% 
  mutate(newspaper_name2 = coalesce(newspaper1, newspaper_name)) %>% 
  mutate(newspaper_name2 = str_replace(newspaper_name2, "[[:punct:]]", "")) 

test <- test %>% 
    mutate(newspaper_name2 = gsub("\\d", "", newspaper_name2)) 


test$newspaper_name2 <- str_replace_all(test$newspaper_name2, "[[:punct:]]", "")

test <- test %>% 
  subset(select = -c(crap, newspaper_name, newspaper1, city)) %>% 
  rename(newspaper_name = newspaper_name2) %>% 
  mutate(black_press = coalesce(black, black_press))

test <- test %>% 
  subset(select = -c(black)) 

test1 <- test %>% 
  distinct(file_id, .keep_all = TRUE)


master_index_june_18_2024 <- test1 %>% 
  select( index,  file_id, newspaper_name,  date, year, month,  day, url,  article_id,sn, newspaper_city,  newspaper_state,  black_press,  pages, page,edition,mod_id, seq,   article_title, collection,authors,  document_type,issn,find_a_copy,  start_page,  filepath, file_id_old, file_id2_old, url_backup)

#fixing pages

master_article_index_6_17 <- master_article_index_6_17 %>%
  mutate(page = str_replace(page, "seq-", "")) %>% 
  mutate(page = coalesce(page, pages)) %>% 
  mutate(page = str_replace(page, "A", "")) %>% 
  mutate(start_page = str_replace(start_page, "A", "")) %>% 
  mutate(page2 = coalesce(page, start_page)) %>% 
  rename(page_old = page, page = page2) %>% 
    subset(select = -c(pages, X, seq, start_page, page_old)) 

master_article_index_6_17 %>% 
  count(page) %>% 
  arrange(desc(n))

write.csv(master_article_index_6_17, "../data/master_article_index_june_18_2024.csv")

```

##Fix URL
```{r}
#create date field
extracted_article_index_6_16 <- extracted_article_index_6_16 %>% 
  mutate(date = paste(year, month, day, sep="-")) %>% 
  mutate(date = ymd(date)) %>% 
  mutate(day2 = case_when(
    day==1 ~ "01",
    day==2 ~ "02",
    day==3 ~ "03",
    day==4 ~ "04",
    day==5 ~ "05",
    day==6 ~ "06",
    day==7 ~ "07",
    day==8 ~ "08",
    day==9 ~ "09",
    TRUE ~ as.character(day)
    ))
#strip url down to sn

extracted_article_index_6_16$url_fixed <- sub("sn\\d+/\\K.*", "", extracted_article_index_6_16$URL, perl = TRUE)


extracted_article_index_6_16$date2 <- str_replace_all(extracted_article_index_6_16$date1, pattern=fixed('-'), replacement=fixed('/') )

#paste it back, delete old
extracted_article_index_6_16 <- extracted_article_index_6_16 %>% 
  mutate(url_final = paste(url_fixed, date2, edition, page, "0?user_id=6", sep="/")) %>% 
    subset(select = -c(url_fixed,date1,day2, X))

#clean up
extracted_article_index_6_16 <- extracted_article_index_6_16 %>% 
  rename(url = url_final, url_backup = URL)

#june 17 2024
write_csv(extracted_article_index_6_16, "../data/extracted_article_index_6_16.csv")



```

# Geocoded articles
#Analysis
```{r}
#2783 articles all geocoded
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
```




```{r}
ga_lynchings <- tolnay_beck %>% 
  filter()
```

