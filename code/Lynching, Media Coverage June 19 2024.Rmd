---
title: "Sampling Lynching Coverage"
author: "(redacted - peer review)"
date: "2024-6-19"
output:
  word_document: default
  pdf_document: default
---

# Sample of Lynching Coverage

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
#install.packages("sampler")
library(sampler)
#install.packages("rio")
library(rio)
#install.packages("kableExtra")
#install.packages("formattable")
library(formattable)
library(kableExtra)
library(knitr)
library(here)
#install.packages('htmlTable')
library(htmlTable)
#install.packages("rempsyc")
library(rempsyc)
here::here('/Users/robwells/Code/lynching_press')
```

Next, we import an index of lynching coverage -- 60,042 pages -- captured by these search terms.

# Import Data
```{r}

#60,000 Library of Congress articles on lynching captured.
index <- read_csv("https://osf.io/download/hda4v/?view_only=6c106acd6cb54f6f849e8c6f9098809f")

#Index of 11,223 articles of text extracted from 60,000 lynchin articles

index2 <- read_csv("https://osf.io/download/hzyw6/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>%
  as.data.frame()

# 1633 articles from the black press
# from master_article_index_6_17 has 11,223 articles

blackindex_master <- read_csv("https://osf.io/download/8jbw5/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
```

Index of all Chronicling America coverage.

```{r}
#1789-1963, 18.1 million rows, all Chroniciling America as of 6/2023

#Due to the file size (1.7gb), you will need to download this file to your hard drive and import it in R from your local machine: https://osf.io/download/3kpqa/?view_only=6c106acd6cb54f6f849e8c6f9098809f


index_chron <- rio::import("/Users/YOUR_USER_NAME_HERE/Downloads/chron_am_manifest.csv")

index_chron <- janitor::clean_names(index_chron)

```


## Count years for entries 
```{r}
#Count years for Chroniciling America 
chron_count <- index_chron %>% 
count(year) %>% 
  mutate(pct = formattable::percent(n/(sum(n)))) %>% 
  rename(chron_all = n, chron_pct = pct)

#write.csv(chron_count, "../output_images_tables/chronicle_count.csv")

#Count years for all lynching articles captured
lynch_count <- index %>% 
  count(year) %>% 
  mutate(pct = formattable::percent(n/(sum(n)))) %>% 
  rename(lynch_news = n, lynch_pct = pct)

combolynch_chron <- chron_count %>% 
  inner_join(lynch_count, by="year")


```


## Viz total and lynch papers
```{r}

#Comparing total page count to the pages with lynching news

ggplot(combolynch_chron, aes(x=year)) +
  geom_line(aes(y= chron_pct), size=2, color = "red") +
  geom_line(aes(y=(lynch_pct)), size=1, color = "blue") +
  scale_y_continuous(
    name = "All Chroniciling America (Red)",
    sec.axis = sec_axis(~.*1, name="Lynching News in Chron America (Blue)")
  ) +
  scale_x_continuous(labels = c(seq(1790, 1960, 10)), breaks = seq(1790, 1960, 10)) +
  labs(title = "Lynching News to Overall News Coverage, 1789-1963",
       caption= "Comparing All Chron America to Lynching News in Chron America as Pct of All Coverage.
       Graphic by (redacted - peer review)",
       x = "Year")

```

### Comparing the 60,000 search to extracted articles
## Count years for entries 
```{r}
#Count years for extracted articles
article_count <- index2 %>% 
count(year) %>% 
  mutate(pct = formattable::percent(n/(sum(n)))) %>% 
  rename(article_all = n, article_pct = pct)

#write.csv(chron_count, "../output_images_tables/chronicle_count.csv")

#Count years for all lynching articles captured
lynch_count <- index %>% 
  count(year) %>% 
  mutate(pct = formattable::percent(n/(sum(n)))) %>% 
  rename(loc_count = n, loc_pct = pct)

combo_2_lynch_chron <- article_count %>% 
  inner_join(lynch_count, by="year")


```


## Viz total and lynch papers
```{r}

#Comparing total page count to the pages with lynching news

ggplot(combo_2_lynch_chron, aes(x=year)) +
  geom_line(aes(y= article_pct), size=2, color = "red") +
  geom_line(aes(y=(loc_pct)), size=1, color = "blue") +
  scale_y_continuous(
    name = "Extracted Articles (11,223 articles in LOC) (Red)",
    sec.axis = sec_axis(~.*1, name="Search Results Chron America (Blue)")
  ) +
  scale_x_continuous(labels = c(seq(1805, 1960, 10)), breaks = seq(1805, 1960, 10)) +
  labs(title = "Extracted Article Sample to Total Search Results, 1805-1963",
       caption= "Red line is the 19% of lynching articles actually extracted. The blue line is total search for lynching in Chronicling America.
       Graphic by (redacted - peer review). June 23, 2024",
       x = "Year")

```



### Figure 1: Analyzing the Lynching Data

```{r}

#Here is a chart, Figure 1, that describes lynching search results by year, counting news pages with at least one lynching story in the LOC database.
count_year <- index2 %>% 
count(year) %>% 
   group_by(year) %>% 
#Sandwich it onto a simple ggplot
  ggplot(aes(x = year, y = n, fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  scale_x_continuous(labels = c(seq(1800, 1960, 10)), breaks = seq(1800, 1960, 10)) +
  labs(title = "Lynching Coverage By Year, 1805-1963", 
       subtitle = "Count of Lynching Stories Examined",
       caption = "n=11,223 pages. Peak page count: 1903: 482 pages. Graphic by (redacted - peer review), 6/23/2024",
       y="Count of Pages",
       x="Year")

ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_1_coverage_year_6_21_2024.png"),device = "png",width=9,height=6, dpi=800)

```


### Figure 9: Page 1 stories by decade


```{r}
#Using the 11,223 extracted articles, we determine the number of page 1 stories per decade. 

index2$decade <- paste0(substr(index2$year, 0, 3), "0")

index2<- index2 %>% 
  select(newspaper_name, newspaper_state, sn, year, month, day, decade, edition,page, filepath)

index_pages <- index2 %>% 
  mutate(page_one = ifelse(page > 1, FALSE, TRUE))

pages_decade <- index_pages %>% 
  group_by(page_one, decade) %>%
  count(page_one) %>% 
   ungroup()


pages_decade <- pages_decade %>%
  group_by(decade, page_one) %>%
  summarise(n = sum(n)) %>%
  mutate(percentage = round(n / sum(n) * 100, 1)) 

#write.csv(pages_decade, "../output/pages_decade_6_18.csv")

pages_decade %>% 
  filter(!is.na(page_one)) %>%
  filter(decade > "1820") %>% 
  filter(decade < "1970") %>% 
  #remove outliers
  mutate(page = case_when(
    str_detect(page_one, "TRUE") ~ "PageOne",
    str_detect(page_one, "FALSE") ~ "Inside")) %>% 
  ggplot(aes(x = decade, y = percentage, fill = page)) +
  geom_col(position = "dodge") + 
  scale_fill_manual(values = c("PageOne" = "red", "Inside" = "lightblue")) + 
  labs(title = "Percentage Page One Lynching Stories, 1830-1960", 
       subtitle = "Page One Stories By Decade",
       caption = "Page 1 lynching stories peaked at 45% in the 1920s. Graphic by (redacted - peer review), 6/23/2024",
       y="Pct of Pages",
       x="Decade")


ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_9_pages_decade_6_23_2024.png"),device = "png",width=9,height=6, dpi=800)


```

Here is the list of all newspapers we captured. I'm just supplying the top 20 results.
### Total by newspaper

```{r}

#Table from Pg. 10 top 10 newspapers
total_count <- index2 %>%
  count() %>%
  pull(n)

newspaper <- index2 %>% 
  select(newspaper_name, newspaper_state) %>% 
   group_by(newspaper_name, newspaper_state) %>% 
   count(name = "n") %>% 
  mutate(pct = formattable::percent(n/total_count, 1)) %>% 
  arrange(desc(pct)) %>% 
  ungroup()


newspaper_top <- newspaper %>% 
  top_n(20,pct) %>% 
  as.data.frame()

newspaper_top

#problem with kable until I installed webshot::install_phantomjs()

#top 20 newspapers by page count
newspaper_top %>%
  kable() %>%
  kable_styling("striped") %>%
  save_kable("../output_images_tables/top_newspapers_6_23_2024.png")

```

Here are the total publications by state

```{r}
#total by state
state <- index2 %>% 
count(newspaper_state_clean) %>% 
   group_by(newspaper_state_clean) %>% 
  ungroup()

state <- state %>% 
  mutate(pct_total_pages =(n/sum(n))) %>% 
  arrange(desc(pct_total_pages))
         
 
state$pct_total_pages <-formattable::percent(state$pct_total_pages, 1)
state

#top states by page count
state %>%
  kable() %>%
  kable_styling("striped") %>%
  save_kable("../output_images_tables/top_states_6_21_2024.png")
```


### Regional classification for newspaper

```{r}
#Classification based on https://www.census.gov/programs-surveys/economic-census/guidance-geographies/levels.html#par_textimage_34
index2 <- index2 %>% 
  mutate(region = case_when(newspaper_state=="South Carolina" ~ "South",
                           newspaper_state=="Texas" ~ "South",
                            newspaper_state=="Louisiana" ~ "South",
                            newspaper_state=="Tennessee" ~ "South",
                            newspaper_state=="Mississippi" ~ "South",
                            newspaper_state=="Arkansas" ~ "South",
                            newspaper_state=="Alabama" ~ "South",
                            newspaper_state=="Georgia" ~ "South",
                            newspaper_state=="Virginia" ~ "South",
                            newspaper_state=="Florida" ~ "South",
                            newspaper_state=="North Carolina" ~ "South",
                            newspaper_state=="Maryland" ~ "South",
                            newspaper_state=="Delaware" ~ "South",
                            newspaper_state=="West Virginia" ~ "South",
                            newspaper_state=="Kentucky" ~ "South",
                            newspaper_state=="Missouri" ~ "Midwest",
                            newspaper_state=="Maine" ~ "Northeast",
                            newspaper_state=="New York" ~ "Northeast",
                            newspaper_state=="New Hampshire" ~ "Northeast",
                            newspaper_state=="Vermont" ~ "Northeast",
                            newspaper_state=="Massachusetts" ~ "Northeast",
                            newspaper_state=="Connecticut" ~ "Northeast",
                            newspaper_state=="Rhode Island" ~ "Northeast",
                            newspaper_state=="Pennsylvania" ~ "Northeast",
                            newspaper_state=="New Jersey" ~ "Northeast",
                            newspaper_state=="Ohio" ~ "Midwest",
                            newspaper_state=="Indiana" ~ "Midwest",
                            newspaper_state=="Kansas" ~ "Midwest",
                            newspaper_state=="Michigan" ~ "Midwest",
                             newspaper_state=="Wisconsin" ~ "Midwest",
                             newspaper_state=="Minnesota" ~ "Midwest",
                             newspaper_state=="Iowa" ~ "Midwest",
                             newspaper_state=="California" ~ "West",
                             newspaper_state=="Nevada" ~ "West",
                             newspaper_state=="Oregon" ~ "West",
                            newspaper_state=="Illinois" ~ "Midwest",
                            newspaper_state=="Nebraska" ~ "Midwest",
                            newspaper_state=="Colorado" ~ "West",
                            newspaper_state=="North Dakota" ~ "Midwest",
                            newspaper_state=="South Dakota" ~ "Midwest",
                            newspaper_state=="Montana" ~ "West",
                            newspaper_state=="Washington" ~ "West",
                            newspaper_state=="Idaho" ~ "West",
                            newspaper_state=="Wyoming" ~ "West",
                            newspaper_state=="Utah" ~ "West",
                            newspaper_state=="Oklahoma" ~ "South",
                            newspaper_state=="New Mexico" ~ "West",
                            newspaper_state=="Arizona" ~ "West",
                            newspaper_state=="Alaska" ~ "West",
                            newspaper_state=="Hawaii" ~ "West",
                            newspaper_state=="District of Columbia" ~ "South",))

```


```{r}
### Border Designation
index2 <- index2 %>% 
  mutate(border = case_when(newspaper_state=="Maryland" ~ "Border",
                            newspaper_state=="Delaware" ~ "Border",
                            newspaper_state=="West Virginia" ~ "Border",
                            newspaper_state=="Kentucky" ~ "Border",
                            newspaper_state=="Missouri" ~ "Border",
                               .default = "Not_Border"))

```


### Figure 2: State-region totals
```{r}

#total by region
region <- index2 %>% 
  group_by(region) %>% 
  count() %>% 
  ungroup()

region <- region %>% 
  na.omit() %>% 
  rename(total = n) 


region <- region  %>% 
   mutate(pct_total_pages = round(total/sum(total),2)) %>% 
  mutate(pct = formattable::percent(pct_total_pages,0)) %>% 
  arrange(desc(pct_total_pages))


region %>% 
  ggplot(aes(x = region, y = pct, fill = pct)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none", plot.subtitle = element_text(color = "blue", size = 8, face = "italic")) +
    scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label = scales::percent(pct_total_pages)), size = 4, hjust=.5, vjust=0) + 
  labs(title = "Regional Distribution of Lynching Coverage, 1805-1963", 
       subtitle = "Newspapers by Census Region",
       caption = "Newspapers by region with lynching coverage. n=11,223 articles. Graphic by (redacted - peer review), 6/23/2024",
       y="Pct of Pages",
       x="Region")



ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_2_regional_coverage_6_23_2024.png"),device = "png",width=9,height=6, dpi=800)
```


# Tolnay Beck Bailey Victim Data - Compare Trends with News Coverage

The University of Illinois holds the Tolnay_Bailey_Victim_Data of all known lynching cases from 1882-1929.

\#<https://uofi.app.box.com/s/ffmqd2rjxrdt1tvxl38d/file/110041209555>

There are 2,249 lynching cases in this database, again from 1882-1929

There is also the Tolnay, Beck & Bailey list of 5,872 cases, 1865-2020, probable and confirmed lynchings.
https://app.box.com/s/99ggc6epn4rdvritke0h/file/992017683748


**Below, we compare the frequency of news coverage to the Tolnay victims list.**

```{r}

# This contains tolnay_beck	5871 confirmed and probable	lynchings
tolnay_beck <- read_csv("https://osf.io/download/vb8wa/?view_only=6c106acd6cb54f6f849e8c6f9098809f") %>% 
  as.data.frame()

tolnay_beck <- janitor::clean_names(tolnay_beck)


```

```{r}
#Cleaning the types of lynchings
tolnay_beck <- tolnay_beck %>% 
  mutate(
    status_clean = str_to_lower(status)) 
  

tolnay_beck$status_clean <- stringr::str_trim(tolnay_beck$status_clean)

tolnay_beck %>% 
  count(status_clean) 
```
### Figure 3: Lynching Totals, Tolnay & Beck, 2022
```{r}
tolnay_beck <- tolnay_beck %>% 
  mutate(
    status_clean = case_when(
      status_clean == 'coincident death' ~ 'coincidental death',
      status_clean == 'possiible lynching' ~ 'possible lynching',
      TRUE ~ status_clean
    ))

tolnay_counts <- tolnay_beck %>% 
  count(status_clean) %>% 
  mutate(pct_total = round(n/5871, 3)) %>% 
  mutate(pct_total = formattable::percent(pct_total, 1)) 

nice_table(tolnay_counts, short = TRUE)

library(kableExtra)
kbl(tolnay_counts) %>%
  kable_paper(full_width = F) %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em", background = "yellow")


tolnay_graphic <- tolnay_counts %>%
  arrange(desc(n)) %>% 
  kbl(caption = "Lynching Totals, Tolnay & Beck, 2022", font_size = 30) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em", background = "yellow") 

library(webshot)
# Save the kable table as an image
temp_html <- tempfile(fileext = ".html")
save_kable(tolnay_graphic, file = temp_html)
img_file <- tempfile(fileext = ".png")
webshot(temp_html, file = img_file, zoom = 2) # Adjust zoom to control resolution

# Convert the image to a ggplot object
img <- grid::rasterGrob(png::readPNG(img_file), interpolate = TRUE)

# Save the ggplot object as a high-resolution image
ggsave("tolnay_graphic.png", plot = ggplot() + annotation_custom(img, xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf), dpi = 800, width = 10, height = 10)
```

```{r}
tolnay_events <- tolnay_beck %>% 
  select(year, status_clean) %>% 
group_by(year) %>% 
  count(status_clean)

tolnay_events %>% 
  pivot_wider(names_from = status_clean, values_from = n)



```


```{r}
tolnay_beck %>% 
count(year) %>% 
   group_by(year) %>% 
#Sandwich it onto a simple ggplot
  ggplot(aes(x = year, y = n, fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "Actual, Threatened Lynchings, 1865-2020", 
       subtitle = "Count of Actual, Probable Lynchings. Tolnay-Beck Data",
       caption = "n=5,871 incidents (lynchings = 5,039. Graphic by (redacted - peer review), 4/14/2023",
       y="Count",
       x="Year")
#Actual, Threatened Lynchings Tolnay 4_14_2023
```


